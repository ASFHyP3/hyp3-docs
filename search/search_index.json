{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HyP3: ASF's On Demand Processing Platform","text":"<p>The Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline, or HyP3 (pronounced \"hype\"), is a cloud-native  processing platform developed to efficiently process Synthetic Aperture Radar (SAR) imagery. </p> <p>Through HyP3, ASF offers a family of cloud-native, scalable product generation services that  make its data holdings more accessible. HyP3 primarily runs compute-heavy workflows, which take tens of minutes to  hours of compute time, to create products that are easier to use, GIS-ready, and customized to a user\u2019s needs.</p>"},{"location":"#how-it-works","title":"How it Works","text":"<p>HyP3 was designed to address many common issues for users of SAR data:</p> <ul> <li>Most SAR datasets require at least some processing to remove distortions before they are analysis-ready</li> <li>SAR processing is computing-intensive</li> <li>Software for SAR processing is complicated to use and/or prohibitively expensive</li> <li>Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry</li> </ul> <p>HyP3 solves these problems by providing an interface where users can request SAR processing on demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. Refer to our Architecture page to learn more about the  structure of the HyP3 platform.</p> <p>HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; they only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away.</p>"},{"location":"#the-hyp3-service-family","title":"The HyP3 Service Family","text":"<p>HyP3+ is now available!</p> <p></p> <p>For users who need more processing than the 8,000 credit limit allows, ASF now hosts  HyP3+, a user-supported  version of HyP3 that allows you to purchase as many credits as you need. </p> <p>Get to know HyP3+!</p> <p>Explore HyP3 at no cost using HyP3 Basic,  our NASA-supported On Demand processing service, or purchase credits  for use with HyP3+,  our user-supported service that lets you take your application global. Need help scaling your own, in-house workflows?  Contact us!</p> HyP3 BasicSupported by NASA Earthdata HyP3+User-supported Deployment hyp3-api.asf.alaska.eduProducts expire after 14 daysRound-robin processing queueProducts distributed via CloudFront        hyp3-plus.asf.alaska.eduProducts expire after 30 daysSmaller user-queue with higher throughput.Get your products faster!Products provided in a public AWS S3 Bucket 8,000 Credits per month free* 1 Credit = $0.05 <p>*More available by request, as our budget allows. Send requests to ASF User Services:  uso@asf.alaska.edu</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>On Demand products processed by HyP3 can be requested quickly and easily, either by using the Vertex web interface  or programmatically.  These services are currently only available for  Sentinel-1 datasets.</p> <p>Different HyP3 job types consume different credit amounts. In  HyP3 Basic,  users are allotted 8,000 credits per month for free. Refer to the Credits page  for more information. </p> <p>If you do not have enough credits to generate all the products you need for your  project, you can purchase additional credits in HyP3+.</p>"},{"location":"#authentication","title":"Authentication","text":"<p>HyP3 users must authenticate with  Earthdata Login (EDL)  credentials before they can submit jobs to HyP3 (either HyP3 Basic or HyP3+) for processing or access information  about the resulting On Demand products. Refer to our  Authentication page for guidance.</p>"},{"location":"#web-access","title":"Web Access","text":"<p>ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis.</p> <ul> <li>Vertex</li> </ul>"},{"location":"#programmatic-access","title":"Programmatic Access","text":"<p>Requesting and downloading On Demand products can also be done programmatically:</p> <ul> <li>HyP3 SDK for Python</li> <li>HyP3 REST API</li> </ul>"},{"location":"#hyp3-api-rest-endpoints","title":"HyP3 API REST Endpoints","text":"<p>The HyP3 API REST Endpoints  are different for HyP3 Basic and HyP3+. Even though the same  EDL username  can be used to process On Demand products in  HyP3 Basic and  HyP3+  deployments, you will not be able to search for products across both APIs. </p> <p>If you generate products for a single project using both the  HyP3 Basic  and HyP3+ deployments,  you will need to use two separate searches to access all of your products, even if the project names are the same,  when using Vertex or the  HyP3 API.</p> <p>When using the HyP3 SDK for Python, you can combine your results  into one list using the following approach: <pre><code>import hyp3_sdk as sdk\nhyp3 = sdk.HyP3()\nhyp3_plus = sdk.HyP3('https://hyp3-plus.asf.alaska.edu')\njobs = hyp3.find_jobs(...)\njobs += hyp3_plus.find_jobs(...)\njobs.download_files()\n</code></pre></p>"},{"location":"#public-visibility-of-jobs","title":"Public Visibility of Jobs","text":"<p>Warning</p> <p>All jobs submitted to HyP3, whether via web access or programmatic access, are publicly visible. Anyone with access to HyP3 can potentially:</p> <ul> <li>View your jobs and associated metadata, including job name and user ID.</li> <li>Download any products generated by your jobs.</li> </ul> <p>In particular, do not include any sensitive information in your job names.</p>"},{"location":"#whats-new","title":"What's New","text":"<p>Follow @ASFHyP3 on Twitter, or check our What's New page to keep up to date on all things HyP3!</p>"},{"location":"#contact-us","title":"Contact Us","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"about/","title":"HyP3: ASF's On Demand Processing Platform","text":"<p>The Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline, or HyP3 (pronounced \"hype\"), is a cloud-native  processing platform developed to efficiently process Synthetic Aperture Radar (SAR) imagery. </p> <p>Through HyP3, ASF offers a family of cloud-native, scalable product generation services that  make its data holdings more accessible. HyP3 primarily runs compute-heavy workflows, which take tens of minutes to  hours of compute time, to create products that are easier to use, GIS-ready, and customized to a user\u2019s needs.</p>"},{"location":"about/#how-it-works","title":"How it Works","text":"<p>HyP3 was designed to address many common issues for users of SAR data:</p> <ul> <li>Most SAR datasets require at least some processing to remove distortions before they are analysis-ready</li> <li>SAR processing is computing-intensive</li> <li>Software for SAR processing is complicated to use and/or prohibitively expensive</li> <li>Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry</li> </ul> <p>HyP3 solves these problems by providing an interface where users can request SAR processing on demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. Refer to our Architecture page to learn more about the  structure of the HyP3 platform.</p> <p>HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; they only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away.</p>"},{"location":"about/#the-hyp3-service-family","title":"The HyP3 Service Family","text":"<p>HyP3+ is now available!</p> <p></p> <p>For users who need more processing than the 8,000 credit limit allows, ASF now hosts  HyP3+, a user-supported  version of HyP3 that allows you to purchase as many credits as you need. </p> <p>Get to know HyP3+!</p> <p>Explore HyP3 at no cost using HyP3 Basic,  our NASA-supported On Demand processing service, or purchase credits  for use with HyP3+,  our user-supported service that lets you take your application global. Need help scaling your own, in-house workflows?  Contact us!</p> HyP3 BasicSupported by NASA Earthdata HyP3+User-supported Deployment hyp3-api.asf.alaska.eduProducts expire after 14 daysRound-robin processing queueProducts distributed via CloudFront        hyp3-plus.asf.alaska.eduProducts expire after 30 daysSmaller user-queue with higher throughput.Get your products faster!Products provided in a public AWS S3 Bucket 8,000 Credits per month free* 1 Credit = $0.05 <p>*More available by request, as our budget allows. Send requests to ASF User Services:  uso@asf.alaska.edu</p>"},{"location":"about/#getting-started","title":"Getting started","text":"<p>On Demand products processed by HyP3 can be requested quickly and easily, either by using the Vertex web interface  or programmatically.  These services are currently only available for  Sentinel-1 datasets.</p> <p>Different HyP3 job types consume different credit amounts. In  HyP3 Basic,  users are allotted 8,000 credits per month for free. Refer to the Credits page  for more information. </p> <p>If you do not have enough credits to generate all the products you need for your  project, you can purchase additional credits in HyP3+.</p>"},{"location":"about/#authentication","title":"Authentication","text":"<p>HyP3 users must authenticate with  Earthdata Login (EDL)  credentials before they can submit jobs to HyP3 (either HyP3 Basic or HyP3+) for processing or access information  about the resulting On Demand products. Refer to our  Authentication page for guidance.</p>"},{"location":"about/#web-access","title":"Web Access","text":"<p>ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis.</p> <ul> <li>Vertex</li> </ul>"},{"location":"about/#programmatic-access","title":"Programmatic Access","text":"<p>Requesting and downloading On Demand products can also be done programmatically:</p> <ul> <li>HyP3 SDK for Python</li> <li>HyP3 REST API</li> </ul>"},{"location":"about/#hyp3-api-rest-endpoints","title":"HyP3 API REST Endpoints","text":"<p>The HyP3 API REST Endpoints  are different for HyP3 Basic and HyP3+. Even though the same  EDL username  can be used to process On Demand products in  HyP3 Basic and  HyP3+  deployments, you will not be able to search for products across both APIs. </p> <p>If you generate products for a single project using both the  HyP3 Basic  and HyP3+ deployments,  you will need to use two separate searches to access all of your products, even if the project names are the same,  when using Vertex or the  HyP3 API.</p> <p>When using the HyP3 SDK for Python, you can combine your results  into one list using the following approach: <pre><code>import hyp3_sdk as sdk\nhyp3 = sdk.HyP3()\nhyp3_plus = sdk.HyP3('https://hyp3-plus.asf.alaska.edu')\njobs = hyp3.find_jobs(...)\njobs += hyp3_plus.find_jobs(...)\njobs.download_files()\n</code></pre></p>"},{"location":"about/#public-visibility-of-jobs","title":"Public Visibility of Jobs","text":"<p>Warning</p> <p>All jobs submitted to HyP3, whether via web access or programmatic access, are publicly visible. Anyone with access to HyP3 can potentially:</p> <ul> <li>View your jobs and associated metadata, including job name and user ID.</li> <li>Download any products generated by your jobs.</li> </ul> <p>In particular, do not include any sensitive information in your job names.</p>"},{"location":"about/#whats-new","title":"What's New","text":"<p>Follow @ASFHyP3 on Twitter, or check our What's New page to keep up to date on all things HyP3!</p>"},{"location":"about/#contact-us","title":"Contact Us","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"application-snippet/","title":"Application snippet","text":"<p>New On Demand users must request HyP3 access</p> <p>To ensure responsible use of ASF's On Demand resources, new users must submit an access request form. Requests will be processed within 48 hours (#TODO: adjust this limit as necessary), and the user will receive an email indicating if their request has been approved. Users must be approved before they can submit jobs for On Demand processing. Refer to the Requesting Access page for more information.</p> <p>On Demand services are provided at no cost to the user, and anyone can request access to this service. To ensure that the processing is equitably distributed throughout the user community, we have implemented a number of policies:</p> <ul> <li>Users must have Earthdata Login Credentials</li> <li>Users must request access to On Demand services, using their Earthdata Login credentials</li> <li>Each user is given a monthly allotment of credits to use for processing, and different job types consume different credit amounts</li> <li>Job processing rotates through the users in the queue, so it will take longer to process all of your jobs when there are more users in the queue, especially if you submit a large batch of jobs</li> </ul>"},{"location":"citing-snippet/","title":"Citing snippet","text":"<p>To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository:</p> <p>Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., &amp; Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138</p>"},{"location":"contact-snippet/","title":"Contact snippet","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"contact/","title":"Contact Us","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in helping make custom On Demand SAR processing accessible!</p> <p>We're excited you would like to contribute to HyP3! Whether you're finding bugs,  adding new features, fixing anything broken, or improving documentation, get  started by submitting an issue or pull request!</p> <p>Please read our Code of Conduct before contributing.</p>"},{"location":"contributing/#issues-and-pull-requests-are-welcome","title":"Issues and Pull Requests are welcome","text":"<p>If you have any questions or ideas, or notice any problems or bugs, and want to open an issue, great! We recommend first searching our open issues to see if the issue has already been submitted (we may already be working on it!). If you think your  issue is new, you're welcome to create a new issue in our general issues tracker. If you know the specific repository that your issue pertains to, you can use its issues tracker.</p> <p>Found a typo, know how to fix a bug, want to update the docs, want to add a new feature? Even better! The smaller the PR, the easier it is to review and test, and the more likely it is to be successful. For major contributions, consider opening an issue describing the contribution, so we can help guide and breakup the work into digestible pieces.</p>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>We ask that you follow these guidelines with your contributions</p>"},{"location":"contributing/#style","title":"Style","text":"<p>We generally follow python community standards (PEP8), except we allow line lengths up to 120 characters. We recommend trying to keep lines 80--100 characters long, but allow  up to 120 when it improves readability.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We are working to improve our documentation!</p> <p>For all public-facing functions/methods (not marked internal use), please include type hints (when reasonable) and a docstring formatted Google style.</p>"},{"location":"contributing/#tests","title":"Tests","text":"<p>All of the automated tests for the project need to pass before your submission will be accepted.</p> <p>If you add new functionality, please consider adding tests for that functionality as well.</p>"},{"location":"contributing/#commits","title":"Commits","text":"<ul> <li>Make small commits that show the individual changes you are making</li> <li>Write descriptive commit messages that explain your changes</li> </ul> <p>Example of a good commit message:</p> <pre><code>Improve contributing guidelines. Fixes #10\n\nImprove contributing docs and consolidate them in the standard location\nhttps://help.github.com/articles/setting-guidelines-for-repository-contributors/\n</code></pre>"},{"location":"dems/","title":"Digital Elevation Models","text":"<p>Digital Elevation Models are required when processing SAR data to higher-level products, such as the Radiometric Terrain Correction (RTC) and Interferometric SAR (InSAR) products available On Demand from ASF. </p> <p>In the past, ASF maintained a collection of DEMs that were pre-processed as appropriate for SAR workflows, and applied a preference hierarchy so that the best available DEM in any given area would be automatically selected for processing. With the public release of the GLO-30 Copernicus DEM, we have changed our DEM strategy to leverage a cloud-hosted copy of the global Copernicus DEM. This is now the only DEM option available for processing RTC and InSAR products.</p> <p>Removal of option to use Legacy DEMs for RTC Processing</p> <p>Users no longer have the option to use legacy DEMs when processing RTC jobs On Demand in Vertex or when using the API or SDK. The Copernicus GLO-30 DEM is now used for all RTC processing.</p> <p>We use the 2022 Release of the Copernicus GLO-30 Public DEM, available on AWS. </p> <p>Coverage gaps in Copernicus DEM GLO-30 filled using GLO-90</p> <p>The 2022 release of the Copernicus DEM GLO-30 dataset does not provide coverage over Armenia and Azerbaijan. In the past, we have not supported On Demand product generation over those areas, due to the lack of DEM coverage. We now use the Copernicus DEM GLO-90 to fill those gaps. </p> <p>The GLO-90 dataset has a pixel spacing of 90 meters, which is not as detailed as the 30-m pixel spacing in the GLO-30 DEM, but it does allow us to provide On Demand products in these regions, where they were previously unavailable. </p> <p>Table 1 summarizes ASF's source DEM. The Copernicus GLO-30 DEM is now the only option available for both RTC and InSAR processing. Note that the DEM is reprojected to the UTM Zone (WGS84) appropriate for the granule location, and a geoid correction is applied before being used for processing. For RTC processing, the DEM is resampled to the pixel spacing of the output product. For InSAR processing, the DEM is resampled to twice the pixel spacing of the output InSAR product (160 m for 20x4 looks, 80 m for 10x2 looks).</p> Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default <p>Table 1: DEM used for On Demand processing. The Copernicus DEM is the only option available when processing RTC and InSAR products.</p> <p>When ordering On-Demand products, you can choose to include a copy of the DEM used for processing in the output product package. For RTC products, this DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. For InSAR products, the DEM copy is output in 32-bit float format, and is upsampled from the DEM resolution used for processing to match the pixel spacing of the output InSAR products.</p> <p>Note that the height values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in SAR processing.</p>"},{"location":"dems/#copernicus-dem","title":"Copernicus DEM","text":"<p>The GLO-30 Copernicus DEM provides global coverage at 30-m pixel spacing (with the current exception of an area covering Armenia and Azerbaijan, see Figure 2). </p> <p>When an On Demand job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS, managed by Sinergise. We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them as required for processing. A geoid correction is applied before it is used for On Demand processing.</p> <p>For the area that does not have coverage with the GLO-30 DEM, we use the Copernicus DEM GLO-90 dataset, which provides elevation data at 90-meter pixel spacing. Users ordering products over this area should be aware that a lower-resolution DEM is used for processing. </p> <p>Figure 1 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 2 details the land area currently only covered by the GLO-30 DEM at 90-m pixel spacing.</p> <p></p> <p>Figure 1: Copernicus DEM GLO-30 coverage map</p> <p></p> <p>Figure 2: Detail of area currently not covered by Copernicus DEM GLO-30. On Demand jobs requested over this area will use the Copernicus DEM GLO-90.</p>"},{"location":"dems/#special-use-dems","title":"Special Use DEMs","text":"<p>AutoRIFT, a process developed by the NASA MEaSUREs ITS_LIVE project, uses custom Greenland and Antarctica DEMs with 240-m resolution. The DEM, associated process input files, and their details are available on the ITS_LIVE project website. </p>"},{"location":"how_it_works/","title":"How it Works","text":"<p>HyP3 is built around three core concepts: Platform, Plugins, and Products.</p>"},{"location":"how_it_works/#platform","title":"Platform","text":"<p>The HyP3 platform makes it easy for users to request processing, monitor their requests, and download processed products. The platform delegates each processing request to a plugin on the user's behalf. A deployment of the HyP3 platform can be integrated with any number of plugins.</p>"},{"location":"how_it_works/#plugins","title":"Plugins","text":"<p>Plugins are the workhorses of HyP3. Each plugin implements a particular processing workflow and produces a product. At their most basic level, HyP3 plugins are Docker containers that handle the entire processing workflow for a single product, including:</p> <ul> <li>Marshaling the required input data</li> <li>performing any needed transformations and computations on the data</li> <li>creating the final product</li> <li>uploading the product to an AWS S3 bucket for distribution</li> </ul> <p>Plugins only need to define a simple interface (entrypoint) that HyP3 understands and is used to run the container. By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need.</p>"},{"location":"how_it_works/#products","title":"Products","text":"<p>Products are the end result of processing, typically one or more data files. For more information about our current products, see our products page.</p>"},{"location":"plugins/","title":"Plugins","text":"<p>Plugins are the science backbone of HyP3; they do all of the data processing and product generation. Plugins can be added to HyP3 to generate new science products, or support different tools/software/algorithms/options/etc that are not currently supported by HyP3.</p>"},{"location":"plugins/#how-plugins-work","title":"How plugins work","text":"<p>At their most basic level, HyP3 plugins are Docker containers with an interface (entrypoint) HyP3 understands. Plugins handle the entire processing workflow for a single product, including:</p> <ul> <li>Marshaling the required input data</li> <li>performing any needed transformations and computations on the data</li> <li>creating the final product</li> <li>uploading the product to an AWS S3 bucket for distribution</li> </ul> <p>By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need.</p>"},{"location":"plugins/#developing-a-plugin","title":"Developing a plugin","text":"<p>To create a new HyP3 plugin, we recommend starting from a Minimal Working Example (MWE) of generating the product you're plugin will generate. Importantly, the MWE should be entirely self contained, and include all the necessary data to generate the product.</p> <p>Once a MWE is developed, it's important to define your plugin's interface  -- this is where HyP3 connects the product generation and users. When designing the interface, you may find it helpful to ask yourself:</p> <ul> <li>what options do I want to provide to users?</li> <li>what's the minimal set information I need to gather from users?<ul> <li>is this information easily input by users?</li> <li>is this information serializable? For example, can the information be written in a JSON file?</li> <li>could I define this information more simply?</li> </ul> </li> </ul> <p>Once a MWE is developed and an interface is defined, you can use our  HyP3 plugin cookiecutter to help you build a plugin that conforms to the plugin requirements.</p>"},{"location":"plugins/#plugin-requirements","title":"Plugin requirements","text":"<p>In order to be supported by HyP3, a plugin must meet a few requirements:</p> <ul> <li>the plugin must be a Docker image that is hosted in a repository where HyP3 will be able to pull it</li> <li>the plugin's entrypoint must minimally accept the following arguments<ul> <li><code>--bucket BUCKET-NAME</code> where <code>BUCKET-NAME</code> is the name of an AWS S3 bucket that output products will be uploaded to</li> <li><code>--bucket-prefix BUCKET-PREFIX</code> where <code>BUCKET-PREFIX</code> is a string appended to the key of any file uploaded to AWS S3 (this is effectively a subfolder in AWS S3)</li> <li><code>--username USER</code> where <code>USER</code> is the username used to authenticate to Earthdata Login</li> <li><code>--password PASSWORD</code> where <code>PASSWORD</code> is the password used to authenticate to Earthdata Login</li> </ul> </li> <li>any necessary user input should be able to be provided through entrypoint arguments</li> <li> <p>when uploading files to the S3 Bucket</p> <ul> <li>products files must be tagged with <code>filetype: product</code></li> <li>if you wish to upload thumbnails or browse images, they must be tagged <code>filetype: thumbnail</code> or <code>filetype: browse</code>   respectively</li> </ul> <p>Note: the <code>aws</code> subpackage of <code>hyp3lib</code> provides helper functions for tagging and uploading files</p> </li> </ul>"},{"location":"plugins/#add-the-plugin-to-hyp3","title":"Add the plugin to HyP3","text":"<p>Once the plugin itself is created, it can be added to the HyP3 system by... TBD.</p>"},{"location":"products/","title":"Available HyP3 Products","text":"<p>On Demand SAR products generated using HyP3 are currently available for the  Sentinel-1 mission  only. Unless otherwise noted, On Demand products are available for 14 days after they have been processed.</p> <p>A Digital Elevation Model (DEM) is required to generate each of the On Demand products offered by ASF, and we  generally use the  GLO-30 Copernicus DEM  in our processing workflows. For more information, refer to our  Digital Elevation Models  documentation.</p>"},{"location":"products/#rtc","title":"RTC","text":"<p>SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric Terrain Correction (RTC) removes  these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications.</p> <p>Sentinel-1 RTC products are generated from Level-1 Sentinel-1 IW acquisitions (either GRD or SLC files), leveraging  GAMMA Software.  Products are distributed as GeoTIFFs projected to a UTM Zone, with a pixel spacing of  10, 20, or 30 meters.  Users can choose to output the products in  gamma-0 or sigma-0 radiometry,  and in  power, amplitude, or dB scale.  Users also have the option to  apply a speckle filter.  To learn more, refer to the Sentinel-1 RTC Product Guide.</p> <p>For step-by-step instructions on searching for, ordering, downloading and using On Demand RTC products, visit our  RTC On Demand!  tutorial.</p>"},{"location":"products/#insar","title":"InSAR","text":"<p>Interferometric SAR (InSAR) uses the phase differences from repeat passes over the  same area to identify regions where the distance between the sensor and the Earth's  surface has changed. This allows for the detection and quantification of surface  deformation or ground movement. </p> <p>There are three different processing approaches available for generating On Demand InSAR products from Sentinel-1: </p> <ul> <li>Full-scene processing using GAMMA software </li> <li>Burst-based processing using ISCE2 software</li> <li>ARIA Frame-based processing using ISCE2 software</li> </ul> <p>For an in-depth comparison of the On Demand InSAR product options, visit our Sentinel-1 InSAR On Demand Product Comparison StoryMap. </p>"},{"location":"products/#full-scene-insar-gamma","title":"Full-scene InSAR (GAMMA)","text":"<p>These products take Sentinel-1 IW SLC scene pairs as input, and processing is performed using  GAMMA Software.  Products are packaged as a collection of GeoTIFFs in a zip file. They are projected to the appropriate UTM Zone for  the product location and can be generated at a pixel spacing of either 80 or 40 meters. To learn more, refer to the  Sentinel-1 InSAR Product Guide.</p> <p>For step-by-step instructions on searching for, ordering and downloading On Demand InSAR products, visit our InSAR On Demand! tutorial.</p>"},{"location":"products/#burst-based-insar-isce2","title":"Burst-based InSAR (ISCE2)","text":"<p>These products take sets of individual  SLC bursts  extracted from Sentinel-1 IW SLC products as input, and processing is performed using  ISCE2 software. Products are packaged as a collection of  GeoTIFFs in a zip file. They are projected to the appropriate UTM Zone for the product  location, and can be generated at a pixel spacing of 80, 40, or 20 meters. </p> <p>The advantage of using burst-based processing is that users have more control of the extent of the output  interferogram, and the burst footprints always fully overlap from one acquisition to the next. Users can select  sets of up to 15 contiguous along-track bursts to generate a single output interferogram. Refer to the  Sentinel-1 Burst InSAR Product Guide  for more information.</p> <p>For step-by-step instructions on searching for, ordering and downloading On Demand Burst InSAR products, visit our  Burst-Based InSAR for Sentinel-1 On Demand  tutorial.</p>"},{"location":"products/#aria-sentinel-1-gunw-products-isce2","title":"ARIA Sentinel-1 GUNW Products (ISCE2)","text":"<p>There is an extensive archive of  ARIA-S1-GUNW  (ARIA Sentinel-1 Geocoded Unwrapped Interferogram) products  available from ASF,  but they are only generated in specific geographic locations. If the existing archive does not provide the  products you need, you can generate ARIA-S1-GUNW products On Demand. </p> <p>ARIA-S1-GUNW products are delivered as netCDF files with 90-m pixel spacing. Products generated On Demand use the same  ISCE2-based  code used to generate the archived products to ensure interoperability, and ARIA S1 GUNW products generated On Demand  are automatically added to the archive. This allows all users to access products generated On Demand indefinitely,  in contrast to the 14-day availability period that applies to most On Demand products generated by ASF.</p> <p>The ARIA-S1-GUNW products use a set framing system to select consistent bursts from input Sentinel-1 IW SLCs to generate interferograms.  Refer to the  ARIA Sentinel-1 GUNW Product Guide  for more information.</p>"},{"location":"products/#autorift","title":"autoRIFT","text":"<p>AutoRIFT  produces a velocity map from observed motion using a feature tracking algorithm developed as part of the  NASA MEaSUREs ITS_LIVE  project. </p> <p>To learn more, visit the  ITS_LIVE project website.</p>"},{"location":"sentinel1/","title":"Sentinel-1 Mission","text":"<p>The Sentinel-1 satellite constellation is part of the Copernicus Earth Observation program, coordinated by the European  Space Agency (ESA) on behalf of the European Commission (EC). Sentinel-1 satellites carry C-band Synthetic Aperture  Radar (SAR) instruments for global, around-the-clock imagery acquisition, even through cloud cover. </p> <p>The mission was designed to support surface deformation applications, and the stable orbits and consistent  acquisition plans of the Sentinel-1 satellites make it easy to generate high-quality Interferometric SAR (InSAR)  products. These products can measure deformation to the centimeter scale, though the 5.6-cm wavelength  of the C-band SAR sensor limits the viability of InSAR in densely vegetated areas. </p> <p>The relatively short interval between acquisitions makes this SAR dataset a very useful tool for monitoring rapid or  sudden landscape changes. In addition, SAR can image the earth's surface through cloud or smoke cover and does not  require sunlight, so valid imagery can be collected on every pass. This is particularly useful for monitoring  conditions during natural disasters such as hurricanes or wildfires, or in areas that are prone to frequent cloud cover.</p> <p>More information about the mission is available from the European Space Agency Sentinel-1 Mission website.</p>"},{"location":"sentinel1/#the-sentinel-1-constellation","title":"The Sentinel-1 Constellation","text":"<p>The Sentinel-1 mission was designed to be a two-satellite constellation, though there have been periods when  only one satellite has been available for image acquisition.</p> <ul> <li>Sentinel-1A was launched April 3, 2014, and is still actively acquiring imagery. </li> <li>Sentinel-1B  was launched April 25, 2016, but ended its mission on December 23, 2021.</li> <li>Sentinel-1C was launched December 5, 2024, replacing Sentinel-1B in the constellation, and    has been acquiring imagery regularly since March 26, 2025.</li> </ul> <p>Each Sentinel-1 satellite has a 12-day repeat cycle, and they all use the same orbit pattern. When there are two active  sensors in the constellation, their orbits are offset 180 degrees to allow repeat passes every 6 days. In this  scenario, most global landmasses are imaged every 12 days. However, some areas of particular interest to the EC,  including Europe and areas undergoing rapid changes due to uplift or subsidence activity, are imaged every 6 days. </p> <p>Refer to the  Sentinel-1 Observation Scenario  for more information on the acquisition plans that have been used to meet mission goals under different  constellation configurations.</p>"},{"location":"sentinel1/#transition-from-sentinel-1b-to-sentinel-1c","title":"Transition from Sentinel-1B to Sentinel-1C","text":"<p>As of December 23, 2021, Sentinel-1B was no longer able to acquire data. An anomaly related to the power supply  could not be repaired, and the satellite has been decommissioned. Refer to  ESA documentation of the end of the Sentinel-1B mission  for more information.</p> <p>The loss of Sentinel-1B resulted in a significant reduction in the spatial and temporal coverage of the Sentinel-1  mission. Refer to  this article by Iain Woodhouse  for an illustration of the global impact of the Sentinel-1B failure. The image below illustrates a gap in the  acquisitions over Alaska. This area of the Yukon-Kuskokwim Delta did not have a Sentinel-1 acquisition during  the summer of 2022 until August 15.</p> <p></p> <p>The gaps in coverage were particularly noticeable the first few months after Sentinel-1B lost power, but some areas  continued to have little or no coverage in the period from December 2021 to April 2025, when Sentinel-1C began  acquiring data regularly. Keep this in mind as you search for data in your area of interest. If there are fewer  results than you would expect, you can  download acquisition plans for the mission from ESA to view the acquisition plan for your area and time period of interest.</p>"},{"location":"sentinel1/#the-future-of-the-sentinel-1-mission","title":"The Future of the Sentinel-1 Mission","text":"<p>Now that Sentinel-1C has replaced Sentinel-1B, and Sentinel-1A continues to acquire data, the constellation has  returned to the same observation scenario used when both Sentinel-1A and Sentinel-1B were active. The  Sentinel-1A platform is approaching the end of its mission, however, and  plans are underway to launch Sentinel-1D to replace it. </p>"},{"location":"tutorials/","title":"HyP3 Tutorials","text":""},{"location":"tutorials/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>We provide step-by-step tutorials for using HyP3 programmatically via Jupyter Notebooks. </p> <ul> <li>Using the HyP3 Python SDK   -- This notebook walks through ordering and accessing RTC, InSAR, and autoRIFT products in Python using the HyP3 SDK.</li> <li>Using the HyP3 SDK to search for jobs run by another user   -- This notebook walks through using the HyP3 SDK to search for jobs run by another user.</li> <li>Using the HyP3 SDK to update a job name   -- This notebook walks through using the HyP3 SDK to rename one of your previously submitted jobs. </li> <li>Using the HyP3 SDK to process new granules for given search parameters   -- These notebooks demonstrate how to process new granules that match particular search parameters,      which is particularly useful for ongoing monitoring of a geographic area of interest.</li> <li>Time series analysis with HyP3 and MintPy   -- This notebook walks through performing a time-series analysis of the 2019    Ridgecrest, CA earthquake with HyP3 On Demand InSAR products and MintPy.</li> <li>Time series analysis with HyP3 ISCE2 burst InSAR products and MintPy   -- This notebook walks through performing a time-series analysis of the 2014   Mount Edgecumbe, AK volcano with HyP3 On Demand Burst InSAR products and MintPy.</li> </ul>"},{"location":"tutorials/#storymaps","title":"StoryMaps","text":"<p>ASF provides a variety of interactive StoryMap tutorials focused on accessing and using Synthetic Aperture Radar (SAR) data available from ASF. They can all be accessed here:</p> <ul> <li>StoryMap Tutorials</li> </ul> <p>The StoryMap collection includes step-by-step tutorials for ordering and accessing RTC and InSAR products in Vertex.</p> <p> </p>"},{"location":"usage_guidelines/","title":"How to Cite HyP3 Products","text":"<p>When using this data in a publication or presentation, we ask that you include the acknowledgement provided with each product. DOIs are also provided for citation when discussing the HyP3 software or plugins.</p> <ul> <li>For multi-file products, the acknowledgement and relevant DOIs are included in   the <code>*.README.md.txt</code> file.</li> <li>For netCDF products, the acknowledgement is included in the <code>source</code> global attribute   and the DOIs are included in the <code>references</code> global attribute.</li> </ul> <p>To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository:</p> <p>Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., &amp; Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138</p>"},{"location":"using-snippet/","title":"Using snippet","text":"<p>On Demand products processed by HyP3 can be requested quickly and easily, either by using the Vertex web interface  or programmatically.  These services are currently only available for  Sentinel-1 datasets.</p> <p>Different HyP3 job types consume different credit amounts. In  HyP3 Basic,  users are allotted 8,000 credits per month for free. Refer to the Credits page  for more information. </p> <p>If you do not have enough credits to generate all the products you need for your  project, you can purchase additional credits in HyP3+.</p>"},{"location":"using-snippet/#authentication","title":"Authentication","text":"<p>HyP3 users must authenticate with  Earthdata Login (EDL)  credentials before they can submit jobs to HyP3 (either HyP3 Basic or HyP3+) for processing or access information  about the resulting On Demand products. Refer to our  Authentication page for guidance.</p>"},{"location":"using-snippet/#web-access","title":"Web Access","text":"<p>ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis.</p> <ul> <li>Vertex</li> </ul>"},{"location":"using-snippet/#programmatic-access","title":"Programmatic Access","text":"<p>Requesting and downloading On Demand products can also be done programmatically:</p> <ul> <li>HyP3 SDK for Python</li> <li>HyP3 REST API</li> </ul>"},{"location":"using-snippet/#hyp3-api-rest-endpoints","title":"HyP3 API REST Endpoints","text":"<p>The HyP3 API REST Endpoints  are different for HyP3 Basic and HyP3+. Even though the same  EDL username  can be used to process On Demand products in  HyP3 Basic and  HyP3+  deployments, you will not be able to search for products across both APIs. </p> <p>If you generate products for a single project using both the  HyP3 Basic  and HyP3+ deployments,  you will need to use two separate searches to access all of your products, even if the project names are the same,  when using Vertex or the  HyP3 API.</p> <p>When using the HyP3 SDK for Python, you can combine your results  into one list using the following approach: <pre><code>import hyp3_sdk as sdk\nhyp3 = sdk.HyP3()\nhyp3_plus = sdk.HyP3('https://hyp3-plus.asf.alaska.edu')\njobs = hyp3.find_jobs(...)\njobs += hyp3_plus.find_jobs(...)\njobs.download_files()\n</code></pre></p>"},{"location":"using-snippet/#public-visibility-of-jobs","title":"Public Visibility of Jobs","text":"<p>Warning</p> <p>All jobs submitted to HyP3, whether via web access or programmatic access, are publicly visible. Anyone with access to HyP3 can potentially:</p> <ul> <li>View your jobs and associated metadata, including job name and user ID.</li> <li>Download any products generated by your jobs.</li> </ul> <p>In particular, do not include any sensitive information in your job names.</p>"},{"location":"using/","title":"Using ASF HyP3","text":"<p>On Demand products processed by HyP3 can be requested quickly and easily, either by using the Vertex web interface  or programmatically.  These services are currently only available for  Sentinel-1 datasets.</p> <p>Different HyP3 job types consume different credit amounts. In  HyP3 Basic,  users are allotted 8,000 credits per month for free. Refer to the Credits page  for more information. </p> <p>If you do not have enough credits to generate all the products you need for your  project, you can purchase additional credits in HyP3+.</p>"},{"location":"using/#authentication","title":"Authentication","text":"<p>HyP3 users must authenticate with  Earthdata Login (EDL)  credentials before they can submit jobs to HyP3 (either HyP3 Basic or HyP3+) for processing or access information  about the resulting On Demand products. Refer to our  Authentication page for guidance.</p>"},{"location":"using/#web-access","title":"Web Access","text":"<p>ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis.</p> <ul> <li>Vertex</li> </ul>"},{"location":"using/#programmatic-access","title":"Programmatic Access","text":"<p>Requesting and downloading On Demand products can also be done programmatically:</p> <ul> <li>HyP3 SDK for Python</li> <li>HyP3 REST API</li> </ul>"},{"location":"using/#hyp3-api-rest-endpoints","title":"HyP3 API REST Endpoints","text":"<p>The HyP3 API REST Endpoints  are different for HyP3 Basic and HyP3+. Even though the same  EDL username  can be used to process On Demand products in  HyP3 Basic and  HyP3+  deployments, you will not be able to search for products across both APIs. </p> <p>If you generate products for a single project using both the  HyP3 Basic  and HyP3+ deployments,  you will need to use two separate searches to access all of your products, even if the project names are the same,  when using Vertex or the  HyP3 API.</p> <p>When using the HyP3 SDK for Python, you can combine your results  into one list using the following approach: <pre><code>import hyp3_sdk as sdk\nhyp3 = sdk.HyP3()\nhyp3_plus = sdk.HyP3('https://hyp3-plus.asf.alaska.edu')\njobs = hyp3.find_jobs(...)\njobs += hyp3_plus.find_jobs(...)\njobs.download_files()\n</code></pre></p>"},{"location":"using/#public-visibility-of-jobs","title":"Public Visibility of Jobs","text":"<p>Warning</p> <p>All jobs submitted to HyP3, whether via web access or programmatic access, are publicly visible. Anyone with access to HyP3 can potentially:</p> <ul> <li>View your jobs and associated metadata, including job name and user ID.</li> <li>Download any products generated by your jobs.</li> </ul> <p>In particular, do not include any sensitive information in your job names.</p>"},{"location":"using/#citing-hyp3","title":"Citing HyP3","text":"<p>To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository:</p> <p>Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., &amp; Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138</p> <p>See the Usage Guidelines section for more information on citing and/or acknowledging On Demand products.</p>"},{"location":"v2-transition/","title":"Welcome to HyP3 v2","text":"<p>As of September 30, 2021, our beta HyP3 service available at https://hyp3.asf.alaska.edu/ has been retired in favor of our new On Demand service powered by HyP3 version 2 (hereafter, just \"HyP3\").</p> <p>On Demand processing through HyP3 is now available directly in Vertex, ASF's data search portal. Vertex provides a friendly interface to request  processing jobs and review previous jobs. To learn how to request jobs through Vertex, please consult the following resources:</p> <ul> <li>Vertex On Demand video tutorial</li> <li>InSAR On Demand! StoryMap tutorial</li> <li>RTC On Demand! StoryMap tutorial</li> </ul> <p>For more information, check out our full documentation at https://hyp3-docs.asf.alaska.edu/. If you have any comments, questions or concerns, please reach out to us! We love feedback. </p>"},{"location":"v2-transition/#contact-us","title":"Contact Us","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"water_masking/","title":"Water Masking","text":"<p>ASF maintains a global water mask dataset for use during InSAR processing. </p> <p>Unwrapping phase differences over waterbodies can introduce unwrapping errors, resulting in misleading deformation signals. Applying a water mask to the interferogram before phase unwrapping can significantly improve the quality of the unwrapped interferogram, as illustrated in ASF's InSAR Water Masking Tutorial. </p> <p>When ordering On-Demand InSAR products from ASF, users can choose the option to apply the water mask prior to phase unwrapping. Even if users choose not to apply the water mask to the interferogram, a copy of the water mask is always included in the InSAR product package for reference. </p>"},{"location":"water_masking/#water-mask-dataset","title":"Water Mask Dataset","text":"<p>ASF implemented the use of a new water mask for InSAR processing on February 15, 2024. The surface water extent datasets available from OpenStreetMap and ESA WorldCover were a significant improvement over the outdated version of the Global Self-consistent, Hierarchical, High-resolution Geography dataset that we were using prior to this change. The data from these new sources is more recent, more detailed, and has fewer geolocation artifacts. </p> <p>The code used to generate this global water mask is available as part of the asf_tools Python package. More information on generating your own water mask using the same approach is available in the readme file for the watermasking subpackage in the asf-tools GitHub repository.</p>"},{"location":"water_masking/#source-data","title":"Source Data","text":"<p>ASF's water mask uses data from both OpenStreetMap and ESA WorldCover. Areas within Canada, Alaska, and Russia are primarily covered by ESA WorldCover data, while the rest of the world is covered by OpenStreetMap data. </p> <p>The water mask identifies coastal waters and most inland waterbodies. All remaining pixels (land, islands in large lakes, very small inland waterbodies, and landfast Antarctic ice) are considered to be not water. </p> <p>Source data for the water mask is only available from 85\u00b0S to 85\u00b0N. Areas north of 85\u00b0N are all treated as water, and areas south of 85\u00b0S are all treated as not water.</p>"},{"location":"water_masking/#openstreetmap-osm","title":"OpenStreetMap (OSM)","text":"<p>OpenStreetMap is a crowd-sourced open-data mapping effort. The OSM database of geographic features can be accessed by anyone, and it includes a number of categories that can be used to map surface water extent.</p> <p>OSM data was used to generate the water mask for all areas except Canada, Alaska, and Russia. To extract the relevant water extent data from the OSM database, the following filters were applied:</p> <ul> <li>wr/natural = water </li> <li>landuse = reservoir</li> <li>waterway = *</li> </ul> <p>In many cases, waterway features stretch from one bank to the other, so islands within those waterways would not be identified as land. To remove islands from the water mask extent, the following filters were applied to the extracted surface water dataset:</p> <ul> <li>place = island </li> <li>place = islet</li> </ul> <p>The resulting list of features was exported as a shapefile, then converted to raster format for inclusion in the reference water mask.</p>"},{"location":"water_masking/#esa-worldcover","title":"ESA WorldCover","text":"<p>In October 2021, the European Space Agency (ESA) released the first version of its global land cover dataset, WorldCover. It uses remote sensing data from the Sentinel-1 and Sentinel-2 missions to generate land cover classes, including water. More information is available from the ESA WorldCover 2020 website.</p> <p>This dataset was used to generate the water masks for Canada, Alaska, and Russia. It includes one class for permanent water bodies. The version 1.0 source rasters were downloaded from the ESA WorldCover 2020 Downloader site. They were reclassified so that all areas with a value of 80 (Permanent water bodies) were defined as water, and all other values were considered not water.</p>"},{"location":"water_masking/#reference-water-mask","title":"Reference Water Mask","text":"<p>The water mask rasters generated from the OSM and WorldCover datasets were mosaicked together, then tiled to 5\u00b0 latitude by 5\u00b0 longitude for storage. Because source data is only available from 85\u00b0S to 85\u00b0N, tiles were added to fill the polar areas. All pixels north of 85\u00b0N are treated as water, and all pixels south of 85\u00b0S are treated as land.</p> <p>This reference dataset is stored in a public AWS S3 bucket: </p> <p><code>s3://asf-dem-west/WATER_MASK/TILES/</code></p> <p>In the reference raster dataset hosted in AWS, pixels with surface water are assigned a value of 1, and all other pixels are assigned a value of 0. </p> <p>Note that the pixel values used in the reference water mask are opposite to the pixel values used for the water masks included in the InSAR product packages. Refer to the Applying the Water Mask section for more information on how the reference water mask is transformed during InSAR processing.</p>"},{"location":"water_masking/#acknowledgments","title":"Acknowledgments","text":""},{"location":"water_masking/#openstreetmap","title":"OpenStreetMap","text":"<p>OpenStreetMap\u00ae is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF).</p>"},{"location":"water_masking/#esa-worldcover_1","title":"ESA WorldCover","text":"<p>\u00a9 ESA WorldCover project. Contains modified Copernicus Sentinel data (2020) processed by ESA WorldCover consortium.</p>"},{"location":"water_masking/#applying-the-water-mask-during-insar-processing","title":"Applying the Water Mask during InSAR Processing","text":"<p>When an InSAR job is submitted for ASF's On Demand processing, the coordinates of the four corners of the input Sentinel-1 scene are used to find the water mask tile(s) that cover the scene. If the scene crosses multiple tiles, the necessary tiles are mosaicked together. The water mask is then clipped to match the spatial extent of the input Sentinel-1 scene pair.</p> <p>The pixel values of the mosaicked and clipped water mask are changed to meet the requirements of the InSAR processing software. Water pixels are assigned a value of 0, and all remaining pixels are assigned a value of 1. Note that these pixel values differ from the reference water mask, where water pixels have a value of 1 and all other pixels have a value of 0.</p> <p>If the option to Apply Water Mask was selected by the user submitting the InSAR job, this mask is then used as an input, along with coherence values, to generate the validity mask used for phase unwrapping. The 0-value water pixels are excluded from use in phase unwrapping.</p> <p>A copy of the water mask is always included in the InSAR product package for reference, even if the user chose not to select the option to apply the water mask. In this copy of the water mask, the pixel values are the same as what is used in InSAR processing: pixels indicating water have a value of 0, and all other pixels are assigned a value of 1.</p>"},{"location":"water_masking/#older-water-mask-versions","title":"Older Water Mask Versions","text":"<p>The first water mask that ASF used for InSAR On-Demand processing was generated using the Global Self-consistent, Hierarchical, High-resolution Geography Database (GSHHG) dataset. </p> <p>This mask combined the GSHHG full-resolution L1 (boundary between land and ocean) and L5 (boundary between Antarctic landfast ice and ocean) datasets, and removed the L2 (boundary between land and large inland waterbodies) dataset minus the L3 (islands) dataset.</p> <p>Originally, the dataset was buffered out 3 km along coastlines and 5 km along the shorelines of inland waterbodies. This buffer was included to decrease the chance that valid land pixels would be excluded from phase unwrapping due to outdated shorelines or geolocation offsets. The discovery that the inclusion of this extra water still led to phase unwrapping errors resulted in the removal of the buffer from the dataset, effective September 27, 2022. </p>"},{"location":"water_masking/#learn-more","title":"Learn More","text":"<p>Refer to the InSAR Water Masking Tutorial for detailed descriptions of the changes to the water mask used for InSAR processing, and interactive content illustrating the impacts these changes have had on output products. </p> <p></p>"},{"location":"whats_new/","title":"What's New","text":""},{"location":"whats_new/#nov-26-2025","title":"Nov 26, 2025","text":"<p>Effective December 1st, monthly credit allotments for ASF\u2019s HyP3 On Demand processing service will be reduced to 8,000 credits per user per month. Due to NASA budget reductions, we have had to reduce our On Demand processing capacity, and can no longer support the same volume of product generation as we have in the past.</p> <p>HyP3 has generated over 12 million analysis-ready Sentinel-1 products since its public debut in October 2020, made possible through generous funding from NASA. We now generate about half a million products a month on behalf of more than 1,000 users.</p> <p>We remain committed to our mission of making remote sensing data accessible. We will continue to balance processing demand against our available budget and adjust quotas as necessary to ensure that we can support as many users as possible.</p> <p>If you are impacted by this reduction in processing capacity, consider these alternatives:</p> <p>For Sentinel-1 InSAR applications:</p> <ul> <li>Consider using our Burst InSAR workflows.   These allow generation of interferograms over a smaller geographic area than a full Sentinel-1 SLC, with a   corresponding reduction in the number of processing credits required for a full InSAR time series analysis.</li> <li>Consider requesting interferograms at a coarser resolution (20x4 looks), which consumes fewer processing credits than   finer resolutions (10x2 or 5x1 looks).</li> <li>ARIA Geocoded Unwrapped Interferograms (GUNW) are   archived and available for many tectonically active regions, potentially eliminating the need for on-demand   processing.</li> </ul> <p>For Sentinel-1 RTC applications:</p> <ul> <li>Consider the OPERA RTC-S1    dataset available through ASF. This data product includes near-global coverage from 2016 to present at 30-m pixel    spacing, potentially eliminating the need for on-demand processing.</li> <li>For applications requiring a finer resolution, consider requesting On Demand RTC products with   20-m pixel spacing, which consumes fewer   processing credits than 10-m pixel spacing while still providing much more detail than 30-m products.</li> </ul> <p>For applications requiring more than 8,000 credits worth of processing each month:</p> <ul> <li>Consider ASF\u2019s recently-launched HyP3+ service, which allows   users to purchase additional processing credits.</li> </ul> <p>If you have questions or concerns about this change, please email uso@asf.alaska.edu.</p>"},{"location":"whats_new/#oct-20-2025","title":"Oct 20, 2025","text":"<p>HyP3+ is now available!</p> <p></p> <p>For users who need more processing than the 10,000 credit limit allows, ASF now hosts  HyP3+, a user-supported  version of HyP3 that allows you to purchase as many credits as you need. </p> <p>Get to know HyP3+!</p>"},{"location":"about/hyp3_basic/","title":"HyP3 Basic","text":"<p>ASF released HyP3 to the public in 2020, allowing all users to request On Demand products at no cost.  This service, now called HyP3 Basic, is funded by NASA, and hosted in  NASA's Earthdata Cloud (EDC). </p> <p>Anyone can use HyP3 Basic. All you need are  Earthdata Login credentials. </p> <p>Each user receives 8,000 credits each month to use for processing any On Demand products. Different HyP3 job types  consume different credit amounts, as defined in the  Credit Cost Table.  If you need to process more data than is possible with this credit allotment, you can purchase additional credits using  HyP3+. </p> <p>Output products are distributed via Amazon CloudFront, and are retained for 14 days before being deleted.  For users who need more time to download products, consider using HyP3+,  which has a 30-day retention period.</p> <p>If you have questions about credit limits, retention policies, or other HyP3-related issues, please contact us at  uso@asf.alaska.edu.</p> <p>Get started with HyP3 Basic!</p>"},{"location":"about/hyp3_plus/","title":"HyP3+","text":"<p>For users who need to process more On Demand products than their HyP3 Basic credit limit allows, ASF provides HyP3+, a separate HyP3 deployment where users can purchase additional credits.</p> <p>HyP3+ offers the same On Demand products as HyP3 Basic,  and users access their purchased credits by logging in with the same  Earthdata Login Credentials used for ordering free On Demand products from HyP3 Basic.</p>"},{"location":"about/hyp3_plus/#hyp3-api-endpoint","title":"HyP3+ API Endpoint","text":"<p>HyP3+ has a different API Endpoint than the standard HyP3 Basic deployment: https://hyp3-plus.asf.alaska.edu</p> <p>Users can interact with HyP3+ using the same methods as HyP3 Basic, but will need to change the API URL.  See the Using HyP3 section for more information on how to request On Demand products with HyP3+ using  Vertex, the  HyP3 Python SDK,  or by using the HyP3 API directly.</p>"},{"location":"about/hyp3_plus/#purchasing-credits","title":"Purchasing Credits","text":"<p>Credits can be purchased at ASF's Web Store. Credit purchases will be processed within two business days, and purchased credits expire 1 year from last date of purchase.</p> <ul> <li>Credits are sold in multiples of 1000. </li> <li>An individual credit costs $0.05, so each 1000-credit bundle costs $50.</li> </ul> <p></p> <ol> <li> <p>Enter the desired quantity of 1000-credit bundles, and click the Add to Cart button.         Refer to the    credit cost table    to calculate the number of credits you will need to process the products you require.</p> </li> <li> <p>Enter your    Earthdata Login (EDL) username.     It is very important to enter your username correctly, as the credits you purchase will be associated with     this username. Type <code>yes</code> in the terms of service field to indicate that you accept the HyP3+     Terms of Service, then click Continue.    </p> <ul> <li>You can purchase credits for multiple EDL usernames in one order. To add additional credits linked to a   different username, click the Continue Shopping button after adding the first credit amount to your cart,   then add another amount of credits to your cart. You will again be prompted for an EDL username, and you can   enter a different username.</li> </ul> <p></p> </li> <li> <p>When you are ready to check out, click the Checkout button. You do not need to Login on the Checkout page.    Simply enter your email in the Contact Information section, and click the Checkout as Guest button.    </p> <ul> <li>It is very important to enter an email address at which you can be reached, as this will be used to send the   confirmation emails indicating when the order was received and processed, and if there are any issues that   need to be resolved.</li> </ul> </li> <li> <p>Enter your payment information, and click the Continue button.</p> </li> <li> <p>Review your order details, and click the Submit Order button if everything is correct.</p> </li> <li> <p>An order summary will be displayed, which you can print if desired. It will also be sent to the email you    entered during checkout.</p> </li> </ol>"},{"location":"about/hyp3_plus/#order-confirmation","title":"Order Confirmation","text":"<p>An email is sent to you when your order is successfully submitted. It includes the details about your purchase, including the EDL username(s) to which the credits will be assigned. If you notice any errors, send an email to uso@asf.alaska.edu describing the issue.</p>"},{"location":"about/hyp3_plus/#order-completion","title":"Order Completion","text":"<p>Once we receive your order, we will add the number of purchased credits to the specified username(s). This process will be completed within two business days of the order confirmation. You will receive an email confirming that the credits have been assigned to your username, at which point they are available for use.</p> <p>Credits expire 1 year from last date of purchase. We do not currently offer a service for automatic credit replenishment, so you will need to place additional orders as necessary when your credit balance is depleted.</p>"},{"location":"about/hyp3_plus/#refunds","title":"Refunds","text":"<p>If you purchase credits in error or need to assign purchased credits to a different username, email ASF User Services, uso@asf.alaska.edu, and we will work with  you to understand and resolve the issue. Any refunds granted will be credited to the original method of payment.</p> <p>For more information about the HyP3+ refund policy, refer to the  HyP3+ Terms of Service.</p>"},{"location":"about/hyp3_plus/#contact-us","title":"Contact Us","text":"<p>If you have any questions or concerns about using HyP3+, or have any suggestions about how we might improve this service, email ASF User Services: uso@asf.alaska.edu.</p>"},{"location":"about/hyp3_plus_terms/","title":"HyP3+ Terms of Service","text":"<p>Effective date: October 20, 2025</p>"},{"location":"about/hyp3_plus_terms/#1-overview","title":"1. Overview","text":"<ol> <li>The following terms and conditions govern Customer access to and use of HyP3+, UAF Geophysical Institute\u2019s (GI) Alaska Satellite Facility\u2019s (ASF) on-demand processing service.</li> <li>HyP3+ is designed to serve users in government, industry, and the general public (hereinafter collectively named Customer) with efficient and easy access to Synthetic Aperture Radar (SAR) data and scalable processing services without the need for specialized software or local computing resources.</li> <li>These Terms of Service (Terms) describe the scope, limitations, and obligations of ASF in relation to the availability, data quality, and performance of HyP3+ service (HyP3+).</li> <li>The Customer's use of HyP3+ signifies their acceptance of the terms outlined in these Terms. By accessing or using HyP3+, Customer agrees to be bound by and comply with the Terms and represents that the Customer understands and can agree to the Terms. If Customer does not agree to be bound by and comply with the Terms, Customer is not permitted to access or use HyP3+.</li> </ol>"},{"location":"about/hyp3_plus_terms/#2-data-quality-and-availability","title":"2. Data Quality and Availability","text":"<ol> <li>The datasets offered via HyP3+ include observations from government-maintained remote sensing sensors such as ESA\u2019s Sentinel-1 and NASA\u2019s NISAR synthetic aperture radar (SAR) missions.</li> <li>As ASF does not have control over the creation, maintenance, or quality of the data produced by these programs, the following limitations apply:<ol> <li>Data Quality: ASF does not guarantee the accuracy, completeness, or reliability of data provided by ESA, NASA, ISRO or any other third-party sources. ASF makes no guarantee of the quality, reliability, usability, availability, or suitability of any ASF data for any particular purpose. ASF data should not be used for any life-critical functions. Customers assume all risks and liabilities, direct or indirect, associated with any use of ASF data.</li> <li>Data Availability: ASF relies on ESA, NASA, ISRO and other third-parties to ensure that federated datasets are accessible. If the resources hosting these data are offline or experience issues, this may result in disruptions to HyP3+. ASF will not be held liable for any unavailability caused by these data outages.</li> </ol> </li> </ol>"},{"location":"about/hyp3_plus_terms/#3-product-usage-guidelines-and-privacy-policy","title":"3. Product Usage Guidelines and Privacy Policy","text":"<ol> <li>Customers are required to follow the HyP3+  Product Usage Guidelines  for citing products and services and  ASF's Privacy policy.</li> </ol>"},{"location":"about/hyp3_plus_terms/#4-pricing","title":"4. Pricing","text":"<ol> <li>The pricing structure of HyP3+ is posted on the Credits page.  Pricing is subject to change without notice.</li> </ol>"},{"location":"about/hyp3_plus_terms/#5-payment","title":"5. Payment","text":"<ol> <li>Payment is due at the time of purchase. Purchases will be approved within two (2) business days, and credits will  be applied to the Customer\u2019s account within that timeframe. Credits expire 1 year from last date of purchase.</li> <li>Payments will not be accepted from Customers residing Iran, North Korea, Cuba, China, Russia, and Libya.  Additionally, we will review requests coming from other countries listed on the  NASA Designated Countries list and on the Office of  Foreign Assets Control\u2019s Sanction Program  list.</li> </ol>"},{"location":"about/hyp3_plus_terms/#6-refunds","title":"6. Refunds","text":"<ol> <li>Credits lost due to failed processing jobs will be refunded automatically.</li> <li>Duplicate jobs and other jobs submitted by Customer error will not be refunded.</li> <li>In the event of ASF terminating HyP3+, remaining credits will be refunded.</li> </ol>"},{"location":"about/hyp3_plus_terms/#7-account-terms","title":"7. Account Terms","text":"<ol> <li>ASF will operate and maintain HyP3+ using software, network resources, hosting, and systems owned or controlled by ASF, and any modifications, enhancements, updates, upgrades, and revisions thereof to provide HyP3+.</li> <li>To access HyP3+, Customer may be asked to provide certain registration details or other information. All information Customer provides must be correct, current and complete and is subject to ASF Privacy Policy. If Customer chooses or is provided with a username, password or any other piece of information as part of ASF security procedures, Customer must treat that information as confidential, and Customer must not disclose it to any other person or entity. Customer agrees to notify us immediately of any unauthorized access to or use of Customer username or password or any other breach of security. ASF has the right to disable any username, password or other identifier, at any time in ASF\u2019s sole discretion for any or no reason, including if, in ASF opinion, Customer have violated any provision of these Terms.</li> </ol>"},{"location":"about/hyp3_plus_terms/#8-customer-generated-products","title":"8. Customer-Generated Products","text":"<ol> <li>HyP3+ products, unless otherwise specified inside the product, carry a  Creative Commons 0 1.0 Universal (CC0) license.</li> <li>ASF makes no guarantees regarding interpretations of HyP3+ data.  Any analysis, conclusions, decisions made based on the data, or products created by Customer are the sole responsibility of the Customer.</li> </ol>"},{"location":"about/hyp3_plus_terms/#9-suspension-of-hyp3-rights","title":"9. Suspension of HyP3+ Rights","text":"<ol> <li>ASF reserves the right to terminate or suspend access to Customer use of HyP3+ or Customer\u2019s account for any reason at ASF discretion, including Customer\u2019s breach of these Terms or any applicable additional Terms. ASF has the sole right to decide whether Customer are in violation of any of the restrictions set forth in these Terms. Account termination may result in destruction of any content associated with Customer\u2019s account.</li> <li>Provisions that, by their nature, should survive termination of these Terms shall survive termination. For example, all of the following will survive termination: any obligation Customer have to pay us or indemnify us, any limitations on ASF liability, any terms regarding ownership or intellectual property rights, and terms regarding governing law and disputes between us.</li> </ol>"},{"location":"about/hyp3_plus_terms/#10-api-terms","title":"10. API Terms","text":"<ol> <li>Abuse or excessively frequent requests to any HyP3 API may result in the temporary or permanent suspension of Customer\u2019s Account's access to the API. ASF, in ASF\u2019s sole discretion, will determine abuse or excessive usage of the API. We will make a reasonable attempt to warn you via email prior to suspension.</li> </ol>"},{"location":"about/hyp3_plus_terms/#11-release-and-indemnification","title":"11. Release and Indemnification","text":"<ol> <li>Access to, and use of, HyP3+ and the downloading of any content are done at Customers\u2019 own risk. ASF does not represent, warrant or guarantee that HyP3+ or its content is compatible with Customer\u2019s computer systems or that HyP3+ or its content is or will be free of viruses, worms, Trojan horses or disabling devices or other code that manifests contaminating or destructive properties or has harmful effects. Customers are responsible for implementing safeguards to protect the security and integrity of Customer\u2019s computer system, and Customer is responsible for the entire cost of any service, repairs or connections of and to Customer\u2019s computer system that may be necessary as a result of Customers use of HyP3+.</li> <li>Customer agrees to defend, indemnify and hold harmless ASF, and their employees, directors, shareholders, and agents (collectively the \u201cIndemnified Group\u201d) from and against any expense, cost, damage, loss, fine, penalty, liability or judgment, and settlements thereof, including reasonable attorneys\u2019 fees, suffered or incurred by the Indemnified Group as a result of any claim, demand, action, arbitration, suit or similar proceeding brought or asserted against one or more members of the Indemnified Group by any third party alleging the Customer performed created, developed and/or produced work that infringes or misappropriates any copyright, patent, trade secret, trademark, or other proprietary rights such third party in using  HyP3+.</li> </ol>"},{"location":"about/hyp3_plus_terms/#12-limitation-of-liability","title":"12. Limitation of Liability","text":"<ol> <li>HyP3+ and all content, and products thereon or delivered thereby are provided \u201cas is\u201d, without warranty or condition of any kind. Access to, and use of, HyP3+ or ASF products through ASF HyP3+ is at Customer\u2019s own risk. Neither ASF, its affiliates, nor any of their respective directors, officers, employees, agents, shareholders or representatives (collectively, \u201cASF representatives\u201d) makes any representation, warranty or condition about the quality, accuracy, reliability, completeness, currency, timeliness, merchantability, fitness for a particular purpose or non-infringement of intellectual property or related rights, of the website, its content or ASF products and services available through ASF websites. Neither ASF nor any ASF representative assumes any responsibility for any errors, omissions or inaccuracies in HyP3+, its content or ASF products available through ASF HyP3+.</li> <li>ASF and all ASF representatives disclaim all warranties, representations and conditions of any kind with respect to HyP3+, its content and ASF products  available through ASF HyP3+, whether express, implied or collateral, including, without limitation, the implied warranties and conditions of merchantability, fitness for a particular purpose and non-infringement of intellectual property and related rights or that the HyP3+ or the content are or will be error-free or will operate without interruption. In no event will ASF or any ASF representatives be liable, whether based on warranty, contract, tort, negligence, strict liability or any other legal theory, for any damages of any kind, including, without limitation, direct, indirect, incidental, consequential, special, exemplary, punitive damages, lost profits, loss of use, loss of data, personal injury, fines, fees, penalties or other liabilities, whether or not ASF or any ASF representatives is advised of the possibility of such damages, resulting from the use of, or the inability to make use of, HyP3+, its content or ASF products available through ASF HyP3+.</li> <li>Any failure by ASF to insist upon or enforce strict performance of any provision of the Terms will not be construed as a waiver of any provisions or right.</li> <li>General Limitations: ASF will not be liable for any direct, indirect, incidental, special, consequential, or punitive damages, including loss of data, loss of profits, or other damages resulting from the use or inability to use HyP3+, even if ASF has been advised of the possibility of such damages.</li> <li>Force Majeure: ASF will not be held responsible for delays or failures to perform its obligations under these Terms caused by events beyond its reasonable control, including but not limited to natural disasters, acts of war, terrorism, labor disputes, or failures of third-party services.</li> <li>External Data: ASF is not responsible for inaccuracies, delays, or failures in data provided by ESA, NASA, ISRO or any third-party data source.</li> </ol>"},{"location":"about/hyp3_plus_terms/#13-export-control-laws-and-sanctions-compliance","title":"13. Export Control Laws and Sanctions Compliance","text":"<ol> <li>ASF represents and warrants that it: (a) shall abide by all applicable local, state, national, and foreign laws, treaties and regulations, including, but not limited to those related to data security, communications, and the transmission of technical or personally identifiable information in connection with providing HyP3+s; (b) shall comply with the Terms of Service of HyP3+, including in providing any support services; and (c) has taken reasonable steps to prevent the introduction of a virus, Trojan horse, self-replicating or other computer instructions that may bypass any internal or external security measure to obtain access to resources of Customer.</li> <li>Customer may not access, download, use or export HyP3+, its content or any of ASF products and services in violation of Canadian or U.S. export laws or regulations, or in violation of any other applicable laws or regulations. Customer agrees to comply with all export laws and restrictions and regulations of any Canadian, United States or foreign agency or authority, and not to directly or indirectly provide or otherwise make available ASF products, services or content in violation of any such restrictions, laws or regulations, or without all necessary approvals, including, without limitation, for the development, design, manufacture or production of nuclear, chemical or biological weapons of mass destruction. As applicable, Customer shall obtain and bear all expenses relating to any necessary licenses and/or exemptions with respect to Customer\u2019s own use of HyP3+ outside the United States. Neither the contents of HyP3+, ASF products, nor the underlying information or technology may be downloaded or otherwise provided or made available, either directly or indirectly, to anyone in contravention of United States United Nations Act and/or Special Economic Measures Act, the U.S. Treasury Department\u2019s list of Specially Designated Nationals and Blocked Persons or the U.S. Commerce Department\u2019s Table of Denial Orders or other lists which may be issued by applicable governments from time to time. By agreeing to these Terms, Customer agree to the foregoing and represent and warrant that Customer are not located in, under the control of, or a national or resident of any such country or on any such list.</li> </ol>"},{"location":"about/hyp3_plus_terms/#14-changes-to-these-terms-of-service","title":"14. Changes to these Terms of Service","text":"<ol> <li>ASF reserves the right to modify these Terms at any time.</li> <li>Customer should review the Terms and such Additional Terms regularly. By accessing or using HyP3+ or the applicable product or service after any such modifications, Customer agree to be bound by, and comply with, the Terms or Additional Terms then posted. If any modification is not acceptable to Customer, Customer must cease accessing and using HyP3+ and ASF products and services.</li> <li>We reserve the right to withdraw or modify all or part of HyP3+ in ASF sole discretion without notice. We may update the content on HyP3+ from time to time, but its content is not necessarily complete or current. We may also suspend or discontinue any part of the products and services offered on  HyP3+, or we may introduce new features or impose limits on certain features or restrict access to parts or all of such services. We\u2019ll try to give Customer notice when we make a material change to such services that would adversely affect Customer, but this isn\u2019t always possible or practical. Except as expressly set out in any Additional Terms, we will not be liable if for any reason all or any part of HyP3+ or ASF products or services are unavailable at any time or for any period.</li> </ol>"},{"location":"about/hyp3_plus_terms/#15-survival","title":"15. Survival","text":"<ol> <li>If any of the provisions contained in the Terms are determined to be void, invalid or otherwise unenforceable by an arbitrator or court of competent jurisdiction, such determination shall not affect the remaining provisions contained herein or the affected provision in a jurisdiction outside the jurisdiction of such court.</li> </ol>"},{"location":"about/hyp3_plus_terms/#16-support","title":"16. Support","text":"<ol> <li>HyP3+ support coverage hours provided by ASF are Monday through Friday, 9:00 AM to 5:00 PM AKT, excluding holidays or campus closures. Please see UAF\u2019s holiday schedule at https://www.alaska.edu/hr/benefits/leave/holidays.php. Hours are subject to change with notice.</li> <li>ASF will provide basic support services to assist with any issues related to HyP3+.</li> <li>This support is limited to HyP3+ functionality and does not cover issues with external data or dependencies beyond ASF\u2019s control.</li> <li>Response Times: ASF will aim to respond to support requests within two (2) business days.</li> <li>Support will primarily be provided via email. Support requests (reporting incidents or requesting help) should be submitted to ASF User Services: uso@asf.alaska.edu.</li> </ol>"},{"location":"about/hyp3_plus_terms/#17-feedback","title":"17. Feedback","text":"<ol> <li>We welcome Customer\u2019s feedback and suggestions about ASF products and HyP3+. By submitting any suggestions, questions, information, material or other content (collectively, \u201cFeedback\u201d), Customer represent and warrant that this Feedback does not infringe or violate the intellectual property or proprietary rights of any third party and that Customer have all rights necessary to convey to us and for us to use this Feedback. By providing any Feedback, Customer grant ASF and ASF licensees, successors and assigns the right to use, reproduce, modify, perform, display, distribute and otherwise disclose to third parties the Feedback for any purpose. All Feedback is considered non-confidential and non-proprietary. Feedback represents the thoughts and opinions of the individual contributing the Feedback and does not necessarily reflect ASF opinion or endorsement.</li> </ol>"},{"location":"guides/burst_insar_product_guide/","title":"Sentinel-1 Burst InSAR Product Guide","text":"<p>This document is a guide for users of Sentinel-1 Burst Interferometric Synthetic Aperture Radar (InSAR) products generated by the Alaska Satellite Facility (ASF).</p> <p>InSAR jobs can be processed on the basis of individual  Sentinel-1 burst SLCs  that comprise the Sentinel-1 SLC products, and users can select up to fifteen contiguous  along-path bursts to merge together into a single interferogram.</p>"},{"location":"guides/burst_insar_product_guide/#burst-insar-software","title":"Burst InSAR Software","text":"<p>The Sentinel-1 Burst InSAR products are generated using the Jet Propulsion Laboratory's  ISCE2 software.  ASF is committed to transparency in product development, and we are pleased to be able to offer an InSAR product  that leverages open-source software for processing.</p> <p>For those who would prefer to work at the scale of a full IW SLC, our original  On Demand InSAR products are still available. These products have a larger  footprint, and are generated using GAMMA software.  If unsure of which InSAR processing option best fits your needs, visit our ASF Sentinel-1 InSAR on Demand Product Comparison StoryMap to explore the capabilities, characteristics, and available products for each of ASF's On Demand InSAR options. </p> <p>Sentinel-1C acquisitions now supported!</p> <p>ISCE2 has been updated to support processing of data collected by Sentinel-1C. Users can now submit  burst-based InSAR jobs for any available bursts from Sentinel-1 IW SLCs, regardless of the platform used  to acquire the data.</p>"},{"location":"guides/burst_insar_product_guide/#burst-insar-job-types","title":"Burst InSAR Job Types","text":"<p>There are currently two different burst-based  InSAR jobs available. The ISCE2 InSAR workflow processes input SLC data on a burst-by-burst basis to generate wrapped  interferograms, regardless of how many bursts are included in the reference and secondary input files. </p> <p>If there are multiple bursts included in the input files, the wrapped interferograms are then merged together for the  final processing steps, so the output is a single interferogram regardless of the number of bursts included in the  input files.</p>"},{"location":"guides/burst_insar_product_guide/#single-burst-insar","title":"Single-Burst InSAR","text":"<p>ASF's original burst-based InSAR job type, <code>INSAR_ISCE_BURST</code>, only accepts a single pair of burst SLCs. This job type  is supported in Vertex as well as the  HyP3 API and Python SDK.</p> <p>All single-burst InSAR jobs cost the same number of  credits, regardless of the processing  options selected. </p> <p>Deprecation of the INSAR_ISCE_BURST job type</p> <p>The original <code>INSAR_ISCE_BURST</code> job type will be deprecated once support for <code>INSAR_ISCE_MULTI_BURST</code> jobs is available in Vertex. For now, only single-burst interferograms are available through the Vertex interface, but support for multi-burst interferograms is coming soon!</p>"},{"location":"guides/burst_insar_product_guide/#multi-burst-insar","title":"Multi-Burst InSAR","text":"<p>The <code>INSAR_ISCE_MULTI_BURST</code> job type accepts sets of burst SLCs. The output is a single merged interferogram over the  full extent of the input bursts. This job type is not yet supported in Vertex, but can be submitted using the  HyP3 API and Python SDK. </p> <p>This job type supports pairings of 1 to 15 contiguous along-track bursts (refer to the  Considerations for Selecting Input Bursts  section for details). The number of bursts processed impacts the number of credits consumed. Refer to the  Credit Cost Table for more details.</p>"},{"location":"guides/burst_insar_product_guide/#sentinel-1-bursts","title":"Sentinel-1 Bursts","text":"<p>Single Look Complex  (SLC) data is required to generate interferograms from Sentinel-1 data. The European Space Agency (ESA) packages this  type of data into Interferometric Wide (IW) SLC products, which are available for download from ASF. These IW SLC  products include three sub-swaths, each containing many individual burst SLCs.</p> <p>Historically, most InSAR processing has been performed using the full IW SLC scene, but ASF has developed a method of  extracting the individual SLC bursts  from IW SLC products, which facilitates burst-based processing workflows.</p> <p>Refer to the  Sentinel-1 Bursts tutorial  to learn more about how ASF extracts burst-level products from Sentinel-1 IW and EW SLCs.</p>"},{"location":"guides/burst_insar_product_guide/#benefits-of-bursts","title":"Benefits of Bursts","text":"<p>Working at the burst level of the Sentinel-1 SLC data provides some key benefits:</p> <p>1. Bursts are consistently geolocated through time. The coverage of a burst is the same for every orbit of the satellite, so you can be confident that every burst with the  same  Full Burst ID  in a stack of acquisitions will cover the same geographic location. In contrast, the framing of the IW SLCs is not  consistent through time, so when using IW SLCs as the basis for InSAR, scene pairs do not always fully overlap.</p> <p>2. Bursts cover a smaller geographic area. IW SLC products are extremely large. In many cases, only a small portion of the IW footprint is of interest.  Burst-based processing allows you to process only the bursts that cover your specific area of interest, which  significantly decreases the time and cost required to generate and analyze InSAR products.</p> <p>3. Bursts provide AOI customization. When using the <code>INSAR_ISCE_MULTI_BURST</code> job type, you can select multiple reference and secondary bursts from along an  orbit path. This allows you to compose a custom area of interest (AOI) and create an InSAR product that spans IW SLC  boundaries. We currently support InSAR jobs that include up to 15 contiguous burst footprints.</p>"},{"location":"guides/burst_insar_product_guide/#using-sentinel-1-burst-insar","title":"Using Sentinel-1 Burst InSAR","text":"<p>Users can request Sentinel-1 Burst InSAR products  On Demand  in ASF's  Vertex  data portal, or make use of our HyP3  Python SDK  or API.  Input pair selection in Vertex uses either the  Baseline Tool  or the SBAS Tool  search interfaces.</p> <p>Only single-pair Burst InSAR processing is currently supported in Vertex</p> <p>We are transitioning from the <code>INSAR_ISCE_BURST</code> to the <code>INSAR_ISCE_MULTI_BURST</code>  HyP3 job type to support multi-burst AOIs.</p> <p><code>INSAR_ISCE_MULTI_BURST</code> job support is currently only available via our API and Python SDK, so  Vertex  users will not be able to submit multi-burst jobs for processing. </p> <p>Burst InSAR jobs submitted in Vertex are currently limited to single-burst pairs, but we plan to add Vertex  support for <code>INSAR_ISCE_MULTI_BURST</code> jobs in the coming months.</p> <p>On Demand InSAR products only include co-polarized interferograms (VV or HH).  Cross-polarized interferograms (VH or HV) are not available using this service.</p> <p>Users are cautioned to read the sections on limitations and error sources in InSAR products  before attempting to use InSAR data. For a more complete description of the properties of SAR, see our  Introduction to SAR  guide.</p>"},{"location":"guides/burst_insar_product_guide/#introduction","title":"Introduction","text":"<p>Interferometric Synthetic Aperture Radar (InSAR) processing uses two SAR images collected over the same area to determine geometric properties of the surface. Missions such as Sentinel-1 are designed for monitoring surface deformation using InSAR, which is optimal when acquisitions are made from a consistent location in space (short perpendicular baseline) over regular time intervals.</p> <p>The phase measurements of two SAR images acquired at different times from the same place in orbit are differenced to detect and quantify surface changes, such as deformation caused by earthquakes, volcanoes, or groundwater subsidence.</p> <p>InSAR can also be used to generate digital elevation models, but the optimal mission for DEM generation has the opposite characteristics of the Sentinel-1 mission. Topography is best mapped when the two acquisitions are obtained as close together as possible in time (short temporal baseline), but from different vantage points in space (larger perpendicular baseline than would be optimal for deformation mapping).</p>"},{"location":"guides/burst_insar_product_guide/#brief-overview-of-insar","title":"Brief Overview of InSAR","text":"<p>SAR is an active sensor that transmits pulses and listens for echoes. These echoes are recorded in phase and amplitude, with the phase being used to determine the distance from the sensor to the target and the amplitude yielding information about the roughness and dielectric constant of that target.</p> <p></p> <p>Figure 1: Two passes of an imaging SAR taken at time T<sub>0</sub> and T<sub>0</sub> + \u2206t, will give two distances to the ground, R<sub>1</sub> and R<sub>2</sub>.  A difference between R<sub>1</sub> and R<sub>2</sub> shows motion on the ground.  In this case, a subsidence makes R<sub>2</sub> greater than R<sub>1</sub>.  Credit: TRE ALTAMIRA</p> <p>InSAR exploits the phase difference between two SAR images to create an interferogram that shows where the phase and, therefore, the distance to the target has changed from one pass to the next, as illustrated in Figure 1.  There are several factors that influence the interferogram, including earth curvature, topographic effects, atmospheric delays, surface motion, and noise.  With proper processing, Sentinel-1 InSAR can be used to detect changes in the earth's surface down to the centimeter scale. Applications include volcanic deformation, subsidence, landslide detection, and earthquake assessment.</p>"},{"location":"guides/burst_insar_product_guide/#wavelengths","title":"Wavelengths","text":"<p>The SAR sensors on the Sentinel-1 satellites transmit C-band signals, with a wavelength of 5.6 cm. The signal wavelength impacts the penetration capability of the signal, so it is important to be aware of the sensor wavelength when working with SAR datasets. C-band SAR will penetrate more deeply into canopy or surfaces than an X-band signal, but not nearly as deep as an L-band SAR signal, which, with a wavelength on the order of 25 cm, is better able to penetrate canopy and return signals from the forest floor.</p> <p>Different wavelengths are also sensitive to different levels of deformation. To detect very small changes over relatively short periods of time, you may require a signal with a smaller wavelength (such as X-band). However, signals with shorter wavelengths are also more prone to decorrelation due to small changes in surface conditions such as vegetation growth.</p> <p>For slower processes that require a longer time interval to detect movement, longer wavelengths (such as L-band) may be necessary. C-band sits in the middle. It can detect fairly small changes over fairly short periods of time, but is not as sensitive to small changes as X-band or as able to monitor surface dynamics under canopy as L-band.</p>"},{"location":"guides/burst_insar_product_guide/#polarizations","title":"Polarizations","text":"<p>Polarization refers to the direction of travel of an electromagnetic wave.  A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged.</p> <p>Most modern SAR systems can transmit chirps with either a horizontal or vertical polarization. In addition, some of these sensors can listen for either horizontal or vertical backscatter. This results in the potential for 4 different types of returns: HH, HV, VV, and VH, with the first letter indicating the transmission method and the second the receive method. For example, VH is a vertically polarized transmit signal with horizontally polarized echoes recorded.</p> <p>For InSAR applications, processing is generally performed on the co-pol (VV or HH) data and not on the cross-pol (VH or HV) data. Each image used in an InSAR pair must be the same polarization - two HH acquisitions of the same area could form a valid pair, and two VV acquisitions of the same area could form a valid pair, but you cannot pair an HH acquisition with a VV acquisition to generate an interferogram.</p> <p>On Demand InSAR products only include co-polarized interferograms. Cross-polarized interferograms are not available using this service.</p>"},{"location":"guides/burst_insar_product_guide/#baselines","title":"Baselines","text":""},{"location":"guides/burst_insar_product_guide/#perpendicular-baseline","title":"Perpendicular Baseline","text":"<p>The term baseline refers to the physical distance between the two vantage points from which images used as an InSAR pair are acquired. The baseline is decomposed into perpendicular (also called normal) and parallel components, as shown in Figure 2.</p> <p>To monitor surface deformation, the perpendicular baseline for the two acquisitions should be very small in order to maximize the coherence of the phase measurements.</p> <p>In order to determine topography, two slightly different vantage points are required. Sensitivity to topography depends on the perpendicular baseline, the sensor wavelength, the distance between the satellite and the ground, and the sensor look angle.</p> <p></p> <p>Figure 2: Geometry of InSAR baselines. Two satellite passes image the same area on the ground from positions S<sub>1</sub> and S<sub>2 </sub>, resulting in a baseline of B, which can be decomposed into perpendicular (B<sub>\u27c2 </sub>) and parallel (B<sub>\u2225 </sub>) components. Here Y is the direction of travel, referred to as the along-track or azimuth direction, and X is the direction perpendicular to motion, referred to as the cross-track or range direction. Credit: ASF</p>"},{"location":"guides/burst_insar_product_guide/#temporal-baseline","title":"Temporal Baseline","text":"<p>In contrast to the (physical) baseline, the temporal baseline refers to the time separation between imaging passes. Along-track interferometry measures motion in the millisecond to second range. This technique can detect ocean currents and rapidly moving objects like boats. Differential interferometry is the standard method used to detect motion in the range of days to years. This is the type of interferometry that is performed by the Sentinel-1 HyP3 InSAR processing algorithm. Table 1 lists different temporal baselines, their common names, and what they can be used to measure.</p> Duration Known as Measurement of ms to sec along-track ocean currents, moving object detection, MTI days differential glacier/ice fields/lava flows, surface water extent, hydrology days to years differential subsidence, seismic events, volcanic activity, crustal displacement <p>Table 1: Temporal baselines and what they measure. Different geophysical phenomena can be detected based upon the temporal baseline. In general, the longer the temporal baseline, the smaller the motion that can be detected.</p>"},{"location":"guides/burst_insar_product_guide/#critical-baseline","title":"Critical Baseline","text":"<p>Large baselines are better than small for topographic mapping. However, as the baseline increases, coherence decreases. At some point, it is impossible to create an interferogram because of baseline decorrelation. The maximum viable baseline per platform, referred to as the critical baseline, is a function of the distance to the ground, the wavelength, and the viewing geometry of the platform.</p> <p>For Sentinel-1, this critical baseline is about 5 km. In practice, if the perpendicular baseline between images is more than 3/4 of the critical baseline, interferogram creation will be problematic due to the level of noise.</p> <p>For deformation mapping, it is best to minimize the perpendicular baseline whenever possible, but there may be tradeoffs in terms of finding suitable temporal baselines. In most cases, however, pairs selected for deformation mapping will have perpendicular baselines much smaller than the critical baseline.</p>"},{"location":"guides/burst_insar_product_guide/#ordering-on-demand-insar-products","title":"Ordering On Demand InSAR Products","text":"<p>All of ASF's On Demand InSAR products are generated using the  HyP3 platform.  Jobs can be submitted for processing using the  Vertex data portal, the  HyP3 Python SDK  or the HyP3 API.</p> <p>InSAR Processing Now Supports Sentinel-1C!</p> <p>GAMMA and ISCE2 software have both been updated to support Sentinel-1C acquisitions as input for InSAR processing.  Users can now use any Sentinel-1 IW SLCs in the archive, including those acquired by Sentinel-1C, as input for  either On Demand InSAR or On Demand Burst InSAR  processing.</p>"},{"location":"guides/burst_insar_product_guide/#vertex","title":"Vertex","text":"<p>InSAR pairs are selected in Vertex using either the Baseline Search or the SBAS Search interface. The process of selecting pairs is the same for both IW SLC products and individual SLC bursts, but you will need to select the appropriate dataset when searching for content. As illustrated below, select the Sentinel-1 option in the Dataset menu to search for IW SLC products, and select the S1 Bursts option to search for individual SLC bursts.</p> <p></p> <p>The Baseline tool is the best option for selecting specific InSAR pairs. Use the Geographic Search to find an image that covers your time and area of interest, select that item in the results, and click the Baseline button in the center panel. The Baseline tool then displays all of the scenes that could be used to generate an interferogram using the selected image. Scroll through the results to find pairs to add to the On Demand queue, or click on items displayed in the plot to highlight that particular image pair.</p> <p>The SBAS tool is designed for generating time series of InSAR pairs. As with the Baseline search, you can launch the SBAS search from the center panel of a Geographic Search result. It will display all of the valid InSAR pairs through time based on the acquisition location of the input scene. This functionality is designed for processing a series of interferograms to be used in SBAS (Small BAseline Subset) analysis. The results can be adjusted based on baseline criteria (both perpendicular and temporal), and restricted to specific periods of time. Once the list is refined, you have the option to add all of the InSAR pairs displayed in the results to the On Demand queue.</p>"},{"location":"guides/burst_insar_product_guide/#hyp3-sdk-and-api","title":"HyP3 SDK and API","text":"<p>The HyP3 SDK and API provide support for creating interferograms based on a pair of selected granules. To identify granules you'd like to process, we suggest using the Geographic, Baseline and SBAS search tools in Vertex. If you'd prefer to request interferogram processing programmatically, we suggest using Vertex's companion Python package: <code>asf_search</code>. This HyP3 SDK Jupyter Notebook provides you with an example of how you can use the <code>asf_search</code> and <code>hyp3_sdk</code> packages together to identify and create stacks of InSAR products.</p>"},{"location":"guides/burst_insar_product_guide/#considerations-for-selecting-an-insar-pair","title":"Considerations for Selecting an InSAR Pair","text":"<p>When selecting an InSAR pair, observe the following required conditions:</p> <ol> <li>Images from an identical orbit direction (either ascending or descending)</li> <li>Images with identical incidence angles and beam mode</li> <li>Images with identical resolution and wavelength (usually from the same sensor)</li> <li>Images with the same viewing geometry (same path and frame)</li> <li>Images with identical polarizations (both HH or VV)</li> </ol> <p>In addition, the following suggestions may be helpful:</p> <ol> <li>Use images from similar seasons/growth/weather conditions</li> <li>For deformation mapping: limited spatial separation of acquisition locations (small physical baseline)</li> <li>For topographic mapping: limited time separation between images (small temporal baseline)</li> </ol> <p>To analyze deformation caused by a single discrete event, such as an earthquake, select images that bracket the event as closely in time as possible. Keeping the window narrowly focused on the time of the event will reduce the impacts of other processes that may mask the signal of the event of interest.</p>"},{"location":"guides/burst_insar_product_guide/#processing-options","title":"Processing Options","text":"<p>There are several options users can set when ordering Burst InSAR On Demand products:</p> <ol> <li> <p>The number of looks drives the resolution and pixel spacing of the output products:</p> Looks Resolution Pixel Spacing 20x4 160 m 80 m 10x2 80 m 40 m 5x1 40 m 20 m <p>Products generated with 10x2 looks have a file size roughly 4 times that of 20x4-look products.  Similarly, 5x1-look products have a file size roughly 4 times that of 10x2-look products  (or 16 times that of 20x4-look products).</p> <p>The default is 20x4 looks.</p> </li> <li> <p>There is an option to apply a water mask. This mask includes coastal waters and most inland waterbodies.  Masking waterbodies can have a significant impact during phase unwrapping, as water can sometimes exhibit enough  coherence between acquisitions to allow for unwrapping to occur over waterbodies, which is invalid. Refer to our  InSAR Water Masking tutorial  for more information. </p> <ul> <li>Water masking is turned off by default.</li> <li>When the water mask option is selected, the conditional water mask will be applied before the phase unwrapping process.</li> <li>For <code>INSAR_ISCE_BURST</code> jobs, a GeoTIFF of the water mask is always included with the InSAR product package, even if the water mask option was not selected for application.</li> <li>For <code>INSAR_ISCE_MULTI_BURST</code> jobs, the GeoTIFF of the water mask is only included if the water mask option is selected. </li> </ul> </li> </ol>"},{"location":"guides/burst_insar_product_guide/#burst-insar-workflow","title":"Burst InSAR Workflow","text":"<p>The Burst InSAR workflow used in HyP3 was developed by ASF using ISCE2 software. The steps include pre-processing,  interferogram preparation, and product creation. Once these steps are performed, an output product package is created.  See the Product Packaging section for  details on the individual files included in the package.</p>"},{"location":"guides/burst_insar_product_guide/#pre-processing","title":"Pre-Processing","text":"<p>Pre-processing steps prepare the SAR images to be used in interferometry. The pre-processing steps include downloading the burst SLC data and repackaging it in the SAFE format,  downloading the DEM file, and downloading the orbit and auxiliary data files.</p>"},{"location":"guides/burst_insar_product_guide/#download-burst-data","title":"Download Burst Data","text":"<p>The Burst InSAR workflow accepts as input a reference and secondary set of  Interferometric Wide swath Single Look Complex  (IW SLC) burst granules. Internally, each set of bursts must share the same polarization (VV or HH), and be  contiguous along a single Sentinel-1 orbit path. See  Considerations for Selecting Input Bursts  for more guidance on constructing valid sets of bursts.</p> <p>The bursts are downloaded using ASF's Sentinel-1 Burst Extractor ,  and then repackaged into reference and secondary  ESA SAFE  files using the  <code>burst2safe</code> package.  This repackaging allows the sets of reference and secondary bursts to be processed with ISCE2 as if they were a  pair of full IW SLC files from ESA.</p>"},{"location":"guides/burst_insar_product_guide/#considerations-for-selecting-input-bursts","title":"Considerations for Selecting Input Bursts","text":"<p>A number of conditions need to be met when selecting the sets of bursts to package into the  reference and secondary SAFE files: </p> <ul> <li>Sets of bursts can contain 1-15 bursts</li> <li>There must be the same number of bursts in the secondary set as there are in the reference set</li> <li>All bursts in both the reference and secondary sets must have the same polarization<ul> <li>Only co-polarized inputs are supported</li> <li>All bursts must be either VV or HH (not VH or HV)</li> </ul> </li> <li>Pairwise bursts in the reference and secondary sets must have the same burst and relative orbit numbers</li> <li>All reference bursts must have been acquired within two minutes of each other</li> <li>All secondary bursts must have been acquired within two minutes of each other</li> <li>Reference bursts must have been acquired before the secondary bursts</li> <li>Bursts crossing the antimeridian are not supported</li> </ul> <p>When selecting input bursts that span across sub-swaths in the same relative path, you must also take care not to  leave gaps. The bursts in neighboring sub-swaths can only be offset along the path by one burst. </p> <p>For example, the grouping of bursts shown in the image on the left in Figure 3 can be submitted for processing, while the grouping in the image on the right would not be valid.</p> <p></p> <p>Figure 3: Illustration of acceptable maximum offsets for bursts across sub-swaths.</p>"},{"location":"guides/burst_insar_product_guide/#download-the-dem-file","title":"Download the DEM File","text":"<p>In order to create differential InSAR products that show motion on the ground, one must subtract the topographic  phase from the interferogram. The topographic phase, in this case, is replicated by using an  existing DEM  to calculate the actual topographic phase. This phase is then removed from the interferogram leaving just the  motion or deformation signal (plus atmospheric delays and noise).</p> <p>The DEM that is used for HyP3 InSAR processing is the  2022 Release of the Copernicus GLO-30 Public DEM  dataset  publicly available on AWS,  which provides global coverage at 30-m pixel spacing (except for an area over Armenia and Azerbaijan, which only  has 90-m coverage).</p> <p>The portion of the DEM that covers the extent of the input bursts is downloaded and resampled if necessary  (for products output at 20-m pixel spacing). An ellipsoid correction is applied. </p>"},{"location":"guides/burst_insar_product_guide/#download-orbit-files-and-calibration-auxiliary-data-files","title":"Download Orbit Files and Calibration Auxiliary Data Files","text":"<p>For Sentinel-1 InSAR processing, ISCE2 requires additional satellite orbit and calibration metadata files. The orbit  files are downloaded from the  Copernicus Data Space Ecosystem.  The calibration auxiliary data files are downloaded from the  Sentinel-1 Mission Performance Center.</p>"},{"location":"guides/burst_insar_product_guide/#burst-insar-processing","title":"Burst InSAR Processing","text":"<p>Burst InSAR processing is performed using the outputs from the processes detailed in the  Pre-Processing section. </p> <p>The Burst InSAR processing code is contained in the  <code>insar_tops_burst.py</code>  script. This script follows the ISCE2 InSAR workflow in  <code>topsApp.py</code>  for the steps <code>startup</code> through <code>geocode</code>. </p> <p>If the reference and secondary SAFE files include multiple bursts, processing is performed on a burst-by-burst  basis for the first seven steps. The resulting burst-based wrapped interferograms are then merged together before  the remaining functions are applied. </p> <p>The<code>topsApp</code> steps perform the following processes:</p> <ol> <li>Extract the orbits, Instrument Processing Facility (IPF) version, burst data, and antenna pattern if it is necessary.</li> <li>Calculate the perpendicular and parallel baselines.</li> <li>Map the DEM into the radar coordinates of the reference image. This generates the longitude, latitude, height and LOS angles on a pixel by pixel grid for each burst.</li> <li>Estimate the azimuth offsets between the input SLC bursts. The Enhanced Spectral Diversity (ESD) method is not used.</li> <li>Estimate the range offsets between the input SLC bursts.</li> <li>Co-register the secondary SLC burst by applying the estimated range and azimuth offsets.</li> <li>Produce the wrapped phase interferogram. </li> <li>If the reference and secondary files contain more than one burst, the burst-based interferograms are then merged together into one output.</li> <li>Apply the Goldstein-Werner power spectral filter with a dampening factor of 0.5.</li> <li>Optionally apply a water mask to the data.</li> <li>Unwrap the wrapped phase interferogram using SNAPHU's minimum cost flow (MCF) unwrapping algorithm to produce the unwrapped phase interferogram.</li> <li>Geocode the output products.</li> </ol>"},{"location":"guides/burst_insar_product_guide/#applying-a-water-mask","title":"Applying a Water Mask","text":"<p>There is the option to apply a water mask to the interferogram. This mask includes coastal waters and most  inland waterbodies. Masking waterbodies can have a significant impact during phase unwrapping, as water can sometimes  exhibit enough coherence between acquisitions to allow for unwrapping to occur over waterbodies, which is invalid.</p> <p>Water masking is turned off by default. When this option is selected, the conditional water mask will be applied  along with coherence and intensity thresholds during the phase unwrapping process. </p> <p>For <code>INSAR_ISCE_BURST</code> jobs, a GeoTIFF of the water mask is always included with the InSAR product package, even  when the water masking option is not applied to the interferogram. For <code>INSAR_ISCE_MULTI_BURST</code> jobs, the GeoTIFF of  the water mask is only included in the product package when the water masking option is applied.</p> <p>The water mask is generated by ASF using data from  OpenStreetMap  and/or  ESA WorldCover  depending on location. Areas within Canada, Alaska, and Russia are primarily covered by ESA WorldCover data, while the rest of the world is covered by OpenStreetMap data. Refer to the  Water Masking  documentation page for more details.</p> <p>This water mask is available for all longitudes, but data is only available from -85 to 85 degrees latitude.  All areas between 85 and 90 degrees north latitude are treated as water, and all areas between 85 and 90 degrees  south latitude are treated as land for the purposes of the water mask.</p> <p>Water masks were previously generated from the  Global Self-consistent, Hierarchical, High-resolution Geography Database (GSHHG)  dataset, but we transitioned to using the OpenStreetMap/ESA WorldCover datasets in February 2024 to improve  performance. In addition to being a more recent and accurate dataset, this also allows us to mask most  inland waterbodies. When using the GSHHG dataset, we only masked large inland waterbodies; with the new mask, all  but the smallest inland waterbodies are masked.</p> <p>We originally applied a 3 km buffer on coastlines and a 5 km buffer on the shorelines of inland waterbodies in the  water mask dataset before using it to mask the interferograms, in an effort to reduce the chance that valid land  pixels would be excluded from phase unwrapping. It appears, however, that the inclusion of more water pixels is more  detrimental to phase unwrapping than the exclusion of some land pixels, so as of September 27, 2022,  the water mask used for this option is no longer buffered.</p> <p>Visit our  InSAR Water Masking tutorial  for more information about how different water masking approaches can impact the quality of an interferogram.</p>"},{"location":"guides/burst_insar_product_guide/#post-processing","title":"Post-Processing","text":""},{"location":"guides/burst_insar_product_guide/#product-creation","title":"Product Creation","text":"<p>Image files are exported into the widely-used GeoTIFF format in a Universal Transverse Mercator (UTM) Zone projection.  Images are resampled to a pixel size that reflects the resolution of the output image based on the requested number of  looks: 80 meters for 20x4 looks, 40 meters for 10x2 looks, and 20 meters for 5x1 looks.</p> <p>Supporting metadata files are created, as well as a quick-look browse image.</p>"},{"location":"guides/burst_insar_product_guide/#product-packaging","title":"Product Packaging","text":"<p>The Burst InSAR output is a zip file containing various files including GeoTIFFs, a PNG browse image,  a metadata file, and a README file.</p>"},{"location":"guides/burst_insar_product_guide/#naming-convention-insar_isce_burst","title":"Naming Convention: INSAR_ISCE_BURST","text":"<p>The Burst InSAR product names are packed with information pertaining to the processing of the data, presented in  the following order, as illustrated in Figure 4.</p> <ul> <li>The imaging platform name, always S1 for Sentinel-1.</li> <li>Relative burst ID values assigned by ESA. Each value identifies a consistent burst footprint; relative burst ID    values differ from one sub-swath to the next.</li> <li>The imaging mode, currently only IW is supported.</li> <li>The sub-swath number, either 1, 2, or 3, indicating which sub-swath the burst is located in.</li> <li>The acquisition dates of the reference (older) scene and the secondary (newer) scene.</li> <li>The polarization of the product, either HH or VV.</li> <li>The product type (always INT for InSAR) and the pixel spacing in meters, which will be 80, 40, or 20, based upon the    number of looks selected when the job    was submitted for processing.</li> <li>The filename ends with the ASF product ID, a 4 digit hexadecimal number.</li> </ul> <p></p> <p>Figure 4: Breakdown of ASF Burst InSAR naming scheme.</p>"},{"location":"guides/burst_insar_product_guide/#naming-convention-insar_isce_multi_burst","title":"Naming Convention: INSAR_ISCE_MULTI_BURST","text":"<p>The base filename for <code>INSAR_ISCE_MULTI_BURST</code> products follows the naming convention below,  as illustrated in Figure 5.</p> <p>S1_rrr_bbbbbbs1ntt-bbbbbbs2ntt-bbbbbbs3ntt_IW_yyyymmdd_yyyymmdd_pp_INTzz_uuuu</p> <ul> <li>each file starts with S1, indicating that the data was acquired by Sentinel-1</li> <li>rrr is the relative path (or orbit) number for the bursts included in the product</li> <li>bbbbbb indicates the first burst ID for each sub-swath<ul> <li>if there are no bursts included for a given sub-swath, the value of bbbbbb would be <code>000000</code></li> </ul> </li> <li>the number following s indicates the sub-swath associated with the burst IDs<ul> <li>for example, <code>s1</code> indicates the first sub-swath</li> <li>there is a placeholder for each of the three sub-swaths, even if there aren't bursts included from all three </li> </ul> </li> <li>ntt indicates the number of bursts included in the product for the given sub-swath<ul> <li>for example, <code>n02</code> indicates that there are 2 bursts included for that sub-swath</li> <li>if there are no bursts included from that sub-swath, the value of tt would be <code>00</code></li> </ul> </li> <li>IW indicates the beam mode (interferometric wide swath)</li> <li>the first yyyymmdd indicates the date the reference bursts were acquired</li> <li>the second yyyymmdd indicates the date the secondary bursts were acquired</li> <li>pp indicates the polarization of the input bursts</li> <li>INT indicates that the product is an interferogram</li> <li>zz indicates the pixel spacing of the output InSAR product (20, 40, or 80 meters)</li> <li>uuuu is the unique product identifier</li> </ul> <p></p> <p>Figure 5: Breakdown of ASF Multi-Burst InSAR naming scheme.</p> <p>As an example, the filename for a VV interferogram with 80-m pixel spacing containing bursts  111111_IW1, 111112_IW1, and 111111_IW2 from path 123 for the reference date of January 1, 2024 and the  secondary date of January 15, 2024, would have the following product name:</p> <p>S1_123_111111s1n02-111111s2n01-000000s3n00_IW_20240101_20240115_VV_INT80_AEB4</p>"},{"location":"guides/burst_insar_product_guide/#image-files","title":"Image Files","text":"<p>Most of the main InSAR product files are 32-bit floating-point single-band GeoTIFFs. The exceptions to this are the  connected components and the water mask files, which are both 8-bit unsigned-integer single-band GeoTIFFs.</p> <p>The following image files are geocoded to the appropriate UTM Zone map projection, based on the location of the  output product:</p> <ul> <li>The normalized coherence file contains pixel values that range from 0.0 to 1.0, with 0.0 being completely non-coherent and 1.0 being perfectly coherent.</li> <li>The unwrapped geocoded interferogram file shows the results of the phase unwrapping process. Negative values indicate movement towards the sensor, and positive values indicate movement away from the sensor. This is the main interferogram output.</li> <li>The wrapped geocoded interferogram file indicates the interferogram phase after applying the adaptive filter immediately before unwrapping. Values range from negative pi to positive pi.</li> <li>The connected components file delineates regions unwrapped as contiguous units by the SNAPHU unwrapping algorithm.</li> <li>The look vectors theta (\u03b8) and phi (\u03c6) describe the elevation and orientation angles of the look vector in radians. The look vectors refer to the look direction back towards the sensor.<ul> <li>The lv_theta (\u03b8) file indicates the SAR look vector elevation angle (in radians) at each pixel, ranging from -\u03c0/2 (down) to \u03c0/2 (up). The look vector elevation angle is defined as the angle between the horizontal surface and the look vector with positive angles indicating sensor positions above the surface.</li> <li>The lv_phi (\u03c6) file indicates the SAR look vector orientation angle (in radians) at each pixel. The look vector orientation angle is defined as the angle between the East direction and the projection of the look vector on the horizontal surface plane. The orientation angle increases towards north, with the North direction corresponding to \u03c0/2 (and south to -\u03c0/2). The orientation angle range is -\u03c0 to \u03c0.</li> </ul> </li> <li>The DEM file gives the local terrain heights in meters, with a geoid correction applied.</li> <li>The water mask file indicates coastal waters and large inland waterbodies. Pixel values of 1 indicate land and 0 indicate water. This file is in 8-bit unsigned integer format.</li> </ul> <p>If the water mask option is selected, the water mask is applied prior to phase unwrapping to exclude  water pixels from the process. The water mask is generated using the  OpenStreetMap  and  ESA WorldCover  datasets. Refer to the  Water Masking Processing Option  section and our  InSAR Water Masking tutorial  for more information about water masking.</p> <p>For jobs processed using <code>INSAR_ISCE_BURST</code>, there are also four non-geocoded images that remain in their native  range-doppler coordinates. These four images comprise the image data required if users want to merge output  Burst InSAR products together, and include:</p> <ul> <li>a wrapped Range-Doppler interferogram, which is a Range-Doppler version of the wrapped interferogram</li> <li>a two-band Range-Doppler look vectors image in the native ISCE2 format</li> <li>Range-Doppler latitude coordinates and Range-Doppler longitude coordinates images that provide the information necessary to map Range-Doppler images into the geocoded domain</li> </ul> <p>These range-doppler files are not included in products generated using <code>INSAR_ISCE_MULTI_BURST</code>,  as the individual bursts are already merged together.</p> <p>An unwrapped phase browse image is included for the unwrapped (unw_phase) phase file, which is in PNG format  and is 2048 pixels wide.</p> <p>The tags and extensions used and example file names for each raster are listed in Table 2 below.</p> Extension Description Example (single-burst)\u2e3bExample (multi-burst) _conncomp.tif Connected Components S1_136231_IW2_20200604_20200616_VV_INT80_12E3_conncomp.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_conncomp.tif _corr.tif Normalized coherence file S1_136231_IW2_20200604_20200616_VV_INT80_12E3_corr.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_corr.tif _unw_phase.tif Unwrapped geocoded interferogram S1_136231_IW2_20200604_20200616_VV_INT80_12E3_unw_phase.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_unw_phase.tif _wrapped_phase.tif Wrapped geocoded interferogram S1_136231_IW2_20200604_20200616_VV_INT80_12E3_wrapped_phase.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_wrapped_phase.tif _lv_phi.tif Look vector \u03c6 (orientation) S1_136231_IW2_20200604_20200616_VV_INT80_12E3_lv_phi.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_lv_phi.tif _lv_theta.tif Look vector \u03b8 (elevation) S1_136231_IW2_20200604_20200616_VV_INT80_12E3_lv_theta.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_lv_theta.tif _dem.tif Digital elevation model S1_136231_IW2_20200604_20200616_VV_INT80_12E3_dem.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_dem.tif _water_mask.tif Water mask S1_136231_IW2_20200604_20200616_VV_INT80_12E3_water_mask.tif\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_water_mask.tif _lat_rdr.tif Range-Doppler latitude coordinates S1_136231_IW2_20200604_20200616_VV_INT80_12E3_lat_rdr.tif _lon_rdr.tif Range-Doppler longitude coordinates S1_136231_IW2_20200604_20200616_VV_INT80_12E3_lon_rdr.tif _los_rdr.tif Range-Doppler look vectors S1_136231_IW2_20200604_20200616_VV_INT80_12E3_los_rdr.tif _wrapped_phase_rdr.tif Wrapped Range-Doppler interferogram S1_136231_IW2_20200604_20200616_VV_INT80_12E3_wrapped_phase_rdr.tif _unw_phase.png Unwrapped phase browse image S1_136231_IW2_20200604_20200616_VV_INT80_12E3_unw_phase.png\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5_unw_phase.png <p>Table 2: Image files in product package</p>"},{"location":"guides/burst_insar_product_guide/#metadata-files","title":"Metadata Files","text":"<p>The product package also includes a number of metadata files.</p> Extension Description Example (single-burst)\u2e3bExample (multi-burst) .README.md.txt Main README file for Burst InSAR products S1_136231_IW2_20200604_20200616_VV_INT80_12E3.README.md.txt\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5.README.md.txt .txt Parameters and metadata for the InSAR pair S1_136231_IW2_20200604_20200616_VV_INT80_12E3.txt\u2e3bS1A_064_E053_1_N27_3_E054_1_N27_8_20200604_20200616_VV_INT80_7EB5.txt <p>Table 3: Metadata files in product package</p>"},{"location":"guides/burst_insar_product_guide/#readme-file","title":"README File","text":"<p>The text file with extension <code>.README.md.txt</code> explains the files included in the folder, and is customized to reflect  that particular product. Users unfamiliar with InSAR products should start by reading this README file, which will  give some background on each of the files included in the product folder.</p>"},{"location":"guides/burst_insar_product_guide/#insar-parameter-file","title":"InSAR Parameter File","text":"<p>The text file with the base filename followed directly by a <code>.txt</code> extension includes processing parameters used to  generate the InSAR product as well as metadata attributes for the InSAR pair. These are detailed in Table 4.</p> Name Description Possible Value Reference Granule Granule name(s) for reference burst or list of bursts (of the two acquisitions in the pair, the dataset with the oldest timestamp) S1_136231_IW2_20200604T022312_VV_7C85-BURST Secondary Granule Granule name(s) for secondary burst or list of bursts (of the two acquisitions in the pair, the dataset with the newest timestamp) S1_136231_IW2_20200616T022313_VV_5D11-BURST Reference Pass Direction Orbit direction of the reference scene DESCENDING Reference Orbit Number Absolute orbit number of the reference scene 30741 Secondary Pass Direction Orbit direction of the reference scene DESCENDING Secondary Orbit Number Absolute orbit number of the secondary scene 31091 Baseline Perpendicular baseline in meters 58.3898 UTC time Time in the UTC time zone in seconds 12360.691361 Heading Spacecraft heading measured in degrees clockwise from north 193.2939317 Spacecraft height Height in meters of the spacecraft above nadir point 700618.6318999995 Earth radius at nadir Ellipsoidal earth radius in meters at the point directly below the satellite 6370250.0667 Slant range near Distance in meters from satellite to nearest point imaged 799517.4338 Slant range center Distance in meters from satellite to the center point imaged 879794.1404 Slant range far Distance in meters from satellite to farthest point imaged 960070.8469 Range looks Number of looks taken in the range direction 20 Azimuth looks Number of looks taken in the azimuth direction 4 InSAR phase filter Was an InSAR phase filter used yes Phase filter parameter Dampening factor 0.5 Range bandpass filter Range bandpass filter applied no Azimuth bandpass filter Azimuth bandpass filter applied no DEM source DEM used in processing GLO-30 DEM resolution Pixel spacing in meters for DEM used to process this scene 30 Unwrapping type Phase unwrapping algorithm used snaphu_mcf Speckle filter Speckle filter applied yes Water mask Was a water mask used yes <p>Table 4: List of InSAR parameters included in the parameter text file for all Burst InSAR products</p> <p>For jobs processed using the <code>INSAR_ISCE_BURST</code> job type, the parameter file will also include some additional entries,  as indicated in Table 5. These additional entries are not included in the parameter file for <code>INSAR_ISCE_MULTI_BURST</code> files. </p> Name Description Possible Value Radar n lines Number of lines (y coordinate) in range-doppler 377 Radar n samples Number of samples (x coordinate) in range-doppler 1272 Radar first valid line First line in range-doppler SLC containing valid data 8 Radar n valid lines Number of lines in range-doppler SLC containing valid data 363 Radar first valid sample First sample in range-doppler SLC containing valid data 9 Radar n valid samples Number of samples in range-doppler SLC containing valid data 1220 Multilook Azimuth Time Interval Time-based spacing of range-doppler SLC lines after multilooking in seconds 0.0082222252 Multilook Range Pixel Size Distance-based spacing of range-doppler SLC samples after multilooking in meters 46.59124229430646 Radar sensing stop Last date and time for data collection 2020-06-04T02:23:16.030988 <p>Table 5: List of additional InSAR parameters included in the parameter text file <code>INSAR_ISCE_BURST</code> job types.</p>"},{"location":"guides/burst_insar_product_guide/#data-access","title":"Data Access","text":"<p>Refer to the Downloads  page for more information on viewing and downloading On Demand InSAR products in Vertex or programmatically.  Once processing is complete, download links for On Demand products are valid for 14 days.</p> <p>Step-by-step instructions for finding and downloading Burst InSAR On Demand products in Vertex are available in the  Downloading On Demand Products  section of the  Burst-Based InSAR for Sentinel-1 On Demand interactive StoryMap tutorial.</p>"},{"location":"guides/burst_insar_product_guide/#merging-sentinel-1-single-burst-insar-products","title":"Merging Sentinel-1 Single-Burst InSAR Products","text":"<p>The merge_tops_burst workflow is no longer available</p> <p>In the past, Burst InSAR products generated using the <code>INSAR_ISCE_BURST</code> job type could be merged together manually using the <code>merge_tops_burst.py</code> workflow. This Python script is no longer supported. Please use the <code>INSAR_ISCE_MULTI_BURST</code> job type to generate interferograms that span multiple bursts.</p>"},{"location":"guides/burst_insar_product_guide/#limitations","title":"Limitations","text":""},{"location":"guides/burst_insar_product_guide/#baseline-calculation","title":"Baseline Calculation","text":"<p>The baseline is defined as the difference of the platform positions when a given area is imaged. HyP3 baselines are calculated using the best state vectors available. If precise orbits are not yet available for the input granules, restituted orbits will be used. The original predicted orbits are not used for InSAR processing in HyP3. If no restituted or precise state vectors are available, the process will not run.</p>"},{"location":"guides/burst_insar_product_guide/#coherence","title":"Coherence","text":"<p>The phase measurements in the two images used in InSAR must be coherent in order to detect change. Random changes in phase from one acquisition to the next can mask actual surface deformation. Vegetation is a common driver of decorrelation, as changes can easily take place in the interval between two acquisitions due to growth, seasonal changes, or wind effects. It will be difficult to generate valid interferograms with C-band data in heavily vegetated regions due to lack of coherence even with fairly short time intervals.</p> <p>Consider seasonality when selecting image pairs. Decorrelation can be particularly high when comparing phase from different seasons. Changes in the condition of vegetation (especially deciduous canopies), snow, moisture, or freeze/thaw state can impact phase measurements. In cases where a temporal baseline is required that spans seasons, it may be better to use an annual interferogram if possible so that the images are more comparable in terms of seasonality.</p>"},{"location":"guides/burst_insar_product_guide/#line-of-sight-measurements","title":"Line-of-Sight Measurements","text":"<p>When looking at a single interferogram, the deformation measurements in the line-of-sight orientation of the sensor  indicate relative motion towards or away from the sensor. InSAR is not sensitive to motion in the azimuth direction  of the satellite, so motion that occurs in the same direction as the satellite's direction of travel will not  be detected.</p> <p>A single interferogram cannot be used to determine the relative contributions of vertical and horizontal movement  to the line-of-sight displacement measurement. To determine how much of the signal is driven by vertical vs. horizontal  movement, you must either use a time series of interferograms, or use reference measurements with known vertical and  horizontal components (such as GNSS measurements from the region of deformation) to deconstruct the line-of-sight  displacement.</p>"},{"location":"guides/burst_insar_product_guide/#phase-unwrapping-reference-point","title":"Phase Unwrapping Reference Point","text":"<p>The reference point for phase unwrapping is set automatically by the topsApp.py script. It may not be an ideal location  to use as a reference point for phase unwrapping. If it is located in an area undergoing deformation, or in an area  with low coherence, the unwrapping may be of lower quality than if the reference point was in a more suitable location.</p> <p>Even when there are no phase unwrapping errors introduced by phase discontinuities, it is important to be aware that  unwrapped phase differences are calculated relative to the reference point. The phase difference value of the reference  point is set to 0 during phase unwrapping, so any displacement values will be relative to that benchmark.</p> <p>If you are interested in the amount of displacement in a particular area, you may wish to choose your own reference  point. The ideal reference point would be in an area of high coherence beyond where deformation has occurred. The  unwrapped phase measurements can be adjusted to be relative to this new reference point. To adjust the values in the  unwrapped phase GeoTIFF, simply select a reference point that is optimal for your use case and subtract the unwrapped  phase value of that reference point from each pixel in the unwrapped phase raster:</p> <p>\u0394\u03a8<sup>*</sup> = \u0394\u03a8 - \u0394\u03c8<sub>ref</sub></p> <p>where \u0394\u03a8<sup>*</sup> is the adjusted unwrapped phase, \u0394\u03a8 is the original unwrapped phase, and  \u0394\u03c8<sub>ref</sub> is the unwrapped phase value at the new reference point.</p>"},{"location":"guides/burst_insar_product_guide/#displacement-values-from-a-single-interferogram","title":"Displacement Values from a Single Interferogram","text":"<p>In general, calculating displacement values from a single interferogram is not recommended. It will be more robust to  use a time series approach to more accurately determine the pattern of movement. When using SAR time-series software  such as MintPy,  you have the option to select a specific reference point, and the values of the input rasters will be adjusted  accordingly.</p>"},{"location":"guides/burst_insar_product_guide/#error-sources","title":"Error Sources","text":"<p>On Demand InSAR products do not currently correct for some common sources of error in interferometry, such as atmospheric effects. Further processing or time series analysis can be performed by the user to identify or reduce the impact of some of these errors when using On Demand InSAR products for analysis.</p>"},{"location":"guides/burst_insar_product_guide/#atmospheric-delay","title":"Atmospheric Delay","text":"<p>While SAR signals can penetrate clouds, atmospheric conditions can delay the transmission of the signal. This results in phase differences that can look like surface deformation signals but are actually driven by differences in the atmospheric conditions between the pair of acquisitions used to generate the interferogram.</p> <p>In some cases, atmospheric errors can be corrected by using an atmospheric model to remove the impacts of the turbulent delay from the interferogram. Another approach is to use time series analysis to identify outliers.</p> <p>Always doubt your interferogram first! View the interferogram critically, and consider if fringe patterns could potentially be driven by atmospheric effects. In general, it is best to avoid drawing conclusions from the outcome of a single interferogram.</p> <p>Tropospheric phase may be less impactful when considering small-scale deformation. As such, if you are using ASF's Sentinel-1 Burst InSAR products to look at deformation signals that are smaller than 1 km\u00b2, you should consider using methods other than the typical atmospheric model-based corrections to remove the effects of atmospheric delay. Potential methods in this case include applying band-pass or high-pass spatial filters, or spatial averaging filters such as the approach outlined in Bekaert et al., 2020.</p>"},{"location":"guides/burst_insar_product_guide/#turbulent-delay","title":"Turbulent Delay","text":"<p>These delays are generally caused by differences in water vapor distribution from one image to the next. They often manifest as wobbly or sausage-shaped fringes, and can potentially mask the signal of a small earthquake.</p>"},{"location":"guides/burst_insar_product_guide/#stratified-delay","title":"Stratified Delay","text":"<p>This type of delay is driven mostly by pressure and temperature differences or gradients through the atmospheric column, and often correlates with topography. This atmospheric signature can be confused with movement caused by volcanic activity. If there are multiple volcanoes in an image, and they all exhibit similar patterns, the signal is likely being driven by this type of atmospheric delay.</p>"},{"location":"guides/burst_insar_product_guide/#dem-errors","title":"DEM Errors","text":"<p>A DEM is used to remove topographic phase impacts, but if there are inaccuracies in the DEM, residual impacts of those errors can remain in the interferogram.</p>"},{"location":"guides/burst_insar_product_guide/#orbit-uncertainties","title":"Orbit Uncertainties","text":"<p>This is generally not an issue for Sentinel-1 data, as the orbits are very precise and generally reliable. On Demand InSAR products are only processed once restituted or precise orbits are available. Orbit uncertainties are more problematic when working with datasets from older missions.</p>"},{"location":"guides/gunw_product_guide/","title":"ARIA Sentinel-1 GUNW Product Guide","text":"<p>This document is a guide for users of  ARIA Sentinel-1 Geocoded Unwrapped (GUNW) Interferograms.</p> <p>The  ARIA Sentinel-1 Geocoded Unwrapped Interferogram (ARIA-S1-GUNW)  product is a standardized interferometric SAR (InSAR) dataset that enables rapid analysis of surface deformation  using Sentinel-1 SAR data. Produced by  JPL\u2019s ARIA  project and hosted at the  Alaska Satellite Facility (ASF) DAAC,  it provides CF-compliant NetCDF files at 90-m pixel spacing, containing unwrapped interferometric phase measurements,  imaging geometry, various correction layers, and metadata. Products are available for VV polarization only.</p> <p>With over 1.1 million (and growing!) freely available products covering major fault systems, volcanic regions,  and coastal zones, ARIA-S1-GUNW products facilitate scientific research and disaster response by simplifying access  to centimeter-scale ground displacement measurements. Generated through an open-source, cloud-based  ISCE2 TopsApp processing pipeline,  these products support applications such as earthquake impact assessment, volcanic monitoring, and long-term land  motion studies, with ongoing improvements enhancing their accuracy and usability.</p> <p>The ARIA project also maintains the  ARIA-tools  software package, which is a suite of open-source tools that allows users to automate the seamless download,  post-processing manipulation, aggregation, and management of ARIA-S1-GUNW products. Refer to the  ARIA-tools GitHub page  for a more thorough overview and installation instructions, and  tutorials led by EarthScope Consortium,  which demonstrate practical applications.</p> <p>This work was funded in part by the Enabling Cloud-Based InSAR Science for an Exploding NASA InSAR Data Archive project under NASA's ACCESS program.</p> <p>ARIA-S1-GUNW products are not produced globally</p> <p>ARIA-S1-GUNW products are routinely produced only for specific locations, so the ASF archive may not contain  products in your area of interest. See the  Ordering On Demand Products section  for information on ordering ARIA-S1-GUNW products for specific Sentinel-1 acquisitions.</p>"},{"location":"guides/gunw_product_guide/#archived-and-on-demand-products","title":"Archived and On Demand Products","text":"<p>While there is a large archive of ARIA-S1-GUNW products that have already been generated and are  ready for download,  they may not cover your area of interest. In addition, the archived products may not include the full range of  temporal baseline pairings required for your analysis. If you are interested in ARIA-S1-GUNW products that are not  already represented in the archive, ASF provides the ability to  generate these products using specific Sentinel-1 SLC pairings. </p> <p>On Demand ARIA-S1-GUNW products are generated using the same code used by the ARIA project, and have been validated  to ensure that products generated On Demand and those generated by the ARIA team at JPL are fully interoperable.  Products generated On Demand are automatically added to the ARIA-S1-GUNW archive once processing is complete, where  they can be found by anyone searching for ARIA-S1-GUNW products. </p>"},{"location":"guides/gunw_product_guide/#aria-frame-ids","title":"ARIA Frame IDs","text":"<p>Sentinel-1 IW SLC products are not created in a way that ensures that granules for the same relative orbit and  location always fully overlap over time. This results in inconsistent framing of the Sentinel-1 IW SLCs that can  make it difficult to create longer series of Sentinel-1 InSAR products.</p> <p>In the image below, Sentinel-1 footprints acquired over an area of interest are displayed. Over the full period of  record of the mission, the SLC framing has shifted considerably, resulting in some acquisitions that hardly  overlap at all. </p> <p></p> <p>To address this issue, the ARIA team defined a standard set of geographic footprints, called frames, that set the  geographic extent for each ARIA-S1-GUNW product. This is possible because while the Sentinel-1 IW SLC products are  not consistently framed along the orbit path, the smaller burst SLCs that comprise each Sentinel-1 IW SLC product  do have consistent footprints. </p> <p>Each ARIA frame is defined by the extent of a specific collection of these individual burst SLCs. Each ARIA-S1-GUNW  product is processed to the extent of one of these frames, which results in output products with consistent footprints  through time. ARIA-S1-GUNW products containing the same bursts, and thus sharing the same geographic footprint, are  said to have the same ARIA Frame ID.</p> <p></p>"},{"location":"guides/gunw_product_guide/#accessing-existing-products","title":"Accessing Existing Products","text":"<p>You can download existing ARIA-S1-GUNW products from the Alaska Satellite Facility\u2019s (ASF)  Vertex  search portal by following these steps: </p> <ol> <li>Access Vertex \u2013 Go to the ASF Vertex website:     https://search.asf.alaska.edu.</li> <li>Search for ARIA-S1-GUNW Products \u2013 In the dataset selector, click on ARIA S1 GUNW to filter for these     specific products. You can refine results by specifying a geographic region, date range, or other criteria     using the search filters in the \u201cfilters\u201d panel. </li> <li>Preview and Select Products \u2013 Click on individual results to view metadata, including coverage area and     acquisition details.</li> <li>Download Data \u2013 To download, first add ARIA-S1-GUNW products to your Download Queue using the     shopping cart icon next to each product, then download your selected products using the     options available in the Download Queue interface. </li> </ol> <p>Search results include both products generated by the ARIA team and products generated by users submitting  ARIA-S1-GUNW jobs for On Demand processing.</p>"},{"location":"guides/gunw_product_guide/#ordering-on-demand-products","title":"Ordering On Demand Products","text":"<p>ARIA-S1-GUNW On Demand Search now available in Vertex!</p> <p>Users can now view and select ARIA Frames and submit On Demand processing jobs directly in Vertex!    </p> <p>Simply activate the On Demand toggle switch in the Geographic Search interface for the ARIA S1 GUNW Dataset  to view ARIA frames and select date pairs to submit for processing.</p> <p>If the ARIA-S1-GUNW products you need are not available in the archive, you can use ASF's On Demand platform to submit  custom ARIA-S1-GUNW jobs for processing. Once processing is complete, there are a couple of different approaches for  accessing On Demand ARIA-S1-GUNW products:</p> <ul> <li>You can access them as you would any other    On Demand    products from ASF.<ul> <li>The download links provided will be active for 14 days.</li> </ul> </li> <li>Products generated On Demand are also added to the archive and can be accessed by    searching for ARIA-S1-GUNW    products.<ul> <li>The links to the archived products never expire. </li> </ul> </li> </ul> <p>Sentinel-1C acquisitions now supported for ARIA-S1-GUNW products</p> <p>The code used for processing ARIA-S1-GUNW products has been updated to support Sentinel-1C acquisitions. You  can now find archived ARIA-S1-GUNW products that include S1C acquisitions, and S1C acquisitions can be submitted  for processing ARIA-S1-GUNW On Demand using the HyP3 API or SDK. Vertex support for submitting requests for  On Demand ARIA-S1-GUNW products including S1C acquisitions is coming soon. </p>"},{"location":"guides/gunw_product_guide/#aria-frame-id-maps","title":"ARIA Frame ID Maps","text":"<p>As described in the ARIA Frame IDs section, ARIA-S1-GUNW  products are processed based on frames that are consistent through time. The ARIA project provides a  geojson file indicating the extent of each ARIA Frame ID.  This file can be downloaded and used for reference, but it can also be displayed in Vertex by activating the  On Demand toggle switch when performing a Geographic Search for ARIA S1 GUNW products. Activating this  toggle also reveals a download button for the geojson.</p> <p></p> <p>By default, Vertex displays frames for the Ascending orbit direction. To view the frames for the Descending orbit  direction, click the Filters button and set the Flight Direction option to the desired direction.</p> <p></p>"},{"location":"guides/gunw_product_guide/#search-for-sentinel-1-slc-acquisition-dates-for-an-aria-frame-id","title":"Search for Sentinel-1 SLC Acquisition Dates for an ARIA Frame ID","text":"<p>The ARIA processing code takes a list of reference and secondary Sentinel-1 IW SLC granules as input, but  it can be tricky to find all of the necessary granules for a given ARIA Frame ID. To ensure that there is  full coverage over the desired ARIA Frame, users just pass the ARIA Frame ID and the dates of the  desired primary and secondary passes over that frame into the On Demand job specification rather than  assembling lists of primary and secondary SLCs.</p> <p>To find suitable primary and secondary acquisition dates to use for a specific ARIA Frame ID, use a  Geographic Search  for the ARIA S1 GUNW Dataset in  Vertex,  and activate the On Demand toggle switch to view the ARIA frames. By default, Vertex displays the frames for the  ascending orbit direction, so you will need to adjust the flight direction setting in the filters to view descending  frames. </p> <p>Click on the desired frame and click Build SBAS SLC Stack to find available date pairings for that frame.</p> <p></p> <p>This opens the SBAS Tool in Frame Mode, allowing users to select dates  that have the required 90% coverage of the ARIA Frame to submit for processing. Click on the On Demand icon for the  desired date pair to  add it to the On Demand Queue.</p> <p></p>"},{"location":"guides/gunw_product_guide/#submit-on-demand-aria-s1-gunw-jobs","title":"Submit On Demand ARIA-S1-GUNW Jobs","text":"<p>On Demand ARIA-S1-GUNW jobs can be submitted using the following methods: </p> <ul> <li>directly through Vertex</li> <li>using the <code>ARIA_S1_GUNW</code> job type in the    HyP3 API</li> <li>using the <code>submit_aria_s1_gunw_job</code> method of the <code>HyP3</code> class in the    HyP3 Python SDK</li> </ul> <p>Unlike ASF's other On Demand InSAR workflows, customizable processing options (multilooking, filter strength, etc.)  are not available for ARIA-S1-GUNW jobs.</p> <p>To submit an ARIA_S1_GUNW job programmatically, all you need is: </p> <ul> <li>the ARIA Frame ID number </li> <li>the reference date, which is the more recent pass over the ARIA Frame</li> <li>the secondary date, which is the earlier pass over the ARIA Frame </li> </ul> <p>The dates must be in YYYY-MM-DD format.</p>"},{"location":"guides/gunw_product_guide/#reference-and-secondary-dates","title":"Reference and Secondary Dates","text":"<p>ARIA-S1-GUNW products use the SLCs from the more recent pass as reference, while secondary scenes are from the  earlier pass in the date pair. When submitting jobs in Vertex, the reference and secondary dates are automatically  ordered for you based on the date pairs selected from the frame-based SBAS search results. </p> <p>When submitting a job using the HyP3 API or SDK, the date passed as the reference  date must be more recent than the secondary date. If they are in the opposite order, an error will be raised.</p> <p>Note that this is the opposite order than what is used for all other On Demand InSAR products available from ASF.  Both the InSAR GAMMA products and the  Burst InSAR products use the  earlier acquisition as reference, and the more recent acquisition as secondary. </p> <p>This means that the ARIA unwrapped interferograms have the opposite sign from the unwrapped interferograms  generated by the other ASF On Demand InSAR workflows. In the ARIA-S1-GUNW products, negative phase differences  indicate movement away from the sensor and positive phase differences indicate movement towards the sensor.</p>"},{"location":"guides/gunw_product_guide/#product-packaging","title":"Product Packaging","text":""},{"location":"guides/gunw_product_guide/#naming-convention","title":"Naming convention","text":"<p>The ARIA-S1-GUNW product names contain detailed information about the acquisitions and processing  workflows used to generate them, including:</p> <ul> <li>Satellite orientation. A for ascending or D for descending</li> <li>Satellite look direction. L for left-looking or R for right-looking</li> <li>Satellite track number (3-digit number)</li> <li>Reference and secondary acquisition dates (YYYYMMDD)</li> <li>Center time of reference scene(s) in UTC (HHMMSS)</li> <li>Longitude and latitude in whole degrees</li> <li>Unique product hash</li> <li>Standard product version</li> </ul> <p>This results in a filename constructed as illustrated in the figure below:</p> <p></p>"},{"location":"guides/gunw_product_guide/#product-elements","title":"Product Elements","text":"<p>The product is packaged as a NetCDF4 file, with its top-level group named <code>science</code>. Within the science group,  there is a <code>grids</code> group, which is further divided into three subgroups: <code>data</code>, <code>imagingGeometry</code>, and <code>corrections</code>. </p> <ul> <li>The <code>data</code> group contains 2D datasets at a resolution of 3 arc-seconds (~90 m).</li> <li>The <code>imagingGeometry</code> group includes 3D datasets posted laterally at 0.1-degree intervals (~11 km). </li> <li>The <code>corrections</code> group provides ionospheric and solid Earth corrections, and if a weather model is available,    the corresponding tropospheric correction layer (<code>HRRR/reference/troposphereWet</code>) will be included here. </li> </ul> <p>All 2D and 3D datasets are in the EPSG:4326 projection.</p> <p>The output netCDF file will include the layers listed in the table below.</p> Group Dataset Name Description Units data amplitude 2D Amplitude of IFG watt coherence 2D Coherence [0-1] from filtered IFG unitless connectedComponents 2D Connected component file unitless unfilteredCoherence 2D Coherence [0-1] from unfiltered IFG unitless unwrappedPhase 2D Filtered unwrapped IFG geocoded rad corrections ionosphere 2D Split spectrum ionospheric delay correction rad ionosphereBurstRamps 2D Split spectrum ionospheric burst ramp correction rad reference/solidEarthTide 3D solid earth tide for reference granule rad secondary/solidEarthTide 3D solid earth tide for secondary granule rad HRRR/reference/troposphereWet 3D wet troposphere for reference granule rad HRRR/secondary/troposphereWet 3D wet troposphere for secondary granule rad HRRR/reference/troposphereHydrostatic 3D hydrostatic troposphere for reference granule rad HRRR/secondary/troposphereHydrostatic 3D hydrostatic troposphere for secondary granule rad imagingGeometry azimuthAngle 3D azimuth angle grid degree incidenceAngle 3D Incidence angle grid degree lookAngle 3D look angle grid degree parallelBaseline 3D parallel baseline grid meter perpendicularBaseline 3D perpendicular baseline grid meter"},{"location":"guides/gunw_product_guide/#ionospheric-correction-layer","title":"Ionospheric Correction Layer","text":"<p>Although the ionospheric effects for C-band SAR are only about one-sixteenth of those at L-band, the measurement  accuracy of Sentinel-1 C-band SAR data can still be degraded by long-wavelength ionospheric signals. Utilizing the  range-split spectrum methodology  available within ISCE2,  ARIA-S1-GUNW products include an ionospheric correction layer packaged as a differential field between the  secondary and reference input data. Since these layers have smooth variation in space they are downsampled to  33 arc-seconds (~1 km), and thus have to be interpolated before being subtracted from the unwrappedPhase field,  which is sampled at 3 arc-seconds (~90 m).</p>"},{"location":"guides/gunw_product_guide/#solid-earth-tides-correction-layers","title":"Solid Earth Tides Correction Layers","text":"<p>Solid Earth tides  (SET) are periodic deformations of the Earth's crust caused by gravitational forces from the Moon and Sun,  resulting in surface displacements of up to several centimeters. Correcting for SET in InSAR is crucial to prevent  these predictable, cyclic motions from being misinterpreted as real ground deformation. ARIA-S1-GUNW products  include SET correction layers, created using the  PySolid  python package, for both the reference and secondary input data. These layers are packaged within the products as 3D  datasets posted laterally at 0.1-degree intervals (~11 km) and vertically at the following height intervals:  -1.5 km, 0 km, 3 km, 9 km.</p>"},{"location":"guides/gunw_product_guide/#tropospheric-delay-correction-layers","title":"Tropospheric Delay Correction Layers","text":"<p>Tropospheric delay correction is essential for many InSAR applications because atmospheric variations in temperature,  pressure, and humidity can distort phase measurements, mimicking ground deformation and reducing accuracy.  ARIA-S1-GUNW products for both the continental U.S. and Alaska also contain a tropospheric delay correction layer  that is produced via the Raytracing Atmospheric Delay Estimation for RADAR  (RAiDER) Python package.</p> <p>RAiDER uses the  NOAA High-Resolution Rapid Refresh  weather model to calculate the tropospheric delay correction at a spatial resolution of approximately 3 km. If the  HRRR weather model is not available for a location of interest, (i.e. outside of the continental U.S. and Alaska) the  tropospheric delay correction layer will not be included in the ARIA-S1-GUNW product. The wet and hydrostatic  tropospheric delay correction are provided for both the reference and secondary input data. These layers are packaged within the products as 3D datasets posted laterally at 0.05-degree intervals (~5.5 km) and vertically in 0.5 km increments between -0.5 km and 9 km.</p>"},{"location":"guides/gunw_product_guide/#data-access","title":"Data Access","text":"<p>ARIA-S1-GUNW On Demand products can be accessed like any other On Demand product, but can also be accessed by  searching the archive.</p>"},{"location":"guides/gunw_product_guide/#accessing-products-using-on-demand-interfaces","title":"Accessing Products using On Demand Interfaces","text":"<p>Refer to the  Downloads  page for more information on viewing and downloading ARIA-S1-GUNW On Demand products in Vertex or programmatically.  Once processing is complete, download links for On Demand products are valid for 14 days.</p>"},{"location":"guides/gunw_product_guide/#accessing-products-in-the-archive","title":"Accessing Products in the Archive","text":"<p>Once On Demand ARIA-S1-GUNW jobs have been processed, they are added to the archive of ARIA-S1-GUNW products. Use the  Geographic Search in Vertex  to find all of the products (both those generated by the ARIA project and those generated On Demand) available for your  area of interest. The download links for archived ARIA-S1-GUNW products never expire. </p> <p>You can also search for ARIA-S1-GUNW products programmatically using the  asf_search Python Package.  Again, results will include products generated by the ARIA team along with any On Demand ARIA-S1-GUNW products that  have completed processing.</p>"},{"location":"guides/gunw_product_guide/#accessing-and-leveraging-product-layers-with-aria-tools","title":"Accessing and Leveraging Product Layers with ARIA-tools","text":"<p>The ARIA-tools Python package supports automated workflows for accessing, subsetting, interpolating, and correcting ARIA-S1-GUNW layers. </p> <p>Refer to the Downloading GUNW products using ariaDownload.py Jupyter Notebook to learn how to use ARIA-tools to search for and download ARIA-S1-GUNW products from ASF's archive. </p> <p>The Manipulating Layers of ARIA standard GUNW products Jupyter Notebook demonstrates using ARIA-tools to extract and subset ARIA-S1-GUNW layers, and presents workflows for applying the included correction layers and generating displacement maps.</p>"},{"location":"guides/gunw_product_guide/#references","title":"References","text":"<p>Bekaert, David, et al. \"The ARIA-S1-GUNW: The ARIA Sentinel-1 Geocoded Unwrapped Phase Product for Open InSAR Science  and Disaster Response.\" IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium. IEEE (2023).</p> <p>Buzzanga, Brett, et al. \"Toward sustained monitoring of subsidence at the coast using InSAR and GPS: An application  in Hampton Roads, Virginia.\" Geophysical Research Letters 47.18 (2020): e2020GL090013.</p> <p>Liang, Cunren, et al. \"Ionospheric correction of InSAR time series analysis of C-band Sentinel-1 TOPS data.\"  IEEE Transactions on Geoscience and Remote Sensing 57.9 (2019): 6755-6773.</p> <p>Yunjun, Zhang, et al. \"Range geolocation accuracy of C-/L-band SAR and its implications for operational stack  coregistration.\" IEEE Transactions on Geoscience and Remote Sensing 60 (2022): 1-19.</p>"},{"location":"guides/insar_product_guide/","title":"Sentinel-1 InSAR Product Guide","text":"<p>This document is a guide for users of Interferometric Synthetic Aperture Radar (InSAR) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request InSAR products On Demand in ASF's Vertex data portal, or make use of our HyP3 Python SDK or API. </p> <p>This process requires Sentinel-1 IW SLC products as input. Input pairs can be selected in Vertex using either the Baseline Tool or the SBAS Tool search interfaces. On Demand InSAR products only include co-polarized interferograms (VV or HH). Cross-polarized interferograms (VH or HV) are not available using this service.</p> <p>For a step-by-step tutorial on ordering On-Demand InSAR Products using Vertex, visit our InSAR On Demand! StoryMap. To learn more about the files included in the On Demand InSAR product packages and how to work with them, refer to our Exploring Sentinel-1 InSAR StoryMap.</p> <p>InSAR processing requires a Digital Elevation Model (DEM) for the removal of topographic phase. We use the GLO-30 Copernicus DEM when processing our On Demand InSAR products. Refer to the Prepare the DEM File section for more information. </p> <p>On Demand Burst-Based InSAR Now Available</p> <p>ASF also offers burst-based Sentinel-1 InSAR products. This On Demand processing option allows users to submit InSAR jobs for individual SLC bursts rather than the full Sentinel-1 IW SLC products. Refer to our Sentinel-1 Burst InSAR Product Guide for more information on this option.</p> <p>Users are cautioned to read the sections on limitations and error sources in InSAR products before attempting to use InSAR data. For a more complete description of the properties of SAR, see our Introduction to SAR guide. </p>"},{"location":"guides/insar_product_guide/#introduction","title":"Introduction","text":"<p>Interferometric Synthetic Aperture Radar (InSAR) processing uses two SAR images collected over the same area to determine geometric properties of the surface. Missions such as Sentinel-1 are designed for monitoring surface deformation using InSAR, which is optimal when acquisitions are made from a consistent location in space (short perpendicular baseline) over regular time intervals.</p> <p>The phase measurements of two SAR images acquired at different times from the same place in orbit are differenced to detect and quantify surface changes, such as deformation caused by earthquakes, volcanoes, or groundwater subsidence.</p> <p>InSAR can also be used to generate digital elevation models, but the optimal mission for DEM generation has the opposite characteristics of the Sentinel-1 mission. Topography is best mapped when the two acquisitions are obtained as close together as possible in time (short temporal baseline), but from different vantage points in space (larger perpendicular baseline than would be optimal for deformation mapping).</p>"},{"location":"guides/insar_product_guide/#brief-overview-of-insar","title":"Brief Overview of InSAR","text":"<p>SAR is an active sensor that transmits pulses and listens for echoes. These echoes are recorded in phase and amplitude, with the phase being used to determine the distance from the sensor to the target and the amplitude yielding information about the roughness and dielectric constant of that target.</p> <p></p> <p>Figure 1: Two passes of an imaging SAR taken at time T<sub>0</sub> and T<sub>0</sub> + \u2206t, will give two distances to the ground, R<sub>1</sub> and R<sub>2</sub>.  A difference between R<sub>1</sub> and R<sub>2</sub> shows motion on the ground.  In this case, a subsidence makes R<sub>2</sub> greater than R<sub>1</sub>.  Credit: TRE ALTAMIRA</p> <p>InSAR exploits the phase difference between two SAR images to create an interferogram that shows where the phase and, therefore, the distance to the target has changed from one pass to the next, as illustrated in Figure 1.  There are several factors that influence the interferogram, including earth curvature, topographic effects, atmospheric delays, surface motion, and noise.  With proper processing, Sentinel-1 InSAR can be used to detect changes in the earth's surface down to the centimeter scale. Applications include volcanic deformation, subsidence, landslide detection, and earthquake assessment.</p>"},{"location":"guides/insar_product_guide/#wavelengths","title":"Wavelengths","text":"<p>The SAR sensors on the Sentinel-1 satellites transmit C-band signals, with a wavelength of 5.6 cm. The signal wavelength impacts the penetration capability of the signal, so it is important to be aware of the sensor wavelength when working with SAR datasets. C-band SAR will penetrate more deeply into canopy or surfaces than an X-band signal, but not nearly as deep as an L-band SAR signal, which, with a wavelength on the order of 25 cm, is better able to penetrate canopy and return signals from the forest floor.</p> <p>Different wavelengths are also sensitive to different levels of deformation. To detect very small changes over relatively short periods of time, you may require a signal with a smaller wavelength (such as X-band). However, signals with shorter wavelengths are also more prone to decorrelation due to small changes in surface conditions such as vegetation growth.</p> <p>For slower processes that require a longer time interval to detect movement, longer wavelengths (such as L-band) may be necessary. C-band sits in the middle. It can detect fairly small changes over fairly short periods of time, but is not as sensitive to small changes as X-band or as able to monitor surface dynamics under canopy as L-band.</p>"},{"location":"guides/insar_product_guide/#polarizations","title":"Polarizations","text":"<p>Polarization refers to the direction of travel of an electromagnetic wave.  A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged.</p> <p>Most modern SAR systems can transmit chirps with either a horizontal or vertical polarization. In addition, some of these sensors can listen for either horizontal or vertical backscatter. This results in the potential for 4 different types of returns: HH, HV, VV, and VH, with the first letter indicating the transmission method and the second the receive method. For example, VH is a vertically polarized transmit signal with horizontally polarized echoes recorded.</p> <p>For InSAR applications, processing is generally performed on the co-pol (VV or HH) data and not on the cross-pol (VH or HV) data. Each image used in an InSAR pair must be the same polarization - two HH acquisitions of the same area could form a valid pair, and two VV acquisitions of the same area could form a valid pair, but you cannot pair an HH acquisition with a VV acquisition to generate an interferogram.</p> <p>On Demand InSAR products only include co-polarized interferograms. Cross-polarized interferograms are not available using this service.</p>"},{"location":"guides/insar_product_guide/#baselines","title":"Baselines","text":""},{"location":"guides/insar_product_guide/#perpendicular-baseline","title":"Perpendicular Baseline","text":"<p>The term baseline refers to the physical distance between the two vantage points from which images used as an InSAR pair are acquired. The baseline is decomposed into perpendicular (also called normal) and parallel components, as shown in Figure 2.</p> <p>To monitor surface deformation, the perpendicular baseline for the two acquisitions should be very small in order to maximize the coherence of the phase measurements.</p> <p>In order to determine topography, two slightly different vantage points are required. Sensitivity to topography depends on the perpendicular baseline, the sensor wavelength, the distance between the satellite and the ground, and the sensor look angle.</p> <p></p> <p>Figure 2: Geometry of InSAR baselines. Two satellite passes image the same area on the ground from positions S<sub>1</sub> and S<sub>2 </sub>, resulting in a baseline of B, which can be decomposed into perpendicular (B<sub>\u27c2 </sub>) and parallel (B<sub>\u2225 </sub>) components. Here Y is the direction of travel, referred to as the along-track or azimuth direction, and X is the direction perpendicular to motion, referred to as the cross-track or range direction. Credit: ASF</p>"},{"location":"guides/insar_product_guide/#temporal-baseline","title":"Temporal Baseline","text":"<p>In contrast to the (physical) baseline, the temporal baseline refers to the time separation between imaging passes. Along-track interferometry measures motion in the millisecond to second range. This technique can detect ocean currents and rapidly moving objects like boats. Differential interferometry is the standard method used to detect motion in the range of days to years. This is the type of interferometry that is performed by the Sentinel-1 HyP3 InSAR processing algorithm. Table 1 lists different temporal baselines, their common names, and what they can be used to measure.</p> Duration Known as Measurement of ms to sec along-track ocean currents, moving object detection, MTI days differential glacier/ice fields/lava flows, surface water extent, hydrology days to years differential subsidence, seismic events, volcanic activity, crustal displacement <p>Table 1: Temporal baselines and what they measure. Different geophysical phenomena can be detected based upon the temporal baseline. In general, the longer the temporal baseline, the smaller the motion that can be detected.</p>"},{"location":"guides/insar_product_guide/#critical-baseline","title":"Critical Baseline","text":"<p>Large baselines are better than small for topographic mapping. However, as the baseline increases, coherence decreases. At some point, it is impossible to create an interferogram because of baseline decorrelation. The maximum viable baseline per platform, referred to as the critical baseline, is a function of the distance to the ground, the wavelength, and the viewing geometry of the platform.</p> <p>For Sentinel-1, this critical baseline is about 5 km. In practice, if the perpendicular baseline between images is more than 3/4 of the critical baseline, interferogram creation will be problematic due to the level of noise.</p> <p>For deformation mapping, it is best to minimize the perpendicular baseline whenever possible, but there may be tradeoffs in terms of finding suitable temporal baselines. In most cases, however, pairs selected for deformation mapping will have perpendicular baselines much smaller than the critical baseline.</p>"},{"location":"guides/insar_product_guide/#ordering-on-demand-insar-products","title":"Ordering On Demand InSAR Products","text":"<p>All of ASF's On Demand InSAR products are generated using the  HyP3 platform.  Jobs can be submitted for processing using the  Vertex data portal, the  HyP3 Python SDK  or the HyP3 API.</p> <p>InSAR Processing Now Supports Sentinel-1C!</p> <p>GAMMA and ISCE2 software have both been updated to support Sentinel-1C acquisitions as input for InSAR processing.  Users can now use any Sentinel-1 IW SLCs in the archive, including those acquired by Sentinel-1C, as input for  either On Demand InSAR or On Demand Burst InSAR  processing.</p>"},{"location":"guides/insar_product_guide/#vertex","title":"Vertex","text":"<p>InSAR pairs are selected in Vertex using either the Baseline Search or the SBAS Search interface. The process of selecting pairs is the same for both IW SLC products and individual SLC bursts, but you will need to select the appropriate dataset when searching for content. As illustrated below, select the Sentinel-1 option in the Dataset menu to search for IW SLC products, and select the S1 Bursts option to search for individual SLC bursts.</p> <p></p> <p>The Baseline tool is the best option for selecting specific InSAR pairs. Use the Geographic Search to find an image that covers your time and area of interest, select that item in the results, and click the Baseline button in the center panel. The Baseline tool then displays all of the scenes that could be used to generate an interferogram using the selected image. Scroll through the results to find pairs to add to the On Demand queue, or click on items displayed in the plot to highlight that particular image pair.</p> <p>The SBAS tool is designed for generating time series of InSAR pairs. As with the Baseline search, you can launch the SBAS search from the center panel of a Geographic Search result. It will display all of the valid InSAR pairs through time based on the acquisition location of the input scene. This functionality is designed for processing a series of interferograms to be used in SBAS (Small BAseline Subset) analysis. The results can be adjusted based on baseline criteria (both perpendicular and temporal), and restricted to specific periods of time. Once the list is refined, you have the option to add all of the InSAR pairs displayed in the results to the On Demand queue.</p>"},{"location":"guides/insar_product_guide/#hyp3-sdk-and-api","title":"HyP3 SDK and API","text":"<p>The HyP3 SDK and API provide support for creating interferograms based on a pair of selected granules. To identify granules you'd like to process, we suggest using the Geographic, Baseline and SBAS search tools in Vertex. If you'd prefer to request interferogram processing programmatically, we suggest using Vertex's companion Python package: <code>asf_search</code>. This HyP3 SDK Jupyter Notebook provides you with an example of how you can use the <code>asf_search</code> and <code>hyp3_sdk</code> packages together to identify and create stacks of InSAR products.</p>"},{"location":"guides/insar_product_guide/#considerations-for-selecting-an-insar-pair","title":"Considerations for Selecting an InSAR Pair","text":"<p>When selecting an InSAR pair, observe the following required conditions:</p> <ol> <li>Images from an identical orbit direction (either ascending or descending)</li> <li>Images with identical incidence angles and beam mode</li> <li>Images with identical resolution and wavelength (usually from the same sensor)</li> <li>Images with the same viewing geometry (same path and frame)</li> <li>Images with identical polarizations (both HH or VV)</li> </ol> <p>In addition, the following suggestions may be helpful:</p> <ol> <li>Use images from similar seasons/growth/weather conditions</li> <li>For deformation mapping: limited spatial separation of acquisition locations (small physical baseline)</li> <li>For topographic mapping: limited time separation between images (small temporal baseline)</li> </ol> <p>To analyze deformation caused by a single discrete event, such as an earthquake, select images that bracket the event as closely in time as possible. Keeping the window narrowly focused on the time of the event will reduce the impacts of other processes that may mask the signal of the event of interest.</p>"},{"location":"guides/insar_product_guide/#processing-options-and-optional-files","title":"Processing Options and Optional Files","text":"<p>There are several options users can set when ordering InSAR On Demand products, including setting some processing parameters and selecting additional files to include in the output product package.</p> <p>New: Adaptive Phase Filter parameter is now customizable!</p> <p>There is now an option to adjust the adaptive phase filter parameter value when submitting On Demand InSAR jobs. This option is available in  Vertex,  as well as in the HyP3 API and Python SDK!  Refer to the Adaptive Phase Filter section for more information.</p> <p>Connected Components file not available for GAMMA-generated InSAR products from ASF</p> <p>ASF uses GAMMA software's Minimum Cost Flow (MCF) algorithm to phase unwrap full-scene  Sentinel-1 InSAR products. This workflow does not generate a connected components file, such as what is generated  when using SNAPHU for phase unwrapping.</p> <p>If you require connected components files for your analysis, consider using ASF's  Burst InSAR On Demand option, which uses the ISCE2 software package  to process individual Sentinel-1 SLC bursts to InSAR products. The  Burst InSAR product package contains a connected components file.</p>"},{"location":"guides/insar_product_guide/#processing-options","title":"Processing Options","text":"<p>When submitting jobs for processing, there are a number of parameters that can be set by the user.</p>"},{"location":"guides/insar_product_guide/#number-of-looks","title":"Number of Looks","text":"<p>The number of looks drives the resolution and pixel spacing of the output products. Selecting 10x2 looks will yield larger products with 80 m resolution and pixel spacing of 40 m. Selecting 20x4 looks reduces the resolution to 160 m and reduces the size of the products (roughly 1/4 the size of 10x2 look products), with a pixel spacing of 80 m. The default is 20x4 looks.</p>"},{"location":"guides/insar_product_guide/#adaptive-phase-filter","title":"Adaptive Phase Filter","text":"<p>When ordering InSAR On Demand products, users can choose to set a custom value for the Goldstein-Werner adaptive phase filter (adf). This filter improves fringe visibility and reduces phase noise in interferograms, particularly for InSAR pairs with low coherence. The filter impacts both wrapped and unwrapped interferograms, as well as the optional displacement maps generated from the unwrapped interferogram.</p> <p>Users can set the adaptive phase filter parameter (\ud835\udefc) value within the range of 0 to 1, with 0 indicating that no filtering occurs, and 1 indicating the strongest level of filtering. The default value is 0.6. The value should generally be greater than 0.2, and interferograms with very low coherence will benefit from higher values (closer to 1). Setting this value to 0 will result in no filter being applied.</p> <p>The filter is applied adaptively, meaning that regions with high coherence (where fringe patterns are easily discernible) will have more smoothing applied, while regions with low coherence will undergo less smoothing, so as not to remove residues that may represent actual deformation signals. Using this filter approach allows more of the interferogram to be unwrapped. For more information, refer to Goldstein and Werner's 1998 paper, Radar interferogram filtering for geophysical applications.</p>"},{"location":"guides/insar_product_guide/#apply-water-mask","title":"Apply Water Mask","text":"<p>There is an option to apply a water mask. This mask includes coastal waters and most inland waterbodies. Masking waterbodies can have a significant impact during the phase unwrapping, as water can sometimes exhibit enough coherence between acquisitions to allow for unwrapping to occur over waterbodies, which is invalid. </p> <p>A GeoTIFF of the water mask is always included with the InSAR product package, but when this option is selected, the conditional water mask will be applied along with coherence and intensity thresholds during the phase unwrapping process. Water masking is turned off by default. </p> <p>The water mask is generated by ASF using data from OpenStreetMap and/or ESA WorldCover depending on location. Areas within Canada, Alaska, and Russia are primarily covered by ESA WorldCover data, while the rest of the world is covered by OpenStreetMap data. Refer to the Water Masking documentation page for more details. </p> <p>This water mask is available for all longitudes, but data is only available from -85 to 85 degrees latitude. All areas between 85 and 90 degrees north latitude are treated as water, and all areas between 85 and 90 degrees south latitude are treated as land for the purposes of the water mask. </p> <p>Water masks were previously generated from the Global Self-consistent, Hierarchical, High-resolution Geography Database (GSHHG) dataset, but we transitioned to using the OpenStreetMap/ESA WorldCover datasets in February 2024 to improve performance. In addition to being a more recent and accurate dataset, this also allows us to mask most inland waterbodies. When using the GSHHG dataset, we only masked large inland waterbodies; with the new mask, all but the smallest inland waterbodies are masked.</p> <p>We originally applied a 3 km buffer on coastlines and a 5 km buffer on the shorelines of inland waterbodies in the water mask dataset before using it to mask the interferograms, in an effort to reduce the chance that valid land pixels would be excluded from phase unwrapping. It appears, however, that the inclusion of more water pixels is more detrimental to phase unwrapping than the exclusion of some land pixels, so as of September 27, 2022, the water mask used for this option is no longer buffered. </p> <p>Visit our InSAR Water Masking Tutorial for more information about how different water masking approaches can impact the quality of an interferogram.</p>"},{"location":"guides/insar_product_guide/#optional-files","title":"Optional Files","text":"<p>In addition to the processing options, users can choose to add a number of ancillary files to the product package. These files are not included by default, as they increase the size of the product package and may not be of interest to all users.</p> <p>In Vertex, check the box in the \"Include\" section of the options to add these optional files to the product package. When using the HyP3 API or SDK, set the parameter to true.</p> <ol> <li> <p>The look vectors are stored in two files. The look vector refers the look direction back towards the sensor. The lv_theta (\u03b8) indicates the SAR look vector elevation angle at each pixel, ranging from -\u03c0/2 (down) to \u03c0/2 (up). The look vector elevation angle is defined as the angle between the horizontal surface and the look vector with positive angles indicating sensor positions above the surface. The lv_phi (\u03c6) map indicates the SAR look vector orientation angle at each pixel, ranging from -\u03c0 (west) to \u03c0 (west). The look vector orientation angle is defined as the angle between the East direction and the projection of the look vector on the horizontal surface plane. The orientation angle increases towards north, with the North direction corresponding to \u03c0/2 (and south to -\u03c0/2). Both angles are expressed in radians. The default is to not include these files in the output product bundle.</p> </li> <li> <p>The displacement maps convert the phase difference values from the unwrapped interferogram into measurements of ground displacement in meters. The line-of-sight displacement map indicates the amount of movement away from or towards the sensor. The vertical displacement calculates the vertical component of the line-of-sight displacement, using the assumption that all deformation is in the vertical direction. These files are excluded from the product package by default.</p> </li> <li> <p>The wrapped phase GeoTIFF can be included in the output package. The browse version of this GeoTIFF (_color_phase.png) is always included, but the GeoTIFF version is not included by default. The specific color ramp displayed in the png is most valuable for many users, but some may wish to work with the actual wrapped phase values.</p> </li> <li> <p>The incidence angle maps indicate the angle of the radar signal. The local incidence angle is defined as the angle between the incident radar signal and the local surface normal, expressed in radians, while the ellipsoid incidence angle indicates the angle between the incident radar beam and the direction perpendicular to the WGS84 ellipsoid model. These files are excluded from the product package by default.</p> </li> <li> <p>A copy of the DEM used for processing can optionally be included in the product package. The file has been projected to a UTM Zone coordinate system, and pixel values indicate the elevation in meters. The elevation values will differ from the original Copernicus DEM GLO-30 dataset, as a geoid correction has been applied. The source DEM is also downsampled to twice the pixel spacing of the output product to smooth it for use in processing, then resampled again to match the pixel spacing of the InSAR product. The DEM is excluded by default.</p> </li> </ol>"},{"location":"guides/insar_product_guide/#insar-workflow","title":"InSAR Workflow","text":"<p>The InSAR workflow used in HyP3 was developed by ASF using GAMMA software. The steps include pre-processing steps, interferogram preparation, and product creation. Once these steps are performed, an output product package will be created. See product packaging for details on the individual files included in the package.  </p>"},{"location":"guides/insar_product_guide/#pre-processing","title":"Pre-Processing","text":"<p>Pre-processing steps prepare the SAR images to be used in interferometry. The pre-processing steps include image selection, ingest (including calibration), creation of a suitable DEM, and calculation of the burst overlap.</p>"},{"location":"guides/insar_product_guide/#select-an-insar-pair","title":"Select an InSAR Pair","text":"<p>Although it is possible to start from RAW data, Sentinel-1 InSAR processing is typically done using Interferometric Wide swath Single Look Complex (IW SLC) data as the input.  This means that the data has been formed into an image through SAR processing, but has not been multi-looked.  </p> <p>The SLC pair is defined by the user, either through the Vertex interface, or using the HyP3 API or SDK. To ensure consistency, the older SLC image is always used as the reference image, and the younger SLC image is always used as the secondary image. This means that positive values in the resulting unwrapped interferogram represent movement away from the SAR platform and negative values represent movement towards the SAR platform. However, these values are relative to the reference point of the unwrapped interferogram. See the phase unwrapping section for more details.</p>"},{"location":"guides/insar_product_guide/#ingest-slc-data-into-gamma-format","title":"Ingest SLC data into GAMMA format","text":"<p>Once the InSAR pair has been identified, the selected SLC data are ingested into GAMMA internal format. This is performed by the GAMMA program par_s1_slc. GAMMA format has raw data files (only data, no headers or line leaders) with metadata stored in external files with a .par extension.  </p> <p>During ingest into GAMMA's internal format, the SLC data is calibrated by applying the calibration coefficients that are supplied with each product. This process puts the SAR backscatter into a known scale where the diffuse volume scattering of the Amazon rainforest is a constant -6.5 dB.</p> <p>Immediately after ingesting the SLC, the state vectors are updated to use the best available state vectors. The state vector types in order of absolute correctness are original predicted (O), restituted (R), and precision (P). In practice, one will never receive an InSAR product that uses the original predicted orbit - only granules for which a restituted or precision orbit is available can be used in HyP3 InSAR processing. The orbit type used for generating the InSAR product is indicated in the product filename, as shown in Figure 3.</p> <p></p> <p>Figure 3: Position of the orbit type in the HyP3 product name. </p>"},{"location":"guides/insar_product_guide/#prepare-the-dem-file","title":"Prepare the DEM File","text":"<p>In order to create differential InSAR products that show motion on the ground, one must subtract the topographic phase from the interferogram. The topographic phase, in this case, is replicated by using an existing DEM to calculate the actual topographic phase. This phase is then removed from the interferogram leaving just the motion or deformation signal (plus atmospheric delays and noise).</p> <p>The DEM that is used for HyP3 InSAR processing is the 2022 Release of the Copernicus GLO-30 Public DEM dataset publicly available on AWS. </p> <p>The Copernicus DEM provides higher-quality products over a wider area than the older DEMs (SRTM and NED) previously used to generate ASF's On Demand products. Refer to our Digital Elevation Model Documentation for more information. The Copernicus DEM provides global coverage at 30-m pixel spacing, except for areas over Armenia and Azerbaijan. These gaps in coverage are filled with the Copernicus GLO-90 Public DEM, which has 90-m pixel spacing. </p> <p>The DEM tiles necessary to cover the input granules for the InSAR product are downloaded. A geoid correction is applied to the DEM, and it is resampled to match the output resolution of the InSAR product (160 m for 20x4 products, 80 m for 10x2 products) and projected to the appropriate UTM Zone for the granule location.</p>"},{"location":"guides/insar_product_guide/#calculate-overlapping-bursts","title":"Calculate Overlapping Bursts","text":"<p>The IW SLC Sentinel-1 data comes in three sub-swaths. However, a further subdivision is made in the data, wherein bursts occur. Bursts are the fundamental building block for Sentinel-1 imagery. Each one is a portion of the final image, around 1500 lines long and one sub-swath width wide. Thus, the more busts, the longer the file is in length.</p> <p>Each burst is precisely timed to repeat at a given time interval. This consistent repeat combined with precise velocity control gives rise to the fact that the bursts start at the same time on each pass around the globe. </p> <p>For example, a burst images a piece of the Gal\u00e1pagos Islands. The next time that same piece of the island is imaged, the time of day will be the same, to within few milliseconds. Only the frames containing overlapping bursts can be used to perform InSAR processing. This means that if there is no burst overlap in the pair selected as input, the InSAR process will not run.</p> <p>Repeatable burst timing is exploited by HyP3 in order to calculate the bursts that overlap between two scenes.  These overlapping bursts are the only ones used in the rest of the InSAR process. The rest are discarded.</p>"},{"location":"guides/insar_product_guide/#interferogram-creation-co-registration-and-refinement","title":"Interferogram Creation, Co-registration and Refinement","text":"<p>Before the interferogram is created, the lookup table that maps from the SLC image space into a ground range image space is created. At this time, the interferogram of the topography is simulated using the previously prepared DEM.</p> <p>Once these steps have been performed, the two SLCs are co-registered to within 0.02 pixels. This is done by iteratively using the following steps:</p> <ol> <li>Resample the secondary SLC using previously calculated offset polynomial</li> <li>Match the reference and secondary SLC images using intensity cross-correlation</li> <li>Estimate range and azimuth offset polynomial coefficients from results of matching</li> <li>Create a differential interferogram using the co-registered SLCs and the simulated interferogram</li> <li>Update offset polynomial by adding the current estimates</li> </ol> <p>Note that these steps are automatically run 4 times.  At that point, if the last offset calculated was more than 0.02 pixels, then the procedure will fail to complete.</p> <p>Provided the images passed the check for convergence, the next co-registration step employs the Enhanced Spectral Diversity (ESD) algorithm to match the two scenes to better than 1/100th of a pixel. This is accomplished by examining the overlap area between subsequent bursts. If there is even a small offset, the phase between the bursts will not match. This phase mismatch is then used to calculate the corresponding azimuth offset.</p> <p>To finish interferogram processing, steps 1 through 4 are run once again, this time with the offsets from the ESD included. The output of this entire process is a wrapped interferogram.</p>"},{"location":"guides/insar_product_guide/#phase-unwrapping","title":"Phase Unwrapping","text":"<p>All of the phase differences in wrapped interferograms lie between -\u03c0 and \u03c0. Phase unwrapping attempts to assign multiples of 2\u03c0 to add to each pixel in the interferogram to restrict the number of 2\u03c0 jumps in the phase to the regions where they may actually occur. These regions are areas of radar layover or areas of deformation exceeding half a wavelength in the sensor's line of sight. Thermal noise and interferometric decorrelation can also result in these 2\u03c0 phase discontinuities called residues. </p> <p>The phase unwrapping algorithm used for these products is Minimum Cost Flow (MCF) and Triangulation. Refer to this Technical Report from GAMMA Remote Sensing for more information on the MCF phase unwrapping approach.</p> <p>Note that the MCF algorithm does not generate a connected components file. If you require this file, consider using ASF's Burst InSAR On Demand option, which includes a connected components file in each output product package.</p>"},{"location":"guides/insar_product_guide/#filtering","title":"Filtering","text":"<p>Before the interferogram can be unwrapped, it must be filtered to remove noise. This is accomplished using an adaptive spectral filtering algorithm. This adaptive interferogram filtering aims to reduce phase noise, increase the accuracy of the interferometric phase, and reduce the number of interferogram residues as an aid to phase unwrapping. In this case, residues are points in the interferogram where the sum of the phase differences between pixels around a closed path is not 0.0, which indicates a jump in phase.</p>"},{"location":"guides/insar_product_guide/#masking","title":"Masking","text":"<p>Another step before unwrapping is to create a validity mask to guide the phase unwrapping process. This mask is generated by applying thresholds to the coherence and/or amplitude (backscatter intensity) values for an image pair. For On Demand InSAR products, we set the amplitude threshold to be 0.0 (in power scale), so that data is only excluded based on the coherence threshold.</p> <p>Coherence is estimated from the normalized interferogram. The pixel values in this file range from 0.0 (total decorrelation) to 1.0 (perfectly coherent). Any input pixel with a coherence value less than 0.1 is given a validity mask value of zero and not used during unwrapping.</p> <p>Change to Validity Mask Thresholds</p> <p>In the past, we also used an amplitude threshold of 0.2 (in power scale) when generating the validity mask. While this approach tends to mask out inland waters, providing a less noisy interferogram in some cases, it also masks arid regions that have low amplitude values but reasonably high coherence. As of March 2022, we have set the amplitude threshold to 0.0, so that coherence is the only driver of the validity mask.</p> <p>In some cases, pixels over water may still meet the coherence threshold criteria for inclusion, even though they are not valid for use during phase unwrapping. When the water masking option is applied, the validity mask is further amended to apply 0 values to any pixels classified as water in the water mask. </p> <p>When processing scenes with extensive coverage by coastal waters or large inland waterbodies, there can be erroneous deformation signals or phase jumps introduced if unwrapping proceeds over water as if it were land. In such cases, choosing the option to apply the water mask can significantly improve the results. Refer to our InSAR Water Masking Tutorial for more information.</p>"},{"location":"guides/insar_product_guide/#reference-point","title":"Reference point","text":"<p>In order to perform phase unwrapping, a reference point must be selected. The unwrapping will proceed relative to this reference point; the 2\u03c0 integer multiples will be applied to the wrapped phase using this pixel as the starting point. The unwrapped phase value is set to 0 at the reference point.</p> <p>Ideally, the reference point for phase unwrapping would be located in an area with high coherence in a stable region close to an area with surface deformation. Choosing an optimal reference point requires knowledge of the site characteristics and examination of the interferogram, which is not practical in an automated, global workflow. </p> <p>By default, ASF's On Demand InSAR products use the location of the pixel with the highest coherence value as the reference point. The coherence map is examined to determine the maximum value, and all pixels with this value are examined using a 9-pixel window. The pixel with the highest sum of values within its 9-pixel window is selected as the reference point. If more than one pixel has the same 9-pixel sum, the pixel closest to the origin pixel (bottom left corner for ascending scenes, top right corner for descending scenes) is selected.</p> <p>This may be an appropriate reference point location in many cases, as it meets the criteria of having high coherence, and stable areas have higher coherence than areas undergoing significant deformation. If a user wants to set a different location as the phase unwrapping reference point, however, a correction can be applied to the unwrapped interferogram.</p> <p>The location of the reference point is included in the product readme file, as well as the parameter metadata text file,  both of which are included in the product package by default.</p> <p>For more information on the impact of the phase unwrapping reference point location on unwrapped phase and displacement measurements, refer to the Limitations section of this document, which also includes instructions for applying a correction based on a custom reference point. </p>"},{"location":"guides/insar_product_guide/#geocoding-and-product-creation","title":"Geocoding and Product Creation","text":"<p>After the phase is unwrapped, the final steps are geocoding and product creation.  </p>"},{"location":"guides/insar_product_guide/#geocoding","title":"Geocoding","text":"<p>Geocoding is the process of reprojecting pixels from SAR slant range space (where all the calculations have been performed) into map-projected ground range space (where analysis of products is simplest). Using the look up table previously computed, this process takes each pixel in the input product and relocates it to the UTM zone of the DEM used in processing. This is accomplished using nearest-neighbor resampling so that original pixel values are preserved.</p>"},{"location":"guides/insar_product_guide/#product-creation","title":"Product Creation","text":"<p>Files are next exported from GAMMA internal format into the widely-used GeoTIFF format, complete with geolocation information. GeoTIFFs are created for amplitude, coherence, and unwrapped phase by default, and a water mask GeoTIFF is also included in the product package. Optionally, GeoTIFFs of wrapped phase, look vectors, displacement maps (line-of-sight and vertical), and incidence angle maps can be included, as can a copy of the DEM used for processing.</p>"},{"location":"guides/insar_product_guide/#product-packaging","title":"Product Packaging","text":"<p>HyP3 InSAR output is a zip file containing various files, including GeoTIFFs, PNG browse images with geolocation information, Google Earth KMZ files, a metadata file, and a README file.</p>"},{"location":"guides/insar_product_guide/#naming-convention","title":"Naming Convention","text":"<p>The InSAR product names are packed with information pertaining to the processing of the data, presented in the  following order, as illustrated in Figure 4. </p> <ul> <li>The platform names, one of Sentinel-1A, Sentinel-1B, or Sentinel-1C, are abbreviated with the letters <code>A</code>, <code>B</code>, or <code>C</code><ul> <li>Two of these letters follow <code>S1</code>, indicating the platform(s) used to acquire the reference and    secondary images, in that order (<code>S1AA</code>, <code>S1BA</code>, <code>S1AC</code>, etc.)</li> </ul> </li> <li>The reference start date and time and the secondary start date and time, with the date and time    separated by the letter T</li> <li>The polarizations for the pair, either HH or VV, the orbit type, and the days of separation for the pair</li> <li>The product type (always INT for InSAR) and the pixel spacing, which will be either 80 or 40, based upon the    number of looks selected when the job was submitted for processing</li> <li>The software package used for processing is always GAMMA for GAMMA InSAR products</li> <li>User-defined options are denoted by three characters indicating whether the product is water masked (w) or not (u),    the scene is clipped (e for entire area, c for clipped), and whether a single sub-swath was processed or the entire    granule (either 1, 2, 3, or F for full swath)<ul> <li>Currently, only the water masking is available as a user-selected option; the products always include the    full granule extent with all three sub-swaths</li> </ul> </li> <li>The filename ends with the ASF product ID, a 4 digit hexadecimal number</li> </ul> <p></p> <p>Figure 4: Breakdown of ASF InSAR naming scheme.</p>"},{"location":"guides/insar_product_guide/#image-files","title":"Image Files","text":"<p>All of the main InSAR product files are 32-bit floating-point single-band GeoTIFFs. To learn more about the rasters included in the product package, refer to the Exploring Sentinel-1 InSAR StoryMap tutorial.</p> <ul> <li>The amplitude image is the calibrated radiometric backscatter from the reference granule in sigma-nought power. The image is terrain corrected using a geometric correction, but not radiometrically corrected. </li> <li>The coherence file pixel values range from 0.0 to 1.0, with 0.0 being completely non-coherent and 1.0 being perfectly coherent. </li> <li>The unwrapped phase file shows the results of the phase unwrapping process. Negative values indicate movement towards the sensor, and positive values indicate movement away from the sensor. This is the main interferogram output.</li> <li>The wrapped phase file indicates the interferogram phase after applying the adaptive filter immediately before unwrapping. Values range from negative pi to positive pi. (optional)</li> <li>The line-of-sight displacement file indicates the displacement in meters along the look direction of the sensor. The sign is opposite to that of the unwrapped phase: positive values indicate motion towards the sensor and negative values indicate motion away from the sensor. (optional)</li> <li>The vertical displacement is generated from the line of sight displacement values, and makes the assumption that deformation only occurs in the vertical direction. Note that this assumption may not hold true in cases where the deformation also has a horizontal component. Positive values indicate uplift, and negative values indicate subsidence. (optional)</li> <li>The look vectors theta (\u03b8) and phi (\u03c6) describe the elevation and orientation angles of the sensor's look direction. (optional)</li> <li>The incidence angle maps indicate the angle between the incident signal and the surface normal of either the terrain (local incidence angle) or the ellipsoid (ellipsoid incidence angle). (optional)</li> <li>The DEM file gives the local terrain heights in meters, with a geoid correction applied. (optional)</li> <li>The water mask file indicates coastal waters and most inland waterbodies. Pixel values of 1 indicate land and 0 indicate water. This file is in 8-bit unsigned integer format.</li> </ul> <p>If the water mask option is selected, the water mask is applied prior to phase unwrapping to exclude water pixels from the process. The water mask is generated using the OpenStreetMap and ESA WorldCover datasets. Refer to the Water Masking Processing Option section and our InSAR Water Masking Tutorial for more information about water masking.</p> <p>Browse images are included for the wrapped (color_phase) and unwrapped (unw_phase) phase files, which are in PNG format and are each 2048 pixels wide. The browse images are displayed using a cyclic color ramp to generate fringes. </p> <ul> <li>Each fringe in a wrapped (color_phase) browse image represents a 2-pi phase difference, and the line-of-sight displacement for each fringe is equivalent to half the wavelength of the sensor. The wavelength of Sentinel-1 is about 5.6 cm, so each 2-pi fringe represents a line-of-sight displacement of about 2.8 cm.</li> <li>Each fringe in an unwrapped (unw_phase) browse image represents a phase difference of 6 pi. Because each 2-pi difference is equivalent to half the wavelength of the sensor, each 6-pi fringe represents about 8.3 cm of line-of-sight displacement for these Sentinel-1 products.</li> </ul> <p>KMZ files are included for the wrapped (color_phase) and unwrapped (unw_phase) phase images, which allow users to view the outputs in Google Earth or other platforms that support kmz files. </p> <p>The tags and extensions used and example file names for each raster are listed in Table 2 below. </p> Extension Description Example _amp.tif Amplitude S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_amp.tif _corr.tif Normalized coherence file S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_corr.tif _unw_phase.tif Unwrapped geocoded interferogram S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_unw_phase.tif _wrapped_phase.tif Wrapped geocoded interferogram S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_wrapped_phase.tif _los_disp.tif Line-of-sight displacement S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_los_disp.tif _vert_disp.tif Vertical displacement S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_vert_disp.tif _lv_phi.tif Look vector \u03c6 (orientation) S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_lv_phi.tif _lv_theta.tif Look vector \u03b8 (elevation) S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_lv_theta.tif _dem.tif Digital elevation model S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_dem.tif _inc_map_ell.tif Ellipsoid incidence angle S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_inc_map_ell.tif _inc_map.tif Local incidence angle S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_inc_map.tif _water_mask.tif Water mask S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_water_mask.tif _color_phase.kmz Wrapped phase kmz file S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_color_phase.kmz _unw_phase.kmz Unwrapped phase kmz file S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_unw_phase.kmz _color_phase.png Wrapped phase browse image S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_color_phase.png _unw_phase.png Unwrapped phase browse image S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_unw_phase.png <p>Table 2: Image files in product package</p>"},{"location":"guides/insar_product_guide/#metadata-files","title":"Metadata Files","text":"<p>The product package also includes a number of metadata files.</p> Extension Description Example .README.md.txt Main README file for GAMMA InSAR S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09.README.md.txt .txt Parameters and metadata for the InSAR pair S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09.txt .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_unw_phase.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_color_phase.png.xml .png.aux.xml Geolocation information for png browse images S1AB_20171111T150004_20171117T145926_VVP006_INT80_G_ueF_4D09_color_phase.png.aux.xml <p>Table 3: Metadata files in product package</p>"},{"location":"guides/insar_product_guide/#readme-file","title":"README File","text":"<p>The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with InSAR products should start by reading this README file, which will give some background on each of the files included in the product folder.</p>"},{"location":"guides/insar_product_guide/#insar-parameter-file","title":"InSAR Parameter File","text":"<p>The text file with extension .txt includes processing parameters used to generate the InSAR product as well as metadata attributes for the InSAR pair.  These are detailed in Table 4.  </p> Name Description Possible Value Reference Granule ESA granule name for reference scene (of the two scenes in the pair, the dataset with the oldest timestamp) S1A_IW_SLC__1SDV_20200116T032559_20200116T032627_030820_038928_F5DC Secondary Granule ESA granule name for secondary scene (of the two scenes in the pair, the dataset with the newest timestamp) S1B_IW_SLC__1SDV_20200128T032559_20200128T032627_030995_038F51_7D4F Reference Pass Direction Orbit direction of the reference scene DESCENDING Reference Orbit Number Absolute orbit number of the reference scene 30741 Secondary Pass Direction Orbit direction of the reference scene DESCENDING Secondary Orbit Number Absolute orbit number of the secondary scene 31091 Baseline Perpendicular baseline in meters 58.3898 UTCTime Time in the UTC time zone in seconds 12360.691361 Heading Spacecraft heading measured in degrees clockwise from north 193.2939317 Spacecraft height Height in meters of the spacecraft above nadir point 700618.6318999995 Earth radius at nadir Ellipsoidal earth radius in meters at the point directly below the satellite 6370250.0667 Slant range near Distance in meters from satellite to nearest point imaged 799517.4338 Slant range center Distance in meters from satellite to the center point imaged 879794.1404 Slant range far Distance in meters from satellite to farthest point imaged 960070.8469 Range looks Number of looks taken in the range direction 20 Azimuth looks Number of looks taken in the azimuth direction 4 InSAR phase filter Name of the phase filter used adf Phase filter parameter Dampening factor 0.6 Resolution of output (m) Pixel spacing in meters for output products 80 Range bandpass filter Range bandpass filter applied no Azimuth bandpass filter Azimuth bandpass filter applied no DEM source DEM used in processing GLO-30 DEM resolution Pixel spacing in meters for DEM used to process this scene 160 Unwrapping type Phase unwrapping algorithm used mcf Phase at Reference Point Original unwrapped phase value at the reference point (set to 0 in output unwrapped phase raster) -4.21967 Azimuth line of the reference point in SAR space Row number (in SAR space) of the reference point 2737.0 Range pixel of the reference point in SAR space Column number (in SAR space) of the reference point 739.0 Y coordinate of the reference point in the map projection Latitude of the reference point in projected coordinates (UTM Zone - meters) 4112453.3223 X coordinate of the reference point in the map projection Longitude of the reference point in projected coordinates (UTM Zone - meters) 589307.6248 Latitude of the reference point (WGS84) Latitude of the reference point in WGS84 Geographic Coordinate System (degrees) 37.1542125 Longitude of the reference point (WGS84) Longitude of the reference point in WGS84 Geographic Coordinate System (degrees) 40.00574707 Unwrapping threshold Minimum coherence required to unwrap a given pixel none Speckle filter Speckle filter applied no <p>Table 4: List of InSAR parameters included in the parameter text file</p>"},{"location":"guides/insar_product_guide/#arcgis-compatible-xml-files","title":"ArcGIS-Compatible XML Files","text":"<p>There is an ArcGIS-compatible XML file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated XML file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly.</p> <p>ArcGIS users should take care not to change these XML files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS.</p> <p>Those not using ArcGIS will still find the contents of these XML files useful, but will have to contend with the XML tagging when viewing the files as text or in a browser.</p>"},{"location":"guides/insar_product_guide/#auxiliary-geolocation-files","title":"Auxiliary Geolocation Files","text":"<p>Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms.</p>"},{"location":"guides/insar_product_guide/#data-access","title":"Data Access","text":"<p>Refer to the Downloads  page for more information on viewing and downloading On Demand InSAR products in Vertex or programmatically.  Once processing is complete, download links for On Demand products are valid for 14 days.</p> <p>Step-by-step instructions for finding and downloading InSAR On Demand products in Vertex are available in the  Submit/Download Jobs section of the  InSAR On Demand! interactive StoryMap tutorial.</p>"},{"location":"guides/insar_product_guide/#limitations","title":"Limitations","text":""},{"location":"guides/insar_product_guide/#baseline-calculation","title":"Baseline Calculation","text":"<p>The baseline is defined as the difference of the platform positions when a given area is imaged. HyP3 baselines are calculated using the best state vectors available. If precise orbits are not yet available for the input granules, restituted orbits will be used. The original predicted orbits are not used for InSAR processing in HyP3. If no restituted or precise state vectors are available, the process will not run.</p>"},{"location":"guides/insar_product_guide/#coherence","title":"Coherence","text":"<p>The phase measurements in the two images used in InSAR must be coherent in order to detect change. Random changes in phase from one acquisition to the next can mask actual surface deformation. Vegetation is a common driver of decorrelation, as changes can easily take place in the interval between two acquisitions due to growth, seasonal changes, or wind effects. It will be difficult to generate valid interferograms with C-band data in heavily vegetated regions due to lack of coherence even with fairly short time intervals.</p> <p>Consider seasonality when selecting image pairs. Decorrelation can be particularly high when comparing phase from different seasons. Changes in the condition of vegetation (especially deciduous canopies), snow, moisture, or freeze/thaw state can impact phase measurements. In cases where a temporal baseline is required that spans seasons, it may be better to use an annual interferogram if possible so that the images are more comparable in terms of seasonality.</p>"},{"location":"guides/insar_product_guide/#line-of-sight-measurements","title":"Line-of-Sight Measurements","text":"<p>When looking at a single interferogram, the deformation measurements in the line-of-sight orientation of the sensor indicate relative motion towards or away from the sensor. InSAR is not sensitive to motion in the azimuth direction of the satellite, so motion that occurs in the same direction as the satellite's direction of travel will not be detected.</p> <p>A single interferogram cannot be used to determine the relative contributions of vertical and horizontal movement to the line-of-sight displacement measurement. The vertical displacement map is generated based on the assumption that the movement is entirely in the vertical direction, which may not be realistic for some processes. To determine how much of the signal is driven by vertical vs. horizontal movement, you must either use a time series of interferograms, or use reference measurements with known vertical and horizontal components (such as GNSS measurements from the region of deformation) to deconstruct the line-of-sight displacement.</p> <p>All displacement values are calculated relative to a reference point, which may or may not be an appropriate benchmark for measuring particular areas of displacement within the interferogram.</p>"},{"location":"guides/insar_product_guide/#phase-unwrapping-reference-point","title":"Phase Unwrapping Reference Point","text":"<p>The reference point for phase unwrapping is set to be the location of the pixel with the highest coherence value. As described in the phase unwrapping section, this may not always be an ideal location to use as a reference point. If it is located in an area undergoing deformation, or in a patch of coherent pixels that is separated from the area undergoing deformation by a gap of incoherent pixels, the unwrapping may be of lower quality than if the reference point was in a more suitable location.</p> <p>Even when there are not phase unwrapping errors introduced by phase discontinuities, it is important to be aware that unwrapped phase differences and displacement values are all calculated relative to the reference point. The phase difference value of the reference point is set to 0 during phase unwrapping, so any displacement values will be relative to that benchmark. If the location of the default reference point is in the middle of an area that underwent deformation, displacement values may be different than expected.</p> <p>If you are interested in the amount of displacement in a particular area, you may wish to choose your own reference point. The ideal reference point would be in an area of high coherence beyond where deformation has occurred. The unwrapped phase measurements can be adjusted to be relative to this new reference point, and displacement values can be recalculated accordingly. To adjust the values in the unwrapped phase GeoTIFF, simply select a reference point that is optimal for your use case and subtract the unwrapped phase value of that reference point from each pixel in the unwrapped phase raster:</p> <p>\u0394\u03a8<sup>*</sup> = \u0394\u03a8 - \u0394\u03c8<sub>ref</sub></p> <p>where \u0394\u03a8<sup>*</sup> is the adjusted unwrapped phase, \u0394\u03a8 is the original unwrapped phase, and \u0394\u03c8<sub>ref</sub> is the unwrapped phase value at the new reference point.</p>"},{"location":"guides/insar_product_guide/#impacts-on-displacement-measurements","title":"Impacts on Displacement Measurements","text":"<p>The measurements in the displacement maps are calculated from the unwrapped phase values, so will similarly be impacted by the location of the reference point. You may wish to recalculate the displacement values relative to a new reference point. The approach for correcting the displacement maps will be different for the line-of-sight and vertical measurements.</p>"},{"location":"guides/insar_product_guide/#correcting-line-of-sight-displacement-maps","title":"Correcting Line-of-Sight Displacement Maps","text":"<p>If you have already corrected the unwrapped phase raster, you can calculate a new line-of-sight (LOS) displacement map by applying the following calculation on a pixel-by-pixel basis using the unwrapped phase GeoTIFF:</p> <p>\u0394\u03a9<sup>*</sup> = - \u0394\u03a8<sup>*</sup> \u03bb / 4\u03c0</p> <p>where \u0394\u03a9<sup>*</sup> is the adjusted line-of-sight displacement in meters, \u0394\u03a8<sup>*</sup> is the adjusted unwrapped phase, and \u03bb is the wavelength of the sensor in meters (0.055465763 for Sentinel-1).</p> <p>Setting the \u0394\u03a8<sup>*</sup> value to be negative reverses the sign so that the difference is relative to the earth rather than the sensor. A positive phase difference value indicates subsidence, which is unintuitive when thinking about movement on the earth's surface. Applying the negative will return positive displacement values for uplift and negative values for subsidence.</p> <p>If you are not interested in adjusted unwrapped phase values, you can also directly correct the LOS Displacement map included optionally in the InSAR product package:</p> <p>\u0394\u03a9<sup>*</sup> = \u0394\u03a9 - \u0394\u03c9<sub>ref</sub></p> <p>where \u0394\u03a9<sup>*</sup> is the adjusted line-of-sight displacement in meters, \u0394\u03a9 is the original line-of-sight displacement in meters, and \u0394\u03c9<sub>ref</sub> is the line-of-sight displacement value at the new reference point.</p>"},{"location":"guides/insar_product_guide/#correcting-vertical-displacement-maps","title":"Correcting Vertical Displacement Maps","text":"<p>Vertical displacement maps cannot be adjusted directly, and must be recalculated from the adjusted unwrapped phase image. You will also need the \u03b8 look vector map (lv_theta GeoTIFF) for this calculation. The look vector maps are not included in the InSAR product package by default; the option to Include Look Vectors must be selected when ordering the product.</p> <p>To calculate an adjusted vertical displacement raster, calculate the adjusted unwrapped phase, then apply the following:</p> <p>\u0394\u03d2<sup>*</sup> = - \u0394\u03a8<sup>*</sup> \u03bb cos(\u00bd\u03c0 - LV<sub>\u03b8</sub>) / 4\u03c0</p> <p>where \u0394\u03d2<sup>*</sup> is the adjusted vertical displacement in meters, \u0394\u03a8<sup>*</sup> is the adjusted unwrapped phase, \u03bb is the wavelength of the sensor in meters (0.055465763 for Sentinel-1), and LV<sub>\u03b8</sub> is the theta look vector (from the lv_theta GeoTIFF).</p> <p>As with the LOS Displacement maps, setting the \u0394\u03a8<sup>*</sup> value to be negative reverses the sign so that the difference is relative to the earth rather than the sensor. Applying the negative will return positive displacement values for uplift and negative values for subsidence.</p>"},{"location":"guides/insar_product_guide/#displacement-values-from-a-single-interferogram","title":"Displacement Values from a Single Interferogram","text":"<p>In general, calculating displacement values from a single interferogram is not recommended. While the displacement rasters provided with ASF's On Demand InSAR products can be helpful in visualizing changes, we do not recommend that you rely on a single interferogram when coming to conclusions about surface displacement, even if you apply a correction based on a manually selected reference point. It will be more robust to use a time series approach to more accurately determine the pattern of movement. When using SAR time-series software such as MintPy, you have the option to select a specific reference point, and the values of the input rasters will be adjusted accordingly.</p>"},{"location":"guides/insar_product_guide/#error-sources","title":"Error Sources","text":"<p>On Demand InSAR products do not currently correct for some common sources of error in interferometry, such as atmospheric effects. Further processing or time series analysis can be performed by the user to identify or reduce the impact of some of these errors when using On Demand InSAR products for analysis.</p>"},{"location":"guides/insar_product_guide/#atmospheric-delay","title":"Atmospheric Delay","text":"<p>While SAR signals can penetrate clouds, atmospheric conditions can delay the transmission of the signal. This results in phase differences that can look like surface deformation signals but are actually driven by differences in the atmospheric conditions between the pair of acquisitions used to generate the interferogram.</p> <p>In some cases, atmospheric errors can be corrected by using an atmospheric model to remove the impacts of the turbulent delay from the interferogram. Another approach is to use time series analysis to identify outliers.</p> <p>Always doubt your interferogram first! View the interferogram critically, and consider if fringe patterns could potentially be driven by atmospheric effects. In general, it is best to avoid drawing conclusions from the outcome of a single interferogram.</p> <p>Tropospheric phase may be less impactful when considering small-scale deformation. As such, if you are using ASF's Sentinel-1 Burst InSAR products to look at deformation signals that are smaller than 1 km\u00b2, you should consider using methods other than the typical atmospheric model-based corrections to remove the effects of atmospheric delay. Potential methods in this case include applying band-pass or high-pass spatial filters, or spatial averaging filters such as the approach outlined in Bekaert et al., 2020.</p>"},{"location":"guides/insar_product_guide/#turbulent-delay","title":"Turbulent Delay","text":"<p>These delays are generally caused by differences in water vapor distribution from one image to the next. They often manifest as wobbly or sausage-shaped fringes, and can potentially mask the signal of a small earthquake.</p>"},{"location":"guides/insar_product_guide/#stratified-delay","title":"Stratified Delay","text":"<p>This type of delay is driven mostly by pressure and temperature differences or gradients through the atmospheric column, and often correlates with topography. This atmospheric signature can be confused with movement caused by volcanic activity. If there are multiple volcanoes in an image, and they all exhibit similar patterns, the signal is likely being driven by this type of atmospheric delay.</p>"},{"location":"guides/insar_product_guide/#dem-errors","title":"DEM Errors","text":"<p>A DEM is used to remove topographic phase impacts, but if there are inaccuracies in the DEM, residual impacts of those errors can remain in the interferogram.</p>"},{"location":"guides/insar_product_guide/#orbit-uncertainties","title":"Orbit Uncertainties","text":"<p>This is generally not an issue for Sentinel-1 data, as the orbits are very precise and generally reliable. On Demand InSAR products are only processed once restituted or precise orbits are available. Orbit uncertainties are more problematic when working with datasets from older missions.</p>"},{"location":"guides/insar_product_guide_template/","title":"Insar product guide template","text":""},{"location":"guides/insar_product_guide_template/#introduction","title":"Introduction","text":"<p>Interferometric Synthetic Aperture Radar (InSAR) processing uses two SAR images collected over the same area to determine geometric properties of the surface. Missions such as Sentinel-1 are designed for monitoring surface deformation using InSAR, which is optimal when acquisitions are made from a consistent location in space (short perpendicular baseline) over regular time intervals.</p> <p>The phase measurements of two SAR images acquired at different times from the same place in orbit are differenced to detect and quantify surface changes, such as deformation caused by earthquakes, volcanoes, or groundwater subsidence.</p> <p>InSAR can also be used to generate digital elevation models, but the optimal mission for DEM generation has the opposite characteristics of the Sentinel-1 mission. Topography is best mapped when the two acquisitions are obtained as close together as possible in time (short temporal baseline), but from different vantage points in space (larger perpendicular baseline than would be optimal for deformation mapping).</p>"},{"location":"guides/insar_product_guide_template/#brief-overview-of-insar","title":"Brief Overview of InSAR","text":"<p>SAR is an active sensor that transmits pulses and listens for echoes. These echoes are recorded in phase and amplitude, with the phase being used to determine the distance from the sensor to the target and the amplitude yielding information about the roughness and dielectric constant of that target.</p> <p></p> <p>Figure 1: Two passes of an imaging SAR taken at time T<sub>0</sub> and T<sub>0</sub> + \u2206t, will give two distances to the ground, R<sub>1</sub> and R<sub>2</sub>.  A difference between R<sub>1</sub> and R<sub>2</sub> shows motion on the ground.  In this case, a subsidence makes R<sub>2</sub> greater than R<sub>1</sub>.  Credit: TRE ALTAMIRA</p> <p>InSAR exploits the phase difference between two SAR images to create an interferogram that shows where the phase and, therefore, the distance to the target has changed from one pass to the next, as illustrated in Figure 1.  There are several factors that influence the interferogram, including earth curvature, topographic effects, atmospheric delays, surface motion, and noise.  With proper processing, Sentinel-1 InSAR can be used to detect changes in the earth's surface down to the centimeter scale. Applications include volcanic deformation, subsidence, landslide detection, and earthquake assessment.</p>"},{"location":"guides/insar_product_guide_template/#wavelengths","title":"Wavelengths","text":"<p>The SAR sensors on the Sentinel-1 satellites transmit C-band signals, with a wavelength of 5.6 cm. The signal wavelength impacts the penetration capability of the signal, so it is important to be aware of the sensor wavelength when working with SAR datasets. C-band SAR will penetrate more deeply into canopy or surfaces than an X-band signal, but not nearly as deep as an L-band SAR signal, which, with a wavelength on the order of 25 cm, is better able to penetrate canopy and return signals from the forest floor.</p> <p>Different wavelengths are also sensitive to different levels of deformation. To detect very small changes over relatively short periods of time, you may require a signal with a smaller wavelength (such as X-band). However, signals with shorter wavelengths are also more prone to decorrelation due to small changes in surface conditions such as vegetation growth.</p> <p>For slower processes that require a longer time interval to detect movement, longer wavelengths (such as L-band) may be necessary. C-band sits in the middle. It can detect fairly small changes over fairly short periods of time, but is not as sensitive to small changes as X-band or as able to monitor surface dynamics under canopy as L-band.</p>"},{"location":"guides/insar_product_guide_template/#polarizations","title":"Polarizations","text":"<p>Polarization refers to the direction of travel of an electromagnetic wave.  A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged.</p> <p>Most modern SAR systems can transmit chirps with either a horizontal or vertical polarization. In addition, some of these sensors can listen for either horizontal or vertical backscatter. This results in the potential for 4 different types of returns: HH, HV, VV, and VH, with the first letter indicating the transmission method and the second the receive method. For example, VH is a vertically polarized transmit signal with horizontally polarized echoes recorded.</p> <p>For InSAR applications, processing is generally performed on the co-pol (VV or HH) data and not on the cross-pol (VH or HV) data. Each image used in an InSAR pair must be the same polarization - two HH acquisitions of the same area could form a valid pair, and two VV acquisitions of the same area could form a valid pair, but you cannot pair an HH acquisition with a VV acquisition to generate an interferogram.</p> <p>On Demand InSAR products only include co-polarized interferograms. Cross-polarized interferograms are not available using this service.</p>"},{"location":"guides/insar_product_guide_template/#baselines","title":"Baselines","text":""},{"location":"guides/insar_product_guide_template/#perpendicular-baseline","title":"Perpendicular Baseline","text":"<p>The term baseline refers to the physical distance between the two vantage points from which images used as an InSAR pair are acquired. The baseline is decomposed into perpendicular (also called normal) and parallel components, as shown in Figure 2.</p> <p>To monitor surface deformation, the perpendicular baseline for the two acquisitions should be very small in order to maximize the coherence of the phase measurements.</p> <p>In order to determine topography, two slightly different vantage points are required. Sensitivity to topography depends on the perpendicular baseline, the sensor wavelength, the distance between the satellite and the ground, and the sensor look angle.</p> <p></p> <p>Figure 2: Geometry of InSAR baselines. Two satellite passes image the same area on the ground from positions S<sub>1</sub> and S<sub>2 </sub>, resulting in a baseline of B, which can be decomposed into perpendicular (B<sub>\u27c2 </sub>) and parallel (B<sub>\u2225 </sub>) components. Here Y is the direction of travel, referred to as the along-track or azimuth direction, and X is the direction perpendicular to motion, referred to as the cross-track or range direction. Credit: ASF</p>"},{"location":"guides/insar_product_guide_template/#temporal-baseline","title":"Temporal Baseline","text":"<p>In contrast to the (physical) baseline, the temporal baseline refers to the time separation between imaging passes. Along-track interferometry measures motion in the millisecond to second range. This technique can detect ocean currents and rapidly moving objects like boats. Differential interferometry is the standard method used to detect motion in the range of days to years. This is the type of interferometry that is performed by the Sentinel-1 HyP3 InSAR processing algorithm. Table 1 lists different temporal baselines, their common names, and what they can be used to measure.</p> Duration Known as Measurement of ms to sec along-track ocean currents, moving object detection, MTI days differential glacier/ice fields/lava flows, surface water extent, hydrology days to years differential subsidence, seismic events, volcanic activity, crustal displacement <p>Table 1: Temporal baselines and what they measure. Different geophysical phenomena can be detected based upon the temporal baseline. In general, the longer the temporal baseline, the smaller the motion that can be detected.</p>"},{"location":"guides/insar_product_guide_template/#critical-baseline","title":"Critical Baseline","text":"<p>Large baselines are better than small for topographic mapping. However, as the baseline increases, coherence decreases. At some point, it is impossible to create an interferogram because of baseline decorrelation. The maximum viable baseline per platform, referred to as the critical baseline, is a function of the distance to the ground, the wavelength, and the viewing geometry of the platform.</p> <p>For Sentinel-1, this critical baseline is about 5 km. In practice, if the perpendicular baseline between images is more than 3/4 of the critical baseline, interferogram creation will be problematic due to the level of noise.</p> <p>For deformation mapping, it is best to minimize the perpendicular baseline whenever possible, but there may be tradeoffs in terms of finding suitable temporal baselines. In most cases, however, pairs selected for deformation mapping will have perpendicular baselines much smaller than the critical baseline.</p>"},{"location":"guides/insar_product_guide_template/#ordering-on-demand-insar-products","title":"Ordering On Demand InSAR Products","text":"<p>All of ASF's On Demand InSAR products are generated using the  HyP3 platform.  Jobs can be submitted for processing using the  Vertex data portal, the  HyP3 Python SDK  or the HyP3 API.</p> <p>InSAR Processing Now Supports Sentinel-1C!</p> <p>GAMMA and ISCE2 software have both been updated to support Sentinel-1C acquisitions as input for InSAR processing.  Users can now use any Sentinel-1 IW SLCs in the archive, including those acquired by Sentinel-1C, as input for  either On Demand InSAR or On Demand Burst InSAR  processing.</p>"},{"location":"guides/insar_product_guide_template/#vertex","title":"Vertex","text":"<p>InSAR pairs are selected in Vertex using either the Baseline Search or the SBAS Search interface. The process of selecting pairs is the same for both IW SLC products and individual SLC bursts, but you will need to select the appropriate dataset when searching for content. As illustrated below, select the Sentinel-1 option in the Dataset menu to search for IW SLC products, and select the S1 Bursts option to search for individual SLC bursts.</p> <p></p> <p>The Baseline tool is the best option for selecting specific InSAR pairs. Use the Geographic Search to find an image that covers your time and area of interest, select that item in the results, and click the Baseline button in the center panel. The Baseline tool then displays all of the scenes that could be used to generate an interferogram using the selected image. Scroll through the results to find pairs to add to the On Demand queue, or click on items displayed in the plot to highlight that particular image pair.</p> <p>The SBAS tool is designed for generating time series of InSAR pairs. As with the Baseline search, you can launch the SBAS search from the center panel of a Geographic Search result. It will display all of the valid InSAR pairs through time based on the acquisition location of the input scene. This functionality is designed for processing a series of interferograms to be used in SBAS (Small BAseline Subset) analysis. The results can be adjusted based on baseline criteria (both perpendicular and temporal), and restricted to specific periods of time. Once the list is refined, you have the option to add all of the InSAR pairs displayed in the results to the On Demand queue.</p>"},{"location":"guides/insar_product_guide_template/#hyp3-sdk-and-api","title":"HyP3 SDK and API","text":"<p>The HyP3 SDK and API provide support for creating interferograms based on a pair of selected granules. To identify granules you'd like to process, we suggest using the Geographic, Baseline and SBAS search tools in Vertex. If you'd prefer to request interferogram processing programmatically, we suggest using Vertex's companion Python package: <code>asf_search</code>. This HyP3 SDK Jupyter Notebook provides you with an example of how you can use the <code>asf_search</code> and <code>hyp3_sdk</code> packages together to identify and create stacks of InSAR products.</p>"},{"location":"guides/insar_product_guide_template/#considerations-for-selecting-an-insar-pair","title":"Considerations for Selecting an InSAR Pair","text":"<p>When selecting an InSAR pair, observe the following required conditions:</p> <ol> <li>Images from an identical orbit direction (either ascending or descending)</li> <li>Images with identical incidence angles and beam mode</li> <li>Images with identical resolution and wavelength (usually from the same sensor)</li> <li>Images with the same viewing geometry (same path and frame)</li> <li>Images with identical polarizations (both HH or VV)</li> </ol> <p>In addition, the following suggestions may be helpful:</p> <ol> <li>Use images from similar seasons/growth/weather conditions</li> <li>For deformation mapping: limited spatial separation of acquisition locations (small physical baseline)</li> <li>For topographic mapping: limited time separation between images (small temporal baseline)</li> </ol> <p>To analyze deformation caused by a single discrete event, such as an earthquake, select images that bracket the event as closely in time as possible. Keeping the window narrowly focused on the time of the event will reduce the impacts of other processes that may mask the signal of the event of interest.</p>"},{"location":"guides/insar_product_guide_template/#data-access","title":"Data Access","text":"<p>Refer to the Downloads  page for more information on viewing and downloading On Demand InSAR products in Vertex or programmatically.  Once processing is complete, download links for On Demand products are valid for 14 days.</p>"},{"location":"guides/insar_product_guide_template/#limitations","title":"Limitations","text":""},{"location":"guides/insar_product_guide_template/#baseline-calculation","title":"Baseline Calculation","text":"<p>The baseline is defined as the difference of the platform positions when a given area is imaged. HyP3 baselines are calculated using the best state vectors available. If precise orbits are not yet available for the input granules, restituted orbits will be used. The original predicted orbits are not used for InSAR processing in HyP3. If no restituted or precise state vectors are available, the process will not run.</p>"},{"location":"guides/insar_product_guide_template/#coherence","title":"Coherence","text":"<p>The phase measurements in the two images used in InSAR must be coherent in order to detect change. Random changes in phase from one acquisition to the next can mask actual surface deformation. Vegetation is a common driver of decorrelation, as changes can easily take place in the interval between two acquisitions due to growth, seasonal changes, or wind effects. It will be difficult to generate valid interferograms with C-band data in heavily vegetated regions due to lack of coherence even with fairly short time intervals.</p> <p>Consider seasonality when selecting image pairs. Decorrelation can be particularly high when comparing phase from different seasons. Changes in the condition of vegetation (especially deciduous canopies), snow, moisture, or freeze/thaw state can impact phase measurements. In cases where a temporal baseline is required that spans seasons, it may be better to use an annual interferogram if possible so that the images are more comparable in terms of seasonality.</p>"},{"location":"guides/insar_product_guide_template/#error-sources","title":"Error Sources","text":"<p>On Demand InSAR products do not currently correct for some common sources of error in interferometry, such as atmospheric effects. Further processing or time series analysis can be performed by the user to identify or reduce the impact of some of these errors when using On Demand InSAR products for analysis.</p>"},{"location":"guides/insar_product_guide_template/#atmospheric-delay","title":"Atmospheric Delay","text":"<p>While SAR signals can penetrate clouds, atmospheric conditions can delay the transmission of the signal. This results in phase differences that can look like surface deformation signals but are actually driven by differences in the atmospheric conditions between the pair of acquisitions used to generate the interferogram.</p> <p>In some cases, atmospheric errors can be corrected by using an atmospheric model to remove the impacts of the turbulent delay from the interferogram. Another approach is to use time series analysis to identify outliers.</p> <p>Always doubt your interferogram first! View the interferogram critically, and consider if fringe patterns could potentially be driven by atmospheric effects. In general, it is best to avoid drawing conclusions from the outcome of a single interferogram.</p> <p>Tropospheric phase may be less impactful when considering small-scale deformation. As such, if you are using ASF's Sentinel-1 Burst InSAR products to look at deformation signals that are smaller than 1 km\u00b2, you should consider using methods other than the typical atmospheric model-based corrections to remove the effects of atmospheric delay. Potential methods in this case include applying band-pass or high-pass spatial filters, or spatial averaging filters such as the approach outlined in Bekaert et al., 2020.</p>"},{"location":"guides/insar_product_guide_template/#turbulent-delay","title":"Turbulent Delay","text":"<p>These delays are generally caused by differences in water vapor distribution from one image to the next. They often manifest as wobbly or sausage-shaped fringes, and can potentially mask the signal of a small earthquake.</p>"},{"location":"guides/insar_product_guide_template/#stratified-delay","title":"Stratified Delay","text":"<p>This type of delay is driven mostly by pressure and temperature differences or gradients through the atmospheric column, and often correlates with topography. This atmospheric signature can be confused with movement caused by volcanic activity. If there are multiple volcanoes in an image, and they all exhibit similar patterns, the signal is likely being driven by this type of atmospheric delay.</p>"},{"location":"guides/insar_product_guide_template/#dem-errors","title":"DEM Errors","text":"<p>A DEM is used to remove topographic phase impacts, but if there are inaccuracies in the DEM, residual impacts of those errors can remain in the interferogram.</p>"},{"location":"guides/insar_product_guide_template/#orbit-uncertainties","title":"Orbit Uncertainties","text":"<p>This is generally not an issue for Sentinel-1 data, as the orbits are very precise and generally reliable. On Demand InSAR products are only processed once restituted or precise orbits are available. Orbit uncertainties are more problematic when working with datasets from older missions.</p>"},{"location":"guides/introduction_to_sar/","title":"Introduction to SAR","text":""},{"location":"guides/introduction_to_sar/#how-sar-operates","title":"How SAR Operates","text":"<p>SAR is an active sensor that transmits pulses and listens for echoes, called backscatter. The backscatter is recorded in both phase and amplitude. The phase is used to determine the distance from the sensor to a target, and amplitude indicates the amount of the sent signal that returns to the sensor. Amplitude measurements provide information about the roughness, geometry, wetness, and dielectric constant of that target, while phase measurements are used for SAR interferometry.</p>"},{"location":"guides/introduction_to_sar/#propagation-of-em-waves","title":"Propagation of EM Waves","text":"<p>At the most fundamental level, SAR transmits an encoded burst, called a chirp, of electromagnetic energy (Figure 1) and then listens for the return signal, called echoes.  The wavelength of this chirp is in the centimeter range, with X-band (~3 cm), C-band (~6 cm), and L-band (~23 cm) all in common use.</p> <p></p> <p>Figure 1: The spectrum of electromagnetic radiation. SAR is imaged using microwave wavelengths. The microwave range extends from about 1 mm to 1 m in wavelength, with most radar applications using bands within the 3 mm to 30 cm range.</p>"},{"location":"guides/introduction_to_sar/#polarizations","title":"Polarizations","text":"<p>Polarization refers to the direction of travel of an electromagnetic wave. A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged.</p> <p>There are four different polarization combinations commonly used by SAR sensors: VV, VH, HV and HH, as listed in Table 1. The first letter indicates the polarization used to transmit the signal, and the second letter indicates the polarization of the measured return, as illustrated in Figure 2.</p> <p>Table 1: SAR Polarizations</p> Polarization Code Transmit Signal Polarization Return Signal Polarization VV Vertical Vertical VH Vertical Horizontal HV Horizontal Vertical HH Horizontal Horizontal <p></p> <p>Figure 2: SAR signals are transmitted and received either vertically (V) or horizontally (H). This gives the potential for four different polarization combinations (transmit listed first, receive second): VV, VH, HH, and HV. Credit: ASF</p> <p>Different SAR sensors have different polarization capabilities. Single-pol sensors send out a signal in one polarization and can only measure returns that are in that same polarization (VV or HH). Dual-pol sensors send out a signal in one polarization, but can measure returns that are in that same polarization (co-pol: VV or HH) as well as returns that are in the other polarization (cross-pol: VH or HV). Some SAR systems can transmit chirps with both a horizontal or vertical polarization and listen for both horizontal or vertical returns, giving full quad-pol capabilities (VV, VH, HV, HH).  </p> <p>Polarimetry is an emerging field of SAR processing which is used in a number of applications such as measuring vegetation properties and changes of vegetation over time. Additional applications include oceanography, geology, and disaster response.</p>"},{"location":"guides/introduction_to_sar/#backscatter-contributors","title":"Backscatter Contributors","text":"<p>Many factors influence the backscatter received by the SAR sensor. The wavelength used by the SAR influences the signal's penetration, and, thus, what is being imaged. Surface roughness will modulate the backscatter returns from nothing up to a strong return, decreasing or increasing the brightness of the resulting pixel. Scattering mechanisms like volume scattering or double bounce can strongly influence the brightness of the SAR image as well, sometimes resulting in total saturation by the received signal.</p>"},{"location":"guides/introduction_to_sar/#wavelength","title":"Wavelength","text":"<p>The wavelength of the SAR system influences the amount of ground penetration that occurs. As shown in Figure 3, X-band has the least penetration, scattering from the top of the canopy in vegetated areas. All three bands will penetrate dry sand, with stronger returns from both C-band and L-band. L-band has the most penetration overall, with returns from the ground in vegetated areas, strong returns from substances under dry alluvium, and deep penetration of ice and snow.</p> <p></p> <p>Figure 3: Effects of the SAR band on penetration of surfaces. The longer the wavelength, the deeper the penetration through most land types. Credit: The SAR Handbook</p>"},{"location":"guides/introduction_to_sar/#surface-roughness","title":"Surface Roughness","text":"<p>The strength of the return, or backscatter, is partially based upon relative roughness of the surface imaged. The smoother the surface, the more reflection away from the sensor, while rough surfaces give a much stronger return towards the imaging platform. As can be seen in Figure 4, if the height of the surface's roughness is less than 1/32 of the wavelength, mostly specular reflection occurs. If the height of the surface's roughness is greater than 1/2 the wavelength used, the echoes are scattered in all directions, giving a strong return back to the sensor.</p> <p></p> <p>Figure 4: The amount of backscatter from a surface depends largely on the surface's roughness, with smooth surfaces getting the least returns and rough surfaces getting the strongest returns. Credit: The SAR Handbook</p>"},{"location":"guides/introduction_to_sar/#types-of-scattering","title":"Types of Scattering","text":"<p>Figure 5: Scattering mechanisms. Rough surfaces give bright returns due to the wide scattering.  Vegetated surfaces cause volumetric scattering, which gives a darker return to the imaging platform.  Double bounce returns, found mostly in urban areas, give the brightest return, as the majority of the energy is re-directed back towards the sensor. Credit: The SAR Handbook</p> <p>The resolution of Sentinel-1 SAR images is roughly 10 m.  This means that a square of 10 meters on the ground is represented by a single pixel in the SAR image. The relative roughness of this patch of ground compared to the wavelength used will affect the backscatter strength (see Figure 4).  However, there are additional types of bounce mechanisms beyond specular and diffuse, as shown in Figure 5.  In vegetation, volumetric scattering occurs when signals bounce around inside the vegetation imaged.  The double bounce mechanism which occurs in urban areas and is exploited by corner reflectors, causes chirp to be reflected directly back to the sensor, causing a very strong backscatter.  Double bounce returns are so strong in some places that they cause over saturation of the sensor, resulting in visible sidelobes.  These sidelobes are evidenced by bright crosses surrounding the double bounce target.</p>"},{"location":"guides/introduction_to_sar/#sar-scale","title":"SAR Scale","text":"<p>SAR backscatter are recorded in both return strength and phase.  Each pixel in a single-look complex SAR image represents these values as an imaginary number (I,Q).  To create the visible images we are used to looking at, the SAR image is detected.  This process calculates the square root of the sum of the squares of the I and Q values found in an SLC image, creating a so-called intensity image.  This image is real valued, and, when calibrated, gives the absolute backscatter of the surface imaged.  Detected images can be stored using several different scales, including power, amplitude, and dB.  Note the default scale of Sentinel-1 RTC products from HyP3 is power. However, in some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and dB.</p>"},{"location":"guides/introduction_to_sar/#power-scale","title":"Power Scale","text":"<p>The values in this scale are generally very close to zero, so the dynamic range of the SAR image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the SAR dataset, but may not always be the best option for data visualization. </p> <p>When viewing a SAR image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values.</p>"},{"location":"guides/introduction_to_sar/#amplitude-scale","title":"Amplitude Scale","text":"<p>Amplitude scale is the square root of the power scale values. This brightens the darker pixels and darkens the brighter pixels, narrowing the dynamic range of the image. In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios (see ASF Sentinel-1 RTC Product Guide).</p>"},{"location":"guides/introduction_to_sar/#db-scale","title":"dB Scale","text":"<p>The dB scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale brightens the pixels, allowing for better differentiation among very dark pixels. When identifying water on the landscape, this is often a good scale to use; the water pixels generally remain very dark, while the terrestrial pixels are even brighter (see Identifying Surface Water).</p> <p>This scale is not always the best choice for general visualization of SAR products, as it can give a washed-out appearance, and because it is in a log scale, it is not appropriate for all types of statistical analyses.</p>"},{"location":"guides/introduction_to_sar/#geometric-distortions","title":"Geometric Distortions","text":"<p>There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis.</p> <p>The key distortions present in SAR images are foreshortening, layover and shadow (Figure 6).</p> <p></p> <p>Figure 6: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer</p> <p>In the case of foreshortening, the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band.</p> <p>When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns.</p> <p>Another condition that results in missing data is radar shadow. In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor.</p> <p>When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application.</p>"},{"location":"guides/introduction_to_sar/#speckle","title":"Speckle","text":"<p>In most cases, the patch of ground illuminated by the SAR transmitter will not be homogeneous. Instead it will be comprised of many different types of individual scatterers. The scatterers may interfere with each other either strengthening the return or weakening it. This creates a grainy (salt &amp; pepper) appearance in SAR imagery. This a result of the nature of SAR and, thus, occurs in all SAR scenes.  Speckle in SAR images can be mitigated by multi-looking, which, in effect, uses averaging to smooth out the image, resulting in a more homogeneous appearance at the expense of resolution.</p>"},{"location":"guides/opera_rtc_product_guide/","title":"OPERA RTC for Sentinel-1 (RTC-S1) Product Guide","text":"<p>This document is a guide for users of the  OPERA Radiometric Terrain Corrected Backscatter for Sentinel-1 (RTC-S1)  products. These products were developed by the  Observational Products for End-Users from Remote Sensing Analysis (OPERA)  project at NASA's  Jet Propulsion Laboratory (JPL).</p> <p>OPERA RTC-S1 Products Now Available On Demand</p> <p>If OPERA RTC-S1 products are not available for your full time range or area of interest, you can now request  that they be generated using ASF's On Demand processing platform!</p> <p>OPERA RTC-S1 products are archived in the  OPERA_L2_RTC-S1_V1  collection.</p>"},{"location":"guides/opera_rtc_product_guide/#opera-rtc-s1-products","title":"OPERA RTC-S1 Products","text":"<p>OPERA's  Radiometric Terrain Corrected (RTC) Synthetic Aperture Radar (SAR) Backscatter for Sentinel-1 (S1)  product consists of radar backscatter normalized with respect to topography. The RTC algorithm used for the  OPERA RTC-S1 products was developed by  Gustavo Shiroma and others,  and is available in the  ISCE3 open source software library.</p> <p>The RTC-S1 Level-2 products are projected  into the appropriate UTM Zone or Polar Stereographic coordinate system for the location of each individual product  and provided in  Cloud-Optimized GeoTIFF (COG)  format. The pixel values of the products represent gamma-0 power. The pixel spacing is 30 meters, and no  speckle filter has been applied.</p> <p>RTC products provide users with imagery of the earth's surface regardless of atmospheric conditions. This allows  users to monitor surface processes during natural disasters, such as hurricanes or wildfires, or in areas that are  prone to frequent cloud cover. Backscatter values can be used to determine surface water extent, soil moisture  trends, surface roughness, and vegetation conditions.</p>"},{"location":"guides/opera_rtc_product_guide/#digital-elevation-model-dem","title":"Digital Elevation Model (DEM)","text":"<p>Radiometric Terrain Correction requires the use of a Digital Elevation Model (DEM) both for  correcting distortions caused by topography and for geocoding the output product. The OPERA RTC-S1 products are processed using the  Copernicus GLO-30 DEM.</p>"},{"location":"guides/opera_rtc_product_guide/#polarization","title":"Polarization","text":"<p>Most Sentinel-1 acquisitions are collected in two  polarizations,  and the OPERA project generates RTC-S1 products for all available polarizations. </p> <p>Sentinel-1 acquisitions over land generally have a vertical primary polarization, meaning that the SAR signal is sent  out in a vertical orientation, but both the co-polarized returns (also vertically polarized) and the cross-polarized  returns (horizontally polarized) are measured. This results in both VV and VH RTC-S1 products. </p> <p>In some areas, particularly remote islands and regions close to the Arctic Ocean, acquisitions are collected with a  horizontal primary polarization. This results in HH and HV RTC-S1 products. </p> <p>Different polarizations are sensitive to different surface characteristics, with VV being more sensitive to surface  roughness, VH or HV being more sensitive to volume scatterers such as vegetation, and HH being more sensitive to  double-bounce scattering from objects that stand perpendicular to the ground. </p>"},{"location":"guides/opera_rtc_product_guide/#archived-opera-rtc-s1-products","title":"Archived OPERA RTC-S1 Products","text":"<p>The OPERA project has generated RTC-S1 products in all available polarizations for all Sentinel-1 acquisitions  over landmasses (except Antarctica) since January 1, 2022. As new Sentinel-1 SLCs are acquired and added to  ASF's archive, the OPERA project continues to process them to RTC-S1 products. New RTC-S1 products  are generally available well within 12 hours of the Sentinel-1 SLC acquisition.</p> <p>RTC-S1 products generated by the OPERA project are all archived by ASF and can be accessed using a number of methods: </p> <ul> <li>ASF's Vertex Data Search,      which is a user-friendly and map-centric interface specialized for search and discover of ASF's SAR holdings</li> <li>Programmatically using the asf_search Python package</li> <li>NASA's Earthdata Search      interface, which provides access to all of NASA's Earth Science datasets</li> </ul> <p>For more information on options for accessing archived OPERA RTC-S1 products, refer to ASF's  OPERA Sentinel-1 RTC tutorial.  For more information on the technical specifications of the OPERA RTC-S1 products, refer to JPL's  RTC Product Documentation.</p>"},{"location":"guides/opera_rtc_product_guide/#opera-rtc-s1-products-on-demand","title":"OPERA RTC-S1 Products On Demand","text":"<p>You can also order OPERA RTC-S1 products from ASF On Demand. This is particularly useful if you need a time  series of RTC-S1 products that extends beyond the start of the archive. On-Demand processing is available for any  Sentinel-1 burst with the same burst ID (footprint) as an existing OPERA RTC-S1 product. </p> <p>On-Demand RTC-S1 products are generated using the same code that is used by the OPERA project, but are processed  using ASF's  HyP3  cloud-native processing platform instead of JPL's  OPERA Science Data System (SDS)  processing management software. </p> <p>Archived and On-Demand RTC-S1 products can be used interchangeably in a time series analysis.</p>"},{"location":"guides/opera_rtc_product_guide/#searching-for-archived-opera-rtc-s1-products","title":"Searching for Archived OPERA RTC-S1 Products","text":"<p>To search for existing OPERA RTC-S1 products in  Vertex,  select <code>OPERA-S1</code> from the <code>Dataset</code> drop-down menu. </p> <p></p> <p>Because the OPERA RTC-S1 footprints are so small, there are millions of files in the archive. It is important to  make use of the available search filters to find the products you want.</p> <ol> <li>Set an Area of Interest</li> <li>Set a date range</li> <li>In the Search Filters menu, select the <code>L2 Radiometric Terrain Corrected (RTC)</code> File Type</li> </ol> <p>For step-by-step guidance on searching for OPERA RTC-S1 products using  Vertex  or the  asf_search Python package,  refer to the  ASF Data Search  section of the  OPERA Sentinel-1 RTC StoryMap tutorial.</p> <p>For more information about using  Earthdata Search  to access OPERA RTC-S1 products, refer to the  Earthdata Search  section of the  OPERA Sentinel-1 RTC StoryMap tutorial.</p>"},{"location":"guides/opera_rtc_product_guide/#ordering-on-demand-opera-rtc-s1-products","title":"Ordering On-Demand OPERA RTC-S1 Products","text":"<p>On-Demand OPERA RTC-S1 products can be requested for any Sentinel-1 IW SLC burst acquired on or after April 14, 2016,  and before January 1, 2022, for locations north of -60\u00b0 latitude (i.e. all global landmasses except for Antarctica).</p>"},{"location":"guides/opera_rtc_product_guide/#submitting-on-demand-opera-rtc-s1-jobs","title":"Submitting On-Demand OPERA RTC-S1 Jobs","text":"<p>OPERA RTC-S1 On Demand not yet available in Vertex</p> <p>OPERA RTC-S1 On Demand jobs can be submitted using the HyP3 Python SDK  or HyP3 API. Support in Vertex is coming soon!</p> <p>To order an On-Demand OPERA RTC-S1 product, a user must pass a Sentinel-1 Burst SLC granule for the co-polarized  return (either VV or HH) as input to either the  HyP3 Python SDK  or  HyP3 API. If a cross-pol burst is passed as input, an error will be returned.</p> <p>The output product will include RTC-S1 rasters for all available polarizations (generally VV and VH or HH and HV),  so requiring users to pass a co-polarized burst as input ensures that there will not be accidental duplication of  processing effort if both the co-pol and cross-pol inputs for the same Sentinel-1 burst are submitted for processing. </p> <p>To ensure successful processing, review the  date range and  spatial coverage  limitations below. </p>"},{"location":"guides/opera_rtc_product_guide/#date-range-for-on-demand-opera-rtc-s1-products","title":"Date Range for On-Demand OPERA RTC-S1 Products","text":"<p>The OPERA RTC-S1 code requires that input Sentinel-1 SLCs were processed using ESA's Sentinel-1 Instrument Processing  Facility (IPF) version 2.70, implemented April 13, 2016, or newer. As such, we do not support On-Demand processing  for Sentinel-1 acquisitions prior to April 14, 2016. Jobs submitted for earlier bursts will return an error. </p> <p>All supported Sentinel-1 acquisitions since January 1, 2022, have been processed to OPERA RTC-S1 products by the OPERA  project and are already available for download from ASF's archive. Any On-Demand jobs submitted for acquisitions on  or after that date will also return an error. </p>"},{"location":"guides/opera_rtc_product_guide/#spatial-coverage-for-on-demand-opera-rtc-s1-products","title":"Spatial Coverage for On-Demand OPERA RTC-S1 Products","text":"<p>On-Demand OPERA RTC-S1 Products can only be ordered for bursts that are north of -60\u00b0 latitude,  which includes all global landmasses except for Antarctica. Only bursts from  Sentinel-1 IW SLC  products are supported as input. </p> <p>Any job submitted where the Sentinel-1 burst is from an EW SLC or located south of -60\u00b0 latitude will return an error.</p>"},{"location":"guides/opera_rtc_product_guide/#product-packaging","title":"Product Packaging","text":"<p>The products generated by the OPERA SDS are available for download as individual files associated with the  OPERA_L2_RTC-S1_V1 record corresponding to the Sentinel-1 burst used as input for the product.</p> <p>Products generated On Demand using ASF's HyP3 platform are delivered as zip files that include all the files  available for the products generated by the OPERA SDS. The zip file also includes the color browse image (which is  not georeferenced) used for displaying archived OPERA RTC-S1 products in Vertex. </p> <p>The naming convention for On-Demand OPERA RTC-S1 products generated using HyP3 is the same as for  the OPERA RTC-S1 products generated using the OPERA SDS.</p>"},{"location":"guides/opera_rtc_product_guide/#l2-radiometric-terrain-corrected-rtc-files","title":"L2 Radiometric Terrain Corrected (RTC) Files","text":"<p>OPERA RTC-S1 products are available as a collection of files associated with a source Sentinel-1 burst. If you search  for OPERA-S1 products in Vertex, the results for each Sentinel-1 burst provide access to a number of files. If you  click on an item in the left panel of the search results, the associated files are listed in the right panel. </p> <p></p> <p>The files available for download include: </p> <ul> <li>HDF5 file containing product metadata, specifically orbit position and velocity (no actual RTC images are    included in this file)</li> <li>A single-band 32-bit float Cloud-Optimized GeoTIFF (COG) file for each available polarization containing the    RTC values (most commonly VV and VH, but HH and HV in some areas)</li> <li>Mask COG file indicating pixels in the RTC products that contain valid data and indicating    which pixels are impacted by layover and/or shadow</li> <li>Metadata XML file containing information about the product in ISO format</li> <li>Local Incidence Angle COG file </li> </ul>"},{"location":"guides/opera_rtc_product_guide/#l2-radiometric-terrain-corrected-static-layer-rtc-static-files","title":"L2 Radiometric Terrain Corrected Static Layer (RTC-STATIC) Files","text":"<p>There are some ancillary files generated during the RTC processing workflow that change very little through time.  Instead of including these ancillary products with each OPERA RTC product, they are generated once  for each Sentinel-1 burst ID and archived as <code>Static Layer</code> files. </p> <p>Refer to OPERA's  Product Specification Document for the OPERA Radiometric Terrain Corrected SAR Backscatter from Sentinel-1 Static Layers  for more information on these files.</p> <p>These <code>RTC-STATIC</code> products include the following COG files: </p> <ul> <li>local incidence angle</li> <li>incidence angle</li> <li>mask (layover/shadow validity mask)</li> <li>number of looks</li> <li>RTC Area Normalization Factor (ANF) gamma0 to beta0</li> <li>RTC ANF gamma0 to sigma0</li> </ul> <p>You can access the RTC-STATIC files the same way that you would the RTC-S1 files. Applying a <code>File Type</code> filter of  <code>L2 Radiometric Terrain Corrected Static Layer (RTC-STATIC)</code> in Vertex will restrict search results to just the  static layers. Click the <code>Filters...</code> button to access the <code>File Type</code> drop-down menu.</p> <p></p> <p>If you want to find the static layers that correspond to a specific RTC footprint, you can filter your RTC-STATIC  search results using the OPERA Burst ID. </p> <p></p> <ul> <li>In Vertex, search for both <code>RTC</code> and <code>RTC-STATIC</code> file types for your area of interest. </li> <li>Because the static layers all have an acquisition date set to <code>04/03/2014, 00:00:00Z</code>, the static      files may not be included in the results if a date range has been applied to the search. </li> <li>Select an RTC product from the results list that has the desired footprint, and click on the icon button     next to the Opera Burst ID.</li> <li>Select the option to <code>Add OPERA Burst ID to Search</code> and click the Search button again.</li> <li>Make sure to clear any date range filters that do not include April 3, 2014.</li> <li>The search results will include only the products that have the designated burst ID. If there are too many      results for the RTC-STATIC products to be included in the search results, open the Search Filters panel again and      select only the RTC-STATIC product type to return only the static layers that correspond to that RTC footprint.</li> </ul> <p>OPERA RTC-STATIC products not available for HH and HH+HV polarized scenes</p> <p>The static layers associated with HH and HH+HV polarized acquisitions, located over some parts of Greenland and Arctic Canada, are currently unavailable in ASF's archive. We are working with the OPERA team to add these  files to the ASF archive.</p> <p>This graphic illustrates the current coverage of OPERA RTC-S1 Static Layers. </p>"},{"location":"guides/opera_rtc_product_guide/#duplicate-layer-names","title":"Duplicate layer names","text":"<p>There are Local Incidence Angle and Mask files listed in association with both the <code>RTC</code> search results  and the <code>RTC-STATIC</code> search results. </p>"},{"location":"guides/opera_rtc_product_guide/#local-incidence-angle","title":"Local Incidence Angle","text":"<p>The Local Incidence Angle file can be downloaded using either link, but it is the same file. It is always named with  this pattern:  <code>OPERA_L2_RTC-S1-STATIC_Txxx-xxxxxx-IWx_20140403_S1A_30_v1.0_local_incidence_angle.tif</code></p> <p>The download URL behind the local incidence angle listings in both the <code>RTC</code> and <code>RTC-STATIC</code> results  reference the same source file.</p>"},{"location":"guides/opera_rtc_product_guide/#validity-mask","title":"Validity Mask","text":"<p>There is a file called <code>Mask</code> listed with both the <code>RTC</code> and the <code>RTC-STATIC</code> search results. Unlike the  local incidence angle file, these two mask files are NOT the same. Both are validity masks with the same pixel  value categories: </p> Pixel Value Description 0 Valid sample not affected by layover or shadow 1 Valid sample affected by shadow 2 Valid sample affected by layover 3 Valid sample affected by layover and shadow 255 Invalid sample (fill value) <p>The main difference between the two files is that the data extent matches the other associated files. The  <code>RTC-STATIC</code> files have a larger extent than the <code>RTC</code> files, as data is included for the full raster footprint,  including the NoData pixels that are present around the edges of the RTC data products.</p> <p></p> <p>The mask file in the map on the left is the mask linked to the <code>RTC</code> search results.</p> <ul> <li><code>OPERA_L2_RTC-S1_T115-245714-IW1_20250418T141628Z_20250419T010229Z_S1A_30_v1.0_mask.tif</code></li> </ul> <p>The extent of NoData padding around the area that has valid radar data for that particular burst is displayed with a  transparent pink color in this illustration, but would normally appear transparent. The mask values are only applied  to the pixels with valid radiometry within the radar burst.</p> <p>The mask file in the map on the right is the mask linked to the <code>RTC-STATIC</code> search results.</p> <ul> <li><code>OPERA_L2_RTC-S1-STATIC_T115-245714-IW1_20140403_S1A_30_v1.0_mask.tif</code></li> </ul> <p>It includes validity mask values for the full extent of the burst footprint, including the NoData padding around the  pixels with valid radiometry within the radar burst. All of the static layers include data for this entire area.</p> <p>There may, however, also be differences in the actual pixel values when comparing an RTC-STATIC validity mask to  the validity mask included with a specific RTC product. Because orbits can shift slightly, the layover  or shadow conditions for any given pixel may be different from one pass to another.</p> <p>For investigating the layover/shadow impacts for a specific RTC product, users will generally be better served by  using the validity mask delivered with that product rather than the mask available as a static layer.</p>"},{"location":"guides/opera_rtc_product_guide/#naming-convention","title":"Naming Convention","text":"<p>The file names of OPERA RTC-S1 products are designed to be unique and descriptive. </p> <p>The following file-naming convention is used:</p> <p><code>OPERA_L2_RTC-S1_[BurstID]_[StartDateTime]_[ProductGenerationDateTime] _[Sensor]_[PixelSpacing]_[ProductVersion]_[LayerName].Ext</code></p> <p>For example: OPERA_L2_RTC-S1_T115-245714-IW1_20250418T141628Z_20250419T010229Z_S1A_30_v1.0_VV.tif</p> <p>Table 1 describes the dynamic elements in the naming scheme.</p> Element Description Example BurstID Unique burst identification string consistent with ESA burst map convention in the form of T[TrackNumber]-[ID]-[SubSwath] T115-245714-IW1 StartDateTime The acquisition start date and time in UTC of the S1 SAFE file that was used as an input for processing (i.e. burst SLC) in the format YYYYMMDDTHHMMSSZ 20250418T141628Z ProductGenerationDateTime The date and time (UTC) at which the product was generated by OPERA in the format YYYYMMDDTHHMMSSZ 20250419T010229Z Sensor The input product sensor, including the specific Sentinel-1 platform S1A PixelSpacing Product pixel spacing in meters 30 ProductVersion OPERA RTC-S1 product version number with four characters, including the letter \u201cv\u201d and two digits indicating the major and minor versions, which are delimited by a period v1.0 LayerName Name of the RTC-S1 product layer, if applicable VV Ext File extension: \u201ctif\u201d, \u201ch5\u201d, or \u201cpng\u201d tif <p>Example file names for each of the files associated with OPERA RTC-S1 products:</p> <p>OPERA_L2_RTC-S1_T069-147170-IW1_20210205T163901Z_20220101T140222Z_S1A_30_v1.0.h5</p> <p>OPERA_L2_RTC-S1_T069-147170-IW1_20210205T163901Z_20220101T140222Z_S1A_30_v1.0_VV.tif</p> <p>OPERA_L2_RTC-S1_T069-147170-IW1_20210205T163901Z_20220101T140222Z_S1A_30_v1.0_VH.tif</p> <p>OPERA_L2_RTC-S1_T069-147170-IW1_20210205T163901Z_20220101T140222Z_S1A_30_v1.0_mask.tif</p>"},{"location":"guides/opera_rtc_product_guide/#data-access","title":"Data Access","text":"<p>Refer to the Downloads page for more information on viewing and downloading  OPERA RTC-S1 On Demand products in Vertex or programmatically. Once processing is complete, download links  for On Demand products are valid for 14 days.</p> <p>We plan to support adding standard OPERA RTC-S1 products generated On Demand to the OPERA_L2_RTC-S1_V1 collection,  expanding the archive based on user needs. Stay tuned for that functionality!</p>"},{"location":"guides/opera_rtc_product_guide/#on-demand-rtc-product-options-from-asf","title":"On-Demand RTC Product Options from ASF","text":"<p>In addition to OPERA RTC-S1 products, which use an RTC algorithm available in JPL's open-source  ISCE3 software  to perform radiometric terrain correction, ASF also offers  On-Demand RTC products  generated using commercial GAMMA  SAR processing software. </p> <p>These products are both high-quality Sentinel-1 RTC options, and you can use either with confidence for any given  RTC-based analysis workflow. However, because ISCE3 and GAMMA use different algorithms for RTC processing,  a time-series analysis will be more consistent if you don't mix and match OPERA RTC-S1 products and ASF's RTC GAMMA  products. </p> <p>There are some key characteristics that differ between the two products, which may help you decide which would be  most appropriate for your particular application. </p>"},{"location":"guides/opera_rtc_product_guide/#spatial-extent","title":"Spatial Extent","text":"<p>OPERA RTC-S1 products are processed on the basis of an individual  radar burst  extracted from a Sentinel-1  Interferometric Wide-Swath (IW) Single Look Complex (SLC) file, while ASF's RTC GAMMA On Demand products are  generated using the Sentinel-1 Level 1 IW SLC or Ground Range Detected (GRD) file.</p> <p>Advantages of burst-based products:</p> <ul> <li>RTC footprint is much smaller. Each IW SLC contains many individual bursts (most often about 27), and you      may not need such extensive spatial coverage for your analysis. If you have a fairly small area of interest,      you can download and mosaic only the bursts you need.</li> <li>File sizes are smaller. This is particularly important for users with limited internet access. Each OPERA      RTC-S1 GeoTIFF file is on the order of 8 MB, and you can download each polarization separately. In contrast, ASF's      RTC GAMMA products are delivered as a zip file, which includes all available polarizations. These zip      files are generally upwards of 500 MB for products generated at 30-m pixel spacing, and they can be challenging to      download successfully over slower connections.</li> <li>Footprints are consistent. The individual Sentinel-1 bursts always have the same extent from one      acquisition to the next, which makes it very easy to generate a time series over an area of interest. The framing of     the full Sentinel-1 IW Level 1 products can shift over time, so there is no guarantee that acquisitions with the      same frame number will cover the same extent. This is particularly impactful when the area of interest is near      the top or bottom of a Sentinel-1 IW Level 1 scene.</li> </ul> <p>If your area of interest is large, and is well-covered by a full Sentinel-1 IW scene (or requires several full  scenes to cover the entire area), you may find it easier to work with the full-scene RTC GAMMA products, as there  would be fewer individual files to manage. </p>"},{"location":"guides/opera_rtc_product_guide/#processing-options","title":"Processing Options","text":"<p>On-Demand processing of OPERA RTC-S1 products is currently confined to the default settings of the archived  products. OPERA RTC-S1 products are output in gamma-0 power with 30-m pixel spacing. While we expect to offer  some customization options in the future, the only method currently available for ordering products with a different  radiometry, scale, or pixel spacing is to order ASF's On-Demand Sentinel-1 RTC products  processed using GAMMA software. </p> <p>Learn more about the options available for processing full-scene Sentinel-1 RTC GAMMA products and why you might  find them useful in the  Processing Options section  of the  RTC On Demand! tutorial. </p>"},{"location":"guides/opera_rtc_product_guide/#rgb-decomposition","title":"RGB Decomposition","text":"<p>It can be helpful to combine co-polarized and cross-polarized RTC values into a false-color image. There are a  number of methods for combining VV and VH or HH and HV into the red, green, and blue channels, which is commonly  called RGB Decomposition. </p> <p>The RGB Decomposition approach used by the OPERA team for the RTC-S1 products is very different from the approach  ASF uses for the Sentinel-1 RTC On Demand products processed using GAMMA.</p>"},{"location":"guides/opera_rtc_product_guide/#opera-rtc-s1-rgb-decomposition","title":"OPERA RTC-S1 RGB Decomposition","text":"<p>The OPERA project uses a simple approach to combining polarizations to generate a color browse image, which is  displayed in Vertex when you search for OPERA RTC-S1 products. This RGB Decomposition assigns the co-pol values  (VV or HH) to both the red and blue bands, while the cross-pol values (VH or HV) are assigned to the green band.  Scalars are applied to the different bands to balance the color range. </p> <p>In these color images, water generally appears black, areas with vegetation appear more green, and other areas  appear pink.</p> <p>You can download these browse images from the Vertex interface by selecting an RTC-S1 product from the list of  search results and clicking the download icon above the image preview in the center panel, as shown in Figure 1.  Note that these browse images are not georeferenced, though you can use the individual RTC GeoTIFFs as a  reference for manually georeferencing the browse images. </p> <p></p> <p>This approach is also used to generate the imagery tiles for OPERA RTC-S1 that will be displayed in NASA's  Worldview platform. </p> <p>You can also create your own OPERA RTC-S1 RGB Decompositions using GIS software. </p> <ul> <li>In ArcGIS, use the      Composite Bands geoprocessing tool      or      raster function template      to assign the VV and VH layers to the desired color channels. </li> <li>In QGIS, use the      Build Virtual Raster      algorithm. </li> </ul>"},{"location":"guides/opera_rtc_product_guide/#rtc-gamma-rgb-decomposition","title":"RTC GAMMA RGB Decomposition","text":"<p>Sentinel-1 RTC On-Demand products processed using GAMMA software include a georeferenced RGB Decomposition  browse image in the product package by default. You also have the  option to include a full-resolution RGB Decomposition GeoTIFF  in the output package when submitting the RTC job for processing. </p> <p>The  algorithm used to generate the RGB Decomposition images  included in the product package is very different from the approach used by the OPERA team. It uses a series of  thresholds to determine which values to attribute to the different color bands, and applies scalars to each band  to generate an intuitive false-color image. </p> <p>For pixels with very low values in both the co- and cross-pol RTC products, the co-pol values are assigned  to the blue channel. High cross-pol values are assigned to the green channel. For pixels with high co-pol values but  low cross-pol values, the co-pol values are assigned to the red channel. </p> <p>In these images, water generally appears blue, vegetated areas look green, and other regions (urban areas, agricultural  fields, sparsely vegetated areas) are yellow or orange.</p> <p>If you want to generate a full-resolution RGB image from an RTC GAMMA product but neglected to select the option  to include it in the product package, you can also use the RGB Decomposition Tool in  ASF's ArcGIS Toolbox.</p>"},{"location":"guides/opera_rtc_product_guide/#sentinel-1-mission","title":"Sentinel-1 Mission","text":"<p>The  Sentinel-1 mission  collects C-band band SAR from a pair of polar-orbiting satellites launched by the European Space Agency (ESA) as part  of the  Copernicus program.  The Sentinel-1A satellite was launched April 3, 2014, Sentinel-1B was launched April 25, 2016, and Sentinel-1C was  launched December 5, 2024. </p> <p>Sentinel-1A is still collecting data, but  Sentinel-1B ended its mission  on December 23, 2021. Sentinel-1C has now replaced Sentinel-1B in the constellation, returning the Sentinel-1  mission to full observation capacity as of March 26, 2025. </p> <p>The Sentinel-1 satellites each have a 12-day repeat cycle, but when there are two functioning satellites, their orbits  are offset 180 degrees so that one or the other will pass over the same location on earth every 6 days. Under this  scenario, select areas of interest are imaged with a 6-day interval, as described in the  mission observation scenario,  while most landmasses are imaged on a 12-day repeat cycle.</p> <p>For the time period between when Sentinel-1B stopped acquiring data and Sentinel-1C started acquiring data,  coverage was more sparse. Some areas did not have any imagery acquired between December 2021 and April 2025.  Depending on your area of interest, you may have limited data available during that time. For more information,  visit our Sentinel-1 Mission page.</p> <p>Because this is a polar-orbiting satellite constellation, areas near the poles may have overlapping orbits,  resulting in more frequent acquisitions than indicated by the observation scenario. </p>"},{"location":"guides/rtc_atbd/","title":"RTC Algorithm Theoretical Basis","text":""},{"location":"guides/rtc_product_guide/","title":"Sentinel-1 RTC Product Guide","text":"<p>This document is a guide for users of Radiometrically Terrain Corrected (RTC) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request RTC products On Demand in ASF's Vertex data portal, or make use of our Python SDK or API.</p> <p>SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction corrects these distortions and creates analysis-ready data suitable for use in GIS applications or time-series analysis. RTC processing is a required first step for many amplitude-based SAR applications.</p> <p>ASF's Sentinel-1 On-Demand RTC products are generated using GAMMA Software. Products are distributed as GeoTIFFs (one for each available polarization) projected to the appropriate UTM Zone for the location of the scene.</p> <p>A Digital Elevation Model (DEM) is required for radiometric terrain correction. The GLO-30 Copernicus DEM is used to process all RTC On Demand products. Refer to the Digital Elevation Model section for more information.</p> <p>For a step-by-step tutorial on ordering On-Demand RTC Products using Vertex, visit our RTC On Demand! StoryMap, which also includes links to sample workflows using Sentinel-1 RTC products for GIS applications.</p>"},{"location":"guides/rtc_product_guide/#introduction","title":"Introduction","text":""},{"location":"guides/rtc_product_guide/#sentinel-1-mission","title":"Sentinel-1 Mission","text":"<p>The  Sentinel-1 mission  collects C-band band SAR from a pair of polar-orbiting satellites launched by the European Space Agency (ESA) as part  of the  Copernicus program.  The Sentinel-1A satellite was launched April 3, 2014, the Sentinel-1B satellite was launched April 25, 2016,  and the Sentinel-1C satellite was launched December 5, 2024. </p> <p>Sentinel-1A is still collecting data, but  Sentinel-1B ended its mission  on December 23, 2021. Sentinel-1C has now replaced Sentinel-1B in the constellation, returning the Sentinel-1  mission to full observation capacity as of March 26, 2025. </p> <p>The Sentinel-1 satellites each have a 12-day repeat cycle, but when there are two functioning satellites, their orbits  are offset 180 degrees so that one or the other will pass over the same location on earth every 6 days. Under this  scenario, select areas of interest are imaged with a 6-day interval, as described in the  mission observation scenario,  while most landmasses are imaged on a 12-day repeat cycle.</p> <p>For the time period between when Sentinel-1B stopped acquiring data and Sentinel-1C started acquiring data,  coverage was more sparse. Some areas did not have any imagery acquired between December 2021 and April 2025.  Depending on your area of interest, you may have limited data available during that time. For more information,  visit our Sentinel-1 Mission page.</p> <p>Because this is a polar-orbiting satellite constellation, areas near the poles may have a number of overlapping paths,  resulting in even more frequent acquisitions with similar footprints. </p> <p>The relatively short interval between acquisitions makes this SAR dataset a very useful tool for monitoring rapid or  sudden landscape changes. In addition, SAR can image the earth's surface through cloud or smoke cover and does not  require sunlight, so valid imagery can be collected on every pass. This is particularly useful for monitoring  conditions during natural disasters such as hurricanes or wildfires, or in areas that are prone to frequent cloud cover.</p>"},{"location":"guides/rtc_product_guide/#sar-distortions","title":"SAR Distortions","text":"<p>There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis.</p> <p>The key distortions present in SAR images are foreshortening, layover and shadow (Figure 1). </p> <p></p> <p>Figure 1: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer</p> <p>In the case of foreshortening, the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band. </p> <p>When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns.</p> <p>Another condition that results in missing data is radar shadow. In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor.</p> <p>When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application.</p> <p>The RTC product package includes a Layover-Shadow mask (see Image Files section) If you find that there are No Data pixels in your image, you can refer to that reference raster to see if the missing pixels are due to layover or shadow effects.</p>"},{"location":"guides/rtc_product_guide/#digital-elevation-models","title":"Digital Elevation Models","text":"<p>The quality of the terrain corrections are related to the quality of the digital elevation models (DEMs) used in the process of geometrically and radiometrically correcting the SAR imagery. </p> <p>We use the 2022 Release of the  Copernicus GLO-30 Public DEM,  available on AWS. </p> <p>Coverage gaps in Copernicus DEM GLO-30 filled using GLO-90</p> <p>The Copernicus DEM GLO-30 dataset does not provide coverage over Armenia and Azerbaijan. In the past, we have not supported On Demand product generation over those areas using the Copernicus DEM option. We now use the Copernicus DEM GLO-90 to fill those gaps. </p> <p>Users should be aware that the GLO-90 dataset has a pixel spacing of 90 meters, which is not as detailed as the 30-m pixel spacing in the GLO-30 DEM. </p> <p>Table 1 summarizes ASF's source DEM. Note that we currently only support using the Copernicus 30-m DEM for processing RTC. The source DEM is resampled to the pixel spacing of the output RTC product, and it is reprojected to the UTM Zone (WGS84) for the area covered by the Sentinel-1 scene undergoing correction. A geoid correction is applied before it is used for RTC processing.</p> Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default <p>Table 1: DEM used for RTC processing. The Copernicus DEM is the only option available when processing RTC and InSAR products</p> <p>When ordering On-Demand RTC products, you can choose to include a copy of the DEM used for RTC processing in the RTC product package. This DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. Pixel values indicate the elevation in meters. Note that the elevation values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in RTC processing.</p>"},{"location":"guides/rtc_product_guide/#copernicus-dem","title":"Copernicus DEM","text":"<p>The GLO-30 Copernicus DEM provides global coverage at 30-m pixel spacing (with the current exception of an area covering Armenia and Azerbaijan, see Figure 3). </p> <p>When an On Demand RTC job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS, managed by Sinergise. We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them to match the pixel spacing and alignment of the RTC product. A geoid correction is applied before it is used for RTC processing.</p> <p>For the area that does not have coverage with the GLO-30 DEM, we use the Copernicus DEM GLO-90 dataset, which provides elevation data at 90-meter pixel spacing. Users ordering products over this area should be aware that a lower-resolution DEM is used for processing.</p> <p>Figure 2 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 3 details the land area currently only covered by the GLO-90 DEM at 90-m pixel spacing.</p> <p></p> <p>Figure 2: Copernicus DEM GLO-30 coverage map</p> <p></p> <p>Figure 3: Detail of area currently not covered by Copernicus DEM GLO-30. On Demand jobs requested over this area will use the Copernicus DEM GLO-90.</p>"},{"location":"guides/rtc_product_guide/#pixel-spacing","title":"Pixel Spacing","text":"<p>On Demand Sentinel-1 RTC now available at 10-m and 20-m pixel spacing</p> <p>There are now three pixel spacing options available for On Demand Sentinel-1 RTC products. Users can choose to output the RTC products at a pixel spacing of 30, 20, or 10 meters.</p> <p>RTC products can be output at 30-meter, 20-meter, or 10-meter pixel spacing. In most cases, the input SAR image has a resolution closer to the 10-m products, while the Copernicus DEM (used by default for RTC processing) has a pixel spacing of 30 m. The 10-m RTC product will be closer to the resolution of the source SAR granule, but the 30-m RTC product has a much smaller file size. </p> <p></p> <p>Figure 5: Comparison of RTC products generated with different pixel spacing settings</p> <p>It is much faster to process, download and analyze 30-m RTC products than 10-m products, so it's a good idea to start with the coarser resolution option if possible. If the 30-m pixel spacing is not sufficient for your use case, try the 20-m RTC products. If even more detail is required, the 10-m products may be the best option.</p> <p>The 20-m product may be a good trade-off between resolution and file size. The amount of detail in the 20-m product is much closer to the 10-m product, but the file size is much closer to the 30-m product. Consider the file sizes for the RTC VV GeoTIFFs displayed in the comparison image:</p> Pixel Spacing File Size 30 m 267 MB 20 m 600 MB 10 m 2350 MB"},{"location":"guides/rtc_product_guide/#dem-resolution","title":"DEM Resolution","text":"<p>Keep in mind that the same DEM is used for processing the RTC products, regardless of the output pixel spacing. By default, the DEM is the Copernicus Global DEM, which has a pixel spacing of 30 meters. </p> <p>When processing 10-m RTC products, the source DEM is resampled to a pixel spacing of 10 meters. This resampled DEM can optionally be included in the product package, and the pixel spacing will align with the output RTC product. The same is true for the 20-m products; the DEM is resampled to a pixel spacing of 20 meters, and the resampled version is optionally included in the product package. </p> <p>The pixel spacing of the output DEM file does not indicate that the source DEM used for the 10-m or 20-m products is of higher resolution.</p>"},{"location":"guides/rtc_product_guide/#processing-options-and-optional-files","title":"Processing Options and Optional Files","text":"<p>There are a number of options users can set when ordering RTC On Demand products. Some of these options set parameters used in the RTC processing workflow, others allow users to add additional files to the product package that are not included by default. </p> <p>Table 2 lists all of the options as displayed in the Vertex user interface and the HyP3 API, and the Processing Options and Optional Files sections provide more information about each option.</p> Option Name in Vertex Option Name in HyP3 API/SDK Possible Values Default Description Radiometry radiometry (gamma0, sigma0) gamma0 Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) Scale scale (power, decibel, amplitude) power Scale of output backscatter values Pixel Spacing resolution (10.0, 20.0, 30.0) 30.0 Product pixel spacing in meters DEM Name dem_name (copernicus) copernicus Name of the DEM to use for processing: copernicus will use the Copernicus GLO-30 Public DEM Apply DEM Matching dem_matching (true, false) false Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files Apply Speckle Filter speckle_filter (true, false) false Apply an Enhanced Lee speckle filter Include DEM include_dem (true, false) false Include a copy of the DEM used for RTC processing in the product package Include Incidence Angle Map include_inc_map (true, false) false Include the local incidence angle map in the product package Include Scattering Area Map include_scattering_area (true, false) false Include the scattering area map in the product package Include RGB Decomposition include_rgb (true, false) false Include a false-color RGB decomposition GeoTIFF in the product package <p>Table 2: Processing Options</p>"},{"location":"guides/rtc_product_guide/#processing-options","title":"Processing Options","text":""},{"location":"guides/rtc_product_guide/#radiometry","title":"Radiometry","text":"<p>The radiometry option allows users to set their preferred backscatter coefficient normalization to either gamma-nought (gamma0 or \u03b3<sub>0</sub>) or sigma-nought (sigma0 or \u03c3<sub>0</sub>) radiometry. As illustrated in Figure 6, the scattering coefficient gamma0 is normalized by the illuminated area projected into the look direction (A<sub>\u03b3</sub> - the yellow area with the red outline in the diagram), and the sigma0 is normalized by the ground area (A<sub>\u03c3</sub> - the grey area with the purple outline in the diagram).</p> <p></p> <p>Figure 6:  Normalization areas for SAR backscatter, from David Small, 2011, Flattening Gamma: Radiometric Terrain Correction for SAR Imagery, IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 49, NO. 8, AUGUST 2011</p> <p>Although both sigma0 and gamma0 backscatter include the impact of local topography, the sensitivity of the impact is different. For applications where topographic impacts are an important consideration, gamma0 is generally the preferred choice.</p>"},{"location":"guides/rtc_product_guide/#scale","title":"Scale","text":"<p>The scale option allows users to choose the scale of the output backscatter images from the three commonly used scales for calibrated SAR values: power, amplitude, or decibel (dB). Refer to the SAR Scale section for more information.</p>"},{"location":"guides/rtc_product_guide/#pixel-spacing_1","title":"Pixel Spacing","text":"<p>The resolution parameter sets the pixel spacing of the output images. Users have the option to set a pixel spacing of 30, 20, or 10 meters. </p> <ul> <li>The 30-m product has a much smaller file size, and is easier to work with for large areas of interest. It generally aligns with the native resolution of the DEM used for RTC processing. </li> <li>The 10-m product provides much more detail of surface features, and is closer to the native resolution of the source Sentinel-1 data. The file sizes are also much larger than those of the 30-m products. </li> <li>The 20-m product may be a good compromise between the native resolution of the source SAR imagery and the source DEM. The level of detail in the image is much closer to the 10-m product than the 30-m product, while the file size is much closer to the 30-m product than the 10-m product.</li> </ul> <p>Refer to the Pixel Spacing section for more information. Note that the source Sentinel-1 imagery and the source DEM used for RTC processing are the same regardless of the option selected for the output pixel spacing.</p>"},{"location":"guides/rtc_product_guide/#dem-name","title":"DEM Name","text":"<p>The dem_name parameter selects the DEM to use for RTC processing. We use the Copernicus Global 30-m DEM. </p>"},{"location":"guides/rtc_product_guide/#dem-matching","title":"DEM Matching","text":"<p>The dem_matching option allows users to either try to coregister the SAR image to the DEM file, or simply use the Sentinel-1 orbit files for geocoding the RTC products.</p> <p>The process of terrain corrected geocoding includes 4 steps:</p> <ol> <li>Calculate the initial lookup table and simulated image with the image processing parameters and DEM.</li> <li>(Optional) Measure initial offset between simulated SAR image and actual SAR image. </li> <li>(Optional) Perform refinement of lookup table by offset measurement with respect to the simulated SAR image. </li> <li>Produce terrain geocoded SAR image and DEM in SAR range-Doppler coordinates (RDC). </li> </ol> <p>When DEM matching is applied, the optional steps 2 and 3 are performed. Using this option can improve the quality of the RTC calculations, as the features in the SAR image are matched to the features in the DEM, minimizing the offsets in geometry during the backscatter normalization calculations. Refer to the Terrain Correction section for more information.</p> <p>DEM Matching is not always beneficial, however. If the georeferencing of the DEM doesn't match the georeferencing of the Sentinel-1 imagery, DEM matching can result in variable offsets in the output images from one Sentinel-1 acquisition to the next, making it difficult to overlay images for time series analysis. Coregistration also works best when there are distinct topographic features that allow for reliable matching between the SAR image and the DEM. In areas that lack distinctive topographic features, there may also be substantial and inconsistent image offsets.</p> <p>If you are interested in optimizing the RTC calculations, and are less concerned about consistent geolocation through time, the DEM Matching option is likely a good choice. In cases where consistency is more important than accuracy, consider not applying DEM Matching, or at least testing the outputs to make sure they are suitable for your application.</p> <p>The orbit files of the Sentinel-1 data are generally quite accurate, and not applying the DEM matching should output files with consistent geolocation. While it may not optimize the RTC calculations, it may be a better option for time series analysis, where having consistent alignment of images from one acquisition to the next is more important than optimizing the backscatter normalization.</p>"},{"location":"guides/rtc_product_guide/#speckle-filter","title":"Speckle Filter","text":"<p>When the speckle_filter option is selected, an Enhanced Lee filter is applied during RTC processing to remove speckle while preserving edges of features. Speckle occurs due to interference among signal waves, as they interact with different scatterers on the surface of the earth and return to the sensor. It appears as granular noise in the image. Refer to the Speckle section of our Introduction to SAR document for more information.</p> <p>When applied, the filter is set to a dampening factor of 1, with a box size of 7x7 pixels. The number of looks depends on the multilooking treatment for the RTC processing, and is based on the pixel spacing and the input scene type. Refer to the readme file included with the RTC product to determine the number of looks used for the filter, which is the number of looks taken for RTC processing multiplied by 30.</p> <p>Applying a speckle filter can smooth the appearance of the image, but it comes at a cost to the resolution of the output RTC product. Keep in mind also that there are other speckle filters that may be better suited to a specific application. We do not currently offer any customization of the type of speckle filter used, or the parameters (window size, multilooking, dampening, etc.) used for the filter.</p> <p>You may also want to try applying other spatial speckle filters with custom settings, which can be accomplished programmatically or using GIS software. Some temporal analyses may also mitigate the impacts of speckle, such as calculating the median or mean pixel values of multiple images collected over a period of time. In both cases, it may be better not to apply a speckle filter during RTC processing.</p> <p>If you are unsure whether to apply this option, try generating some of your RTC products with and without the speckle filter applied, and check to see which product works best for your particular application. </p>"},{"location":"guides/rtc_product_guide/#optional-files","title":"Optional Files","text":"<p>In addition to the processing options, users can choose to add a number of ancillary files to the product package. These files are not included by default, as they increase the size of the product package and may not be of interest to all users. </p> <p>In Vertex, check the box in the \"Include\" section of the options to add these optional files to the product package. When using the HyP3 API or SDK, set the parameter to true.</p>"},{"location":"guides/rtc_product_guide/#dem","title":"DEM","text":"<p>Set the include_dem parameter to true to include a copy of the DEM file used for RTC processing. This DEM is not generated from the Sentinel-1 data, but is the reference DEM used for the RTC calculations. Pixel values indicate the elevation in meters. Refer to the Digital Elevation Models section for more information on the DEMs we use for RTC processing.</p> <p>This DEM file is intended as a quick reference to aid in interpretation of the RTC image, and should not be used as a stand-alone DEM product. The DEM used for RTC processing has a geoid correction applied before it is used for RTC, so elevation values in this file will differ from the source DEM. </p> <p>The DEM is resampled to match the pixel spacing of the output product, so the pixel spacing of this file is not a reflection of the resolution of the source DEM. Refer to the readme file included in the RTC product package for details on the pixel spacing of the included DEM file.</p>"},{"location":"guides/rtc_product_guide/#incidence-angle-map","title":"Incidence Angle Map","text":"<p>Set the include_inc_map parameter to true to include the local incidence angle map in the product package. The cell values in this raster indicate the angle between the incident radar beam and the direction perpendicular to the ground surface, expressed in radians.</p>"},{"location":"guides/rtc_product_guide/#scattering-area-map","title":"Scattering Area Map","text":"<p>Set the include_scattering_area parameter to true to include the scattering area map in the product package. This map expresses the scattering area for each pixel in the RTC image in square meters. The values are calculated based on the effectively illuminated gamma-0 terrain surface using a digital elevation model, the local incidence angle map, and the layover-shadow map.</p> <p>This layer can be used to generate composites using the Local Resolution Weighting method, as described in the article Wide-Area Analysis-Ready Radar Backscatter Composites by David Small et al., 2022.</p>"},{"location":"guides/rtc_product_guide/#rgb-decomposition","title":"RGB Decomposition","text":"<p>Set the include_rgb parameter to true to include a full-resolution GeoTIFF of a false-color RGB Decomposition of the co- and cross-polarized RTC values. A low-resolution false-color browse image in PNG format is included in the product package by default, but selecting this option includes the RGB Decomposition image as a GeoTIFF with the same pixel spacing as the RTC images. </p> <p>This option is only available for dual-polarization products, as it uses both the co- and cross-polarized RTC values to determine the RGB values. A full description of the approach ASF uses for generating RGB Decomposition products is available here.</p> <p>In general, blue indicates areas with low backscatter in both co- and cross-polarizations (calm water, dry sand, frozen ground), green indicates high cross-pol values (vegetation or other volume scatterers), and red indicates areas with low cross-pol but relatively high co-pol values (urban areas or sparsely vegetated landscapes). </p>"},{"location":"guides/rtc_product_guide/#radiometric-terrain-correction-workflow","title":"Radiometric Terrain Correction Workflow","text":""},{"location":"guides/rtc_product_guide/#pre-processing","title":"Pre-processing","text":"<p>The first step of pre-processing is the selection of the best DEM for the terrain correction. The DEM tiles are assembled to ensure sufficient coverage for the terrain correction of the Sentinel-1 granule. The application of the calibration parameters and multi-looking are the only pre-processing steps applied to the SAR image.</p>"},{"location":"guides/rtc_product_guide/#terrain-correction","title":"Terrain Correction","text":"<p>The terrain correction is performed in slant range geometry. A look-up table is created to map between DEM space and SAR space. The actual mapping of the initial image into projected space is only applied once to mitigate the propagation of any resampling errors. All intermediate steps only update the look-up table used for the mapping.</p> <p>By default, images are not coregistered to the DEM. While RTC results can be improved by matching imagery to a high-quality DEM, different acquisitions over the same area may not always be matched to the DEM in the same way, due in part to the presence of speckle. This can introduce spatial inconsistencies to the dataset, especially when viewing a time-series of RTC images. For consistency, we use the geolocation from the Sentinel-1 state vectors by default rather than matching the geolocation based on DEM features.</p> <p>When ordering products On Demand, the DEM Matching option is available for selection. When this option is applied, the first step is the co-registration of the SAR image with a simulated SAR image derived from the DEM. An initial offset is first attempted as a single match; if it fails, a larger number of image chips are used to determine an average offset in azimuth and range direction. This initial offset is then refined using strict matching criteria. </p> <p>Matching may fail for three different reasons: (1) no match can be found, (2) the magnitude of the residual offset errors is greater than 2 pixels, or (3) the maximum calculated offset is greater than 50 m. In any of these cases, the dead reckoning approach is taken when matching fails. This approach solely relies on the geolocations calculated from state vectors (the same approach used when DEM matching is not selected as an option) - no geolocation refinement is applied.</p>"},{"location":"guides/rtc_product_guide/#radiometric-correction","title":"Radiometric Correction","text":"<p>During processing, a surface scattering area image for the scene is calculated and saved. This projected area image is used to create the RTC product - the SAR image is multiplied by the ratio of an ellipsoidal scattering image (used during calibration) and this scattering area image. Note that this image is always projected to gamma-nought (\u03b3<sub>0</sub>). </p>"},{"location":"guides/rtc_product_guide/#geocoding","title":"Geocoding","text":"<p>In a final step, the RTC product is geocoded into map-projected space. Thus, radiometric terrain correction results in a geocoded radiometrically calibrated multi-looked image with gamma-nought (\u03b3<sub>0</sub>) power scale values by default, though there are options to process to sigma-nought (\u03c3<sub>0</sub>) radiometry and amplitude or decibel (dB) scale.</p>"},{"location":"guides/rtc_product_guide/#post-processing","title":"Post-Processing","text":"<p>After the terrain correction is completed, the RTC products are exported to GeoTIFF format. If the scene being processed is dual polarization, users have the option to add a full-resolution RGB Decomposition GeoTIFF to the RTC product package.  Side products including the DEM, layover-shadow map (always included), scattering area map, and incidence angle map are converted into GeoTIFF format. In addition, a README text file, browse images, item-specific ArcGIS-compatible XML metadata files, a log file, and a shapefile indicating the data extent are generated for the product.</p>"},{"location":"guides/rtc_product_guide/#product-packaging","title":"Product Packaging","text":""},{"location":"guides/rtc_product_guide/#naming-convention","title":"Naming Convention","text":"<p>The naming convention for the RTC products follows this pattern for its base names:</p> <p><code>S1x_yy_aaaaaaaaTbbbbbb_ppo_RTCzz_u_defklm_ssss</code></p> <p>Example: S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A</p> Element Definition Example x Sentinel-1 Platform: A, B, or C A yy Beam Mode IW aaaaaaaa Start Year-Month-Day 20180128 bbbbbb Start Hour-Minute-Second 161201 pp Polarization: Dual-pol (D) vs. Single-pol (S), Primary Polarization (H or V) DV o Orbit Type: Precise (P), Restituted (R), or Original Predicted (O) P zz Terrain Correction Pixel Spacing (m) 30 u Software Package Used: GAMMA (G) G d Gamma-0 (g) or Sigma-0 (s) Output g e Power (p), Decibel (d), or Amplitude (a) Output p f Unmasked (u) or Water Masked (w) u k Not Filtered (n) or Filtered (f) n l Entire Area (e) or Clipped Area (c) e m Dead Reckoning (d) or DEM Matching (m) d ssss Product ID FD6A <p>Table 3: Naming convention for RTC products</p>"},{"location":"guides/rtc_product_guide/#image-files","title":"Image Files","text":"<p>All files are stored in a folder named using the above convention, and the base name for each file matches the folder name. Multiple types of image files are present in this folder, and some of the files are optional. Users can choose to include the RGB Decomposition GeoTIFF, scattering area map, DEM, and incidence angle map rasters when ordering On-Demand RTC products.</p> Extension Description Example _VV.tif, _VH.tif, _HH.tif, _HV.tif Terrain corrected product stored in separate files for each available polarization in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_VV.tif .png Grayscale browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.png _rgb.png Color browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.png .kmz Zipped Google Earth image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.kmz _rgb.kmz Zipped Google Earth color image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.kmz _rgb.tif Color decomposition in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.tif _area.tif Scattering area map in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_area.tif _dem.tif DEM used for terrain correction in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_dem.tif _inc_map.tif Incidence angle file in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_inc_map.tif _ls_map.tif Layover/shadow mask in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_ls_map.tif <p>Table 4: Image files in product package</p> <p>The RTC products (one for each available polarization) are generated as 32-bit floating-point single-band GeoTIFF files, as are the incidence angle and scattering area maps. The RGB Decomposition is a 3-band unsigned 8-bit GeoTIFF file, the layover/shadow mask is a single-band unsigned 8-bit GeoTIFF, and the DEM is a 16-bit signed integer GeoTIFF. The browse images (both grayscale and color) are generated in PNG format, and are each 2048 pixels wide. Finally, KMZ files suitable for viewing in Google Earth are included. Note that colorized products (RGB Decomposition GeoTIFF or color browse PNG) can only be created for dual-polarization (SDV and SDH) granules, not for single-polarization (SSV or SSH).</p>"},{"location":"guides/rtc_product_guide/#metadata-files","title":"Metadata Files","text":"<p>The product package also includes a number of metadata files.</p> Extension Description Example .README.md.txt README file S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.README.md.txt .log Log file of the processing steps S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.log .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_VV.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.png.xml .png.aux.xml Geolocation metadata for PNG browse images S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.png.aux.xml <p>Table 5: Metadata files and their extensions</p>"},{"location":"guides/rtc_product_guide/#readme-file","title":"README File","text":"<p>The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with RTC products should start by reading this README file, which will give some background on each of the files included in the product folder.</p>"},{"location":"guides/rtc_product_guide/#arcgis-compatible-xml-files","title":"ArcGIS-Compatible XML Files","text":"<p>There is an ArcGIS-compatible XML file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated XML file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly.</p> <p>ArcGIS users should take care not to change these XML files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS.</p> <p>Those not using ArcGIS will still find the contents of these XML files useful, but will have to contend with the XML tagging when viewing the files as text or in a browser.</p>"},{"location":"guides/rtc_product_guide/#auxiliary-geolocation-files","title":"Auxiliary Geolocation Files","text":"<p>Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms.</p>"},{"location":"guides/rtc_product_guide/#log-file","title":"Log File","text":"<p>A log file detailing the processing parameters and outputs is also included for reference.</p>"},{"location":"guides/rtc_product_guide/#shapefile","title":"Shapefile","text":"<p>A shapefile indicating the extent of the RTC data coverage is included in the package.</p> Extension Description Example _shape.dbf _shape.prj _shape.shp _shape.shx Shapefile (.shp) and supporting files S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_shape.shp <p>Table 6: Shapefile files and their extensions</p>"},{"location":"guides/rtc_product_guide/#data-access","title":"Data Access","text":"<p>Refer to the Downloads page for more information on viewing and downloading  Sentinel-1 RTC On Demand products in Vertex or programmatically. Once processing is complete, download links  for On Demand products are valid for 14 days.</p> <p>Step-by-step instructions for finding and downloading RTC On Demand products in Vertex are available in the  Access Products section of the  RTC On Demand! interactive StoryMap tutorial.</p>"},{"location":"guides/rtc_product_guide/#sar-scales","title":"SAR Scales","text":""},{"location":"guides/rtc_product_guide/#power-scale","title":"Power Scale","text":"<p>The default output of Sentinel-1 RTC products from HyP3 is in power scale. The values in this scale are generally very close to zero, so the dynamic range of the RTC image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the RTC dataset, but may not always be the best option for data visualization.</p> <p>When viewing an RTC image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values.</p> <p>In some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and decibel (dB).</p>"},{"location":"guides/rtc_product_guide/#amplitude-scale","title":"Amplitude Scale","text":"<p>Values in the amplitude scale are the square root of the power scale values. This brightens the darker pixels (values &lt;1) and darkens the brighter pixels (values &gt;1), narrowing the dynamic range of the image. </p> <p>In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios, as described in the Change Detection Using RTC Data use case example.</p>"},{"location":"guides/rtc_product_guide/#decibel-db-scale","title":"Decibel (dB) Scale","text":"<p>The decibel (dB) scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale allows for better differentiation among very dark pixels. </p> <p>This is often a good scale to use for identifying water on the landscape; the water pixels generally remain very dark compared to the much brighter pixels of the surrounding landscape. Refer to the Identifying Surface Water use case example for more information.</p> <p>This scale is not always the best choice for general visualization of RTC products, as it can give a washed-out appearance to terrain features. In addition, because it is a logarithmic scale, dB pixel values are not appropriate for some types of statistical analyses.</p>"},{"location":"guides/rtc_product_guide/#rtc-use-examples","title":"RTC Use Examples","text":"<p>The RTC products are presented as Cloud-Optimized GeoTIFFs (COGs), a user-friendly format that is compatible with GIS software. The products include pre-generated overviews, so users will not need to generate pyramids to display the images efficiently in a GIS or web-mapping environment.</p> <p>The following sections present examples of how one might use RTC datasets to identify areas of change and integrate RTC datasets into other datasets for enhanced results. We also present a bibliography of some of the scientific literature making use of Sentinel-1 RTC datasets.</p>"},{"location":"guides/rtc_product_guide/#change-detection-using-rtc-data","title":"Change Detection Using RTC Data","text":"<p>There are a number of ways that SAR data sets can be used to identify areas of change. Here are two examples of what you can do in a GIS environment.</p>"},{"location":"guides/rtc_product_guide/#seasonal-change","title":"Seasonal Change","text":"<p>Stacking RTC images into a multiband image (Figure 7) allows the user to display different times of the year at the same time, using the color bands to highlight areas that differ in radar backscatter values from one month to the next.</p> <p>To generate this type of image, choose three images that capture different seasons or months of interest. These can either be individual RTC images from different times of the year, or rasters displaying the monthly median calculated from multiple RTC images collected in the same month.</p> <p>Combine the three images into a multiband raster and assign each to a different color band. The resulting RGB image highlights areas where there are distinctive differences among the three source image values.</p> <p></p> <p>Figure 7: Monthly median VH gamma-0 power values for May, July and September, displayed as a multiband RGB (May, July, Sept) image. Contains modified Copernicus Sentinel data 2017, processed by ESA.</p>"},{"location":"guides/rtc_product_guide/#quantifying-change-over-time","title":"Quantifying Change over Time","text":"<p>A simple and informative approach to change detection is the calculation of the log difference between two RTC datasets from different dates. By calculating Log10(date2/date1) and applying a classified symbology, it is easy to identify areas where change occurred, as well as the direction of the change. Negative values indicate a decrease in radar backscatter over time, while positive values indicate an increase in backscatter.</p> <p>In the example below (Figure 8), RTC images from before and after heavy rains caused a dam breach. The area where the reservoir was located displays a significant increase in backscatter (symbolized in red). This positive change is driven by land that was once covered by standing water, which generally has very low backscatter, now being exposed saturated soil, which generally returns very high backscatter values. In surrounding areas, decreases in radar backscatter (symbolized by blue) are possibly the result of agricultural fields undergoing desiccation/hardening of the surface soil following the heavy rainfall and standing water. Areas with little change in backscatter are displayed in yellow.</p> <p></p> <p>Figure 8: Log Difference Raster with Classified Symbology. Contains modified Copernicus Sentinel data 2020, processed by ESA.</p>"},{"location":"guides/rtc_product_guide/#identifying-surface-water","title":"Identifying Surface Water","text":"<p>Calm surface water has a very low radar cross section. Because freshwater has a high dielectric constant, most of the signal is reflected off the smooth surface of the water and away from the sensor, resulting in little to no backscatter. As such, surface water can often be delineated using a simple threshold value, where all pixels below the threshold are assumed to be water. It is often best to use datasets in dB scale for this process (refer to the dB Scale Section).</p> <p>When using the threshold approach, surface water can be easily visualized by applying a classified symbology with two classes, using the threshold as the break point between the classes. There is no universal threshold value; it will need to be determined based on the surface water characteristics in the RTC image. </p> <p>When an RTC image contains significant surface water coverage, there is often a bimodal distribution of pixel values. The first peak in a histogram of the pixel values for the image can be expected to contain mostly water pixels, while the second peak contains all remaining pixels. A good first step in selecting a threshold value is to set the break point between classes at the lowest point between those two peaks, then adjust the value as needed to generate a good water mask for the image (Figure 9).</p> <p></p> <p>Figure 9: Setting the break point to fall between the two peaks of the histogram</p> <p>Once you have determined the appropriate threshold (Figure 10), you can reclassify the RTC image to include only those pixels that fall below the threshold value, providing a water mask that can be used for analysis or to overlay with other imagery to show the water extent. </p> <p></p> <p>Figure 10: Water Mask. Contains modified Copernicus Sentinel data 2020, processed by ESA.</p>"},{"location":"guides/rtc_product_guide/#combination-of-rtc-imagery-with-other-remote-sensing-data","title":"Combination of RTC Imagery with other Remote Sensing Data","text":"<p>One of the main advantages of using RTC imagery is that it aligns geographically with other geospatial datasets. This makes it possible to combine SAR data with other remote sensing data, such as optical data. In the example below, the backscatter information of the Sentinel-1 SAR image (Figure 11) is used to enhance the spectral information of the optical Landsat 8 image (Figure 12) in the urban area of Pavia, Italy. </p> <p></p> <p>Figure 11: Sentinel-1 RTC image.</p> <p></p> <p>Figure 12: False color composite (bands 5, 4, 3) of a Landsat 8 image</p> <p>Figure 13 shows the image fusion result of an IHS transformation. In this transformation, the color channels red, green and blue (RGB) are first converted into a different color representation: intensity, hue and saturation (IHS). In the second step, the optical intensity is replaced by the SAR image values before IHS is transformed back to RGB.</p> <p></p> <p>Figure 13: Image fusion result of SAR and optical imagery</p> <p>The color values for the two rivers in the SAR image are far more similar to each other than in the optical image. The vegetated areas (highlighted in red) show up more uniformly in the data fusion result than in the optical false color composite image. Image fusion uses the complementary nature of the different sources to generate an enhanced product.</p>"},{"location":"guides/rtc_product_guide/#arcgis-toolbox","title":"ArcGIS Toolbox","text":"<p>ASF has developed a custom ArcGIS Toolbox for working with RTC datasets in either ArcGIS Desktop or ArcGIS Pro. It includes tools for converting between different SAR scales, calculating the log difference between two images, generating RGB Decomposition (false-color) products, and reclassifying a raster to generate a water mask. For more information and to download the toolbox, visit our website:  https://asf.alaska.edu/how-to/data-tools/gis-tools/.</p>"},{"location":"guides/rtc_product_guide/#application-examples-in-the-literature","title":"Application Examples in the Literature","text":"<p>The following journal articles represent some of the work being done using Radiometric Terrain Corrected Sentinel-1 data sets.</p>"},{"location":"guides/rtc_product_guide/#crop-monitoring","title":"Crop Monitoring","text":"<p>Clauss, K., Ottinger M. and Kuenzer, C. 2018. Mapping rice areas with Sentinel-1 time series and superpixel segmentation. International Journal of Remote Sensing, 39(5):1399-1420. DOI: 10.1080/01431161.2017.1404162</p> <p>Nguyen, D.B., Gruber A. and Wagner, W. 2016. Mapping rice extent and cropping scheme in the Mekong Delta using Sentinel-1A data. Remote Sensing Letters, 7(12):1209-1218. DOI: 10.1080/2150704X.2016.1225172</p>"},{"location":"guides/rtc_product_guide/#disaster-response","title":"Disaster Response","text":"<p>Markert, K.N., Chishtie, F., Anderson, E.R., Saah, D., Griffin, R.E. 2018. On the merging of optical and SAR satellite imagery for surface water mapping applications. Results In Physics, 9:275-277. DOI: 10.1016/j.rinp.2018.02.054</p> <p>Twele, A., Cao, W., Plank, S. and Martinis, S. 2016. Sentinel-1-based flood mapping: a fully automated processing chain. International Journal of Remote Sensing, 37(13):2990-3004. DOI: 10.1080/01431161.2016.1192304</p>"},{"location":"guides/rtc_product_guide/#land-classification-and-change-detection","title":"Land Classification and Change Detection","text":"<p>Muro, J., Canty, M., Conradsen, K., H\u00fcttich, C., Nielsen, A.A., Skriver, H., Remy, F., Strauch, A., Thonfeld, F. and Menz, G. 2016. Short-Term change detection in wetlands using Sentinel-1 time series. Remote Sensing, 8(10):795. DOI: 10.3390/rs8100795</p> <p>R\u00fcetschi, M., Schaepman, M.E., Small, D. 2018. Using Multitemporal Sentinel-1 C-band backscatter to monitor phenology and classify deciduous and coniferous forests in Northern Switzerland. Remote Sensing, 10(1):55. DOI: 10.3390/rs10010055</p>"},{"location":"news/hyp3_plus_introduction/","title":"Introducing HyP3+!  A new, sustainable HyP3 deployment","text":"<p>20 October 2025</p> <p>ASF's on-demand processing service, HyP3, has created more than 11 million analysis-ready Sentinel-1 products since its public debut in October 2020. Five years later, our output has grown to around half a million products a month for more than 1,000 users. </p> <p>This free service is currently made possible through generous funding from NASA.</p> <p>To ensure this service is accessible to the entire community, we have created a credit system and implemented a round-robin processing queue. The current monthly allocation of 8,000 credits per user has served the majority of our users well, but we regularly receive requests for credit increases. We\u2019re happy to grant them when our budget allows!</p> <p>Growing uncertainty around federal funding and the continued growth of on-demand processing needs could soon leave us unable to meet the needs of our user community with our existing HyP3 service. </p> <p>To support on-demand processing going forward, we are piloting HyP3+, a user-sustained deployment of HyP3 available to the public with the ability to purchase processing credits using a credit card. It works just like our basic HyP3 service support by NASA, but you are not constrained by monthly credit allotments, and output products are retained for 30 days instead of 14. For more information, see the HyP3 family feature comparison table.</p> <p>Our on-demand processing platform has become a core component of ASF\u2019s mission to make remote-sensing data accessible, so we are pricing our HyP3+ credits to just cover our costs. Any surplus will be reinvested into our user community and our mission. </p> <p>Importantly, we remain steadfastly committed to continuing our free on-demand processing service as long as it\u2019s funded, and we will continue to fulfill credit increase requests as long as our budget allows. </p> <p>We are providing HyP3+ simply as an option if your allotment of free credits is insufficient for your processing needs. </p> <p>Have questions or concerns? Please get in touch with ASF User Services: uso@asf.alaska.edu </p>"},{"location":"tools/arcgis_toolbox/","title":"ArcGIS Toolbox","text":"<p>The ASF_Tools ArcGIS Python Toolbox can be used with either ArcGIS Desktop or ArcGIS Pro, and contains tools that perform geoprocessing tasks useful for working with Synthetic Aperture Radar (SAR) data. The tools were designed to be used with Sentinel-1 Radiometric Terrain Corrected (RTC) SAR datasets, such as those available on-demand using ASF's Data Search - Vertex portal, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets.</p> <p>The Toolbox is distributed as a zipped archive including the .pyt Toolbox script and associated .xml files. There is an XML file for the toolbox itself and one for each of the tools it contains. These XML files contain the metadata displayed in the item descriptions and tool help windows in ArcGIS, and must be kept in the same directory as the Python Toolbox (.pyt) file, or the information they contain will no longer be accessible.</p>"},{"location":"tools/arcgis_toolbox/#toolbox-contents","title":"Toolbox Contents","text":""},{"location":"tools/arcgis_toolbox/#unzip-files-tool","title":"Unzip Files Tool","text":"<p>This tool assists in file management when downloading .zip files from ASF. It could be used to extract to a specified location any zip files with an additional internal directory containing the individual files. The tool deletes the original zip files once they are extracted, and is especially helpful when dealing with file paths that are so long that they are beyond the maximum allowed in default Windows unzip utilities.</p>"},{"location":"tools/arcgis_toolbox/#scale-conversion-tool","title":"Scale Conversion Tool","text":"<p>This tool converts pixel values in calibrated SAR datasets (such as RTC rasters) from power or amplitude scale into power, amplitude or dB scale. This is an application specific to SAR data values/scales.</p>"},{"location":"tools/arcgis_toolbox/#reclassify-rtc-tool","title":"Reclassify RTC Tool","text":"<p>This tool generates a raster that includes only those pixels below a user-defined threshold value, and is designed for isolating water pixels. While intended for RTC files in dB scale, this tool could be used for any application where the user is interested in generating a spatial mask for values below a given threshold in a single-band raster.</p>"},{"location":"tools/arcgis_toolbox/#log-difference-tool","title":"Log Difference Tool","text":"<p>This tool compares two rasters by calculating the log difference on a pixel-by-pixel basis to identify areas where backscatter values have changed over time. While intended for RTC files in amplitude scale, this tool could be used to compare the pixel values of any two single-band rasters, as long as there are no negative values (NoData values will be returned for pixels with a negative number in either of the datasets).</p>"},{"location":"tools/arcgis_toolbox/#rgb-decomposition-tool","title":"RGB Decomposition Tool","text":"<p>This tool generates an RGB image using the co- and cross-polarized datasets from an RTC product. Input datasets can be in either amplitude or power scale, and the primary polarization can be either vertical (VV/VH) or horizontal (HH/HV). Additional documentation is available regarding the calculations used and the interpretation of these false-color images.</p>"},{"location":"tools/arcgis_toolbox/#prerequisites","title":"Prerequisites","text":"<p>Users must have either ArcGIS Desktop (ArcMap) or ArcGIS Pro installed and licensed on their computer. The Toolbox has been tested with Desktop versions 10.6.1 and 10.7.1 and Pro versions 2.4.2, 2.5.x and 2.6.1, but it may work with earlier versions as well.</p> <p>Note that several of the tools require the Spatial Analyst extension. Users who do not have licensing for this extension in ArcGIS will not be able to use many of the included tools.</p>"},{"location":"tools/arcgis_toolbox/#to-install-the-toolbox","title":"To install the Toolbox","text":"<ul> <li>Download the zip file and extract the contents to any directory accessible by the computer running ArcGIS.</li> <li> <p>Ensure that the Spatial Analyst extension is licensed and enabled.</p> </li> </ul>"},{"location":"tools/arcgis_toolbox/#arcgis-desktop-arcmap","title":"ArcGIS Desktop (ArcMap)","text":"<ul> <li>Click on the Customize menu in ArcMap and select Extensions\u2026 </li> <li>Check the box next to Spatial Analyst and click the Close button at the bottom of the Extensions window. <ul> <li>If you are unable to check this box, you do not have access to the Spatial Analyst extension and will not be able to make use of tools requiring this extension.</li> </ul> </li> </ul>"},{"location":"tools/arcgis_toolbox/#arcgis-pro","title":"ArcGIS Pro","text":"<ul> <li>Click on the Project tab and select the Licensing tab. </li> <li>In the list of Esri Extensions, scroll down to verify that the Spatial Analyst is licensed and enabled. <ul> <li>If it is not, an organization administrator will need to enable the extension in your user account. </li> <li>If your organization does not have a license available for you to use, you will not be able to make use of tools requiring this extension.</li> </ul> </li> </ul>"},{"location":"tools/arcgis_toolbox/#using-the-toolbox","title":"Using the Toolbox","text":"<p>In the ArcMap Catalog window or the ArcGIS Pro Catalog pane/view, navigate to the directory containing the toolbox (create a new folder connection if necessary). - To open the Catalog window in ArcMap, click on the Windows menu and select Catalog. - To open the Catalog pane or view in ArcGIS Pro, click the View tab and click on either the Catalog Pane or Catalog View button.</p> <p>Note that if you explore the extracted contents of the zip file outside of the ArcGIS environment, the directory will contain one .pyt file and a number of .xml files.</p> <p>In the ArcGIS Catalog window/pane/view, only the Toolbox is displayed, and when it is expanded, all of the Tools contained in the Toolbox script are displayed. The XML files are automatically referenced when ArcGIS requires the information they contain, and do not appear as additional files in the ArcGIS Catalog environment. The XML files must remain in the same directory as the .pyt file, and their filenames should not be changed.</p> <ul> <li>Double-click the ASF_Tools.pyt file to display the Tools (Scripts) included in the toolbox.</li> <li>Double-click on a Tool (displayed with a Script icon) to launch the dialog box or geoprocessing pane, as you would for any other ArcGIS Tool/Script.</li> <li>Enter the parameters as prompted and click the OK button to execute the tool.</li> </ul> <p>Note that output products are not automatically added to a project by default. You must navigate to them in the Catalog window/pane/view (or using the Add Data dialog) and add them to your project if desired.</p>"},{"location":"tools/arcgis_toolbox/#tool-help","title":"Tool Help","text":"<p>The XML files included in the zip file are accessed when a user views the metadata for the toolbox, individual tools, or even different fields within the tool dialog.</p>"},{"location":"tools/arcgis_toolbox/#accessing-help-from-within-the-tool-dialog-box","title":"Accessing Help from within the Tool Dialog Box","text":""},{"location":"tools/arcgis_toolbox/#arcgis-desktop","title":"ArcGIS Desktop","text":"<ul> <li>Click on the Show Help button at the bottom of the tool window to open the help panel. <ul> <li>This panel will display information about the tool in general if no field is activated. </li> <li>If the user clicks on any of the parameter fields, information specific to that parameter will be displayed.</li> </ul> </li> <li>Click on the Tool Help button at the bottom of the Help pane to open another window that displays most of the information that would be displayed in the tool\u2019s Item Description. </li> </ul>"},{"location":"tools/arcgis_toolbox/#arcgis-pro_1","title":"ArcGIS Pro","text":"<ul> <li>When you hover over any of the parameter fields in the tool dialog, a blue i appears. Hover over or click the blue i icon to view helpful tips specific to that parameter.</li> <li>Hover over the blue question mark at the top of the geoprocessing pane to display information about the tool. Click on it to open the full tool description in a browser window. </li> </ul>"},{"location":"tools/arcgis_toolbox/#accessing-help-from-the-catalog-interface","title":"Accessing Help from the Catalog Interface","text":""},{"location":"tools/arcgis_toolbox/#arcgis-desktop_1","title":"ArcGIS Desktop","text":"<p>ArcCatalog displays the information contained in the xml metadata files in the Description tab for the toolbox and each tool.</p> <p>In the ArcMap Catalog window, the Item Description for the toolbox or any of its constituent tools displays the xml content. - Right-click the toolbox or tool in the Catalog window and select Item Description to view the information.</p>"},{"location":"tools/arcgis_toolbox/#arcgis-pro_2","title":"ArcGIS Pro","text":"<p>The xml metadata is displayed in the Metadata tab in the Catalog view. - Right-click a tool in the Catalog pane and select View Metadata to open the Metadata tab for the item in the Catalog view. OR - Open the Catalog View directly to navigate to the tool and select the Metadata tab.</p>"},{"location":"tools/asf_tools/","title":"ASF Tools for Python","text":"<p><code>asf_tools</code> is a Python package for working with Synthetic Aperture Radar (SAR) data. It was designed for working with datasets generated by HyP3, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets.</p>"},{"location":"tools/asf_tools/#install","title":"Install","text":"<p>In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda. It is also possible to use Python virtual environments, but installation of non-python dependencies (e.g., <code>gdal</code>) can be challenging. </p> <p><code>asf_tools</code> can be installed into a conda environment with:</p> <pre><code>conda install -c conda-forge asf_tools\n</code></pre> <p>or into a virtual environment with:</p> <pre><code>python -m pip install asf_tools\n</code></pre>"},{"location":"tools/asf_tools/#running-as-a-docker-container","title":"Running as a Docker container","text":"<p>We also publish a Docker image for <code>asf_tools</code>, with all the dependencies pre-installed, to the GitHub Container Registry: https://github.com/ASFHyP3/asf-tools/pkgs/container/asf-tools.</p> <p>You can pull an image with the latest released version of <code>asf_tools</code>  with the command: <pre><code>docker pull ghcr.io/asfhyp3/asf-tools:latest\n</code></pre></p> <p>Or, the development version with:  <pre><code>docker pull ghcr.io/asfhyp3/asf-tools:test\n</code></pre></p> <p>And then run the container with: <pre><code>docker run --rm -it ghcr.io/asfhyp3/asf-tools:latest\n</code></pre> which will drop you into a <code>bash</code> shell inside the container with an active <code>asf-tools</code> conda environment. </p> <p>To move data between your local (host) machine and the container, you can mount a  volume with: <pre><code>docker run --rm -it -v /path/to/data:/home/conda/data ghcr.io/asfhyp3/asf-tools:latest\n</code></pre></p>"},{"location":"tools/asf_tools/#quick-usage","title":"Quick Usage","text":""},{"location":"tools/asf_tools/#local-resolution-weighted-composite","title":"Local Resolution Weighted Composite","text":"<p>The <code>make_composite</code> tool allows you to create a local-resolution-weighted composite from a set of Sentinel-1 RTC products (D. Small, 2012). It is intended to be used with RTC products generated by ASF HyP3.</p> <p>You will need to request RTC products using the <code>Include Scattering Area</code> option, then download and unzip them into an empty directory.</p> <p>To generate a composite of the co-polarization images, navigate to the directory containing the unzipped RTC products and run: <pre><code>make_composite VV-composite */*VV.tif\n</code></pre></p> <p>To generate a composite of the cross-polarization images, navigate to the directory containing the unzipped RTC products and run: <pre><code>make_composite VH-composite */*VH.tif\n</code></pre></p>"},{"location":"tools/asf_tools/#usage-tip","title":"Usage Tip","text":"<p>Because the imagery has been radiometrically terrain corrected (RTC), geometric and radiometric distortions have been removed from the files to be composited. One the strong points of LRW composites is that you combine both ascending and descending datatakes into a single product. In this manner no layover or shadow masks are required - what is shadowed on an ascending pass is visible in a descending pass and vice-versa. Thus, not only is it possible to combine ascending and descending, but it is highly encouraged. Using many datatakes from both the ascending and descending satellite passes will make the best composites possible.</p>"},{"location":"tools/asf_tools/#about-local-resolution-weighting-lrw","title":"About Local Resolution Weighting (LRW)","text":"<p>In an LRW composite, each satellite pass contributes to creating the output pixels. The amount of this contribution is scaled by the inverse of the scattering area used during terrain correction (thus the need for requesting the area map option of HyP3 RTC). The inverse of the surface scattering area, also referred to as local resolution, is multiplied by each pixel's backscatter value. The results of all of the images covering any single pixel are then summed. This total is then divided by the sum of the weights used to get the output average backscatter.</p>"},{"location":"tools/asf_tools/#water-extent-mapping","title":"Water extent mapping","text":"<p>[!WARNING] The HydroSAR codes (<code>flood_map</code>, <code>water_map</code> and <code>hand</code> modules) are being moved to the HydroSAR project repository and will be provided in a new pip/conda installable package <code>hydrosar</code>.</p> <p>The <code>asf_tools.hydrosar</code> subpackage will be removed in a future release.</p> <p>The <code>water_map</code> tool allows you to create a surface water extent map from a Sentinel-1 dual-pol (VV+VH) RTC product. It is intended to be used with RTC products generated by ASF HyP3.</p> <p>Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the RTC images is required, and preferably derived from the same DEM used to correct the RTC images -- the quality of the HAND used is directly tied to the quality of the output water extent map.</p> <p>To make a water extent map, run: <pre><code>water_map [OUT_RASTER] [VV_RASTER] [VH_RASTER] [HAND_RASTER]\n</code></pre></p> <p>For more information and to see the options available, see: <pre><code>water_map --help\n</code></pre></p> <p>For details on the algorithm see the <code>asf_tools.water_map.make_water_map</code> docstring.</p>"},{"location":"tools/asf_tools/#flood-depth-mapping","title":"Flood depth mapping","text":"<p>[!WARNING] The HydroSAR codes (<code>flood_map</code>, <code>water_map</code> and <code>hand</code> modules) are being moved to the HydroSAR project repository and will be provided in a new pip/conda installable package <code>hydrosar</code>.</p> <p>The <code>asf_tools.hydrosar</code> subpackage will be removed in a future release.</p> <p>The <code>flood_map</code> tool allows you to create an estimated flood depth map from the surface water extent map created by the <code>water_map</code> tool. </p> <p>Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the surface water extent map is required. An ideal candidate is the HAND image created  by the <code>water_map</code> tool. </p> <p>To make a flood depth map, run: <pre><code>flood_map [OUT_RASTER] [SURFACE_WATER_MAP] [HAND_RASTER]\n</code></pre></p> <p>For more information and to see the options available, see: <pre><code>flood_map --help\n</code></pre></p> <p>For details on the algorithm see the <code>asf_tools.flood_map.make_flood_map</code> docstring.</p>"},{"location":"tools/asf_tools/#water-mask-dataset-generation","title":"Water Mask Dataset Generation","text":"<p>The <code>asf_tools.watermasking</code> sub-package allows you to create a watermasking dataset  over an arbitrary ROI using OpenStreetMap and ESA WorldCover data. Note, this program requires <code>osmium-tool</code>. See the watermasking subpackage readme  for more information on setup and usage.</p>"},{"location":"tools/asf_tools_api/","title":"<code>asf_tools</code> v0.8.4 API Reference","text":"<p>Tools developed by ASF for working with SAR data</p>"},{"location":"tools/asf_tools_api/#asf_tools.composite","title":"<code>composite</code>","text":"<p>Create a local-resolution-weighted composite from Sentinel-1 RTC products.</p> <p>Create a local-resolution-weighted composite from a set of Sentinel-1 RTC products (D. Small, 2012). The local resolution, defined as the inverse of the local contributing (scattering) area, is used to weight each RTC products' contributions to the composite image on a pixel-by-pixel basis. The composite image is created as a Cloud Optimized GeoTIFF (COG). Additionally, a COG specifying the number of rasters contributing to each composite pixel is created.</p> References <p>David Small, 2012: https://doi.org/10.1109/IGARSS.2012.6350465</p>"},{"location":"tools/asf_tools_api/#asf_tools.composite.get_area_raster","title":"<code>get_area_raster(raster)</code>","text":"<p>Determine the path of the area raster for a given backscatter raster based on naming conventions for HyP3 RTC products</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>path of the backscatter raster, e.g. S1A_IW_20181102T155531_DVP_RTC30_G_gpuned_5685_VV.tif</p> required <p>Returns:</p> Name Type Description <code>area_raster</code> <code>str</code> <p>path of the area raster, e.g. S1A_IW_20181102T155531_DVP_RTC30_G_gpuned_5685_area.tif</p> Source code in <code>asf_tools/composite.py</code> <pre><code>def get_area_raster(raster: str) -&gt; str:\n\"\"\"Determine the path of the area raster for a given backscatter raster based on naming conventions for HyP3 RTC\n    products\n\n    Args:\n        raster: path of the backscatter raster, e.g. S1A_IW_20181102T155531_DVP_RTC30_G_gpuned_5685_VV.tif\n\n    Returns:\n        area_raster: path of the area raster, e.g. S1A_IW_20181102T155531_DVP_RTC30_G_gpuned_5685_area.tif\n    \"\"\"\n    return '_'.join(raster.split('_')[:-1] + ['area.tif'])\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.composite.get_full_extent","title":"<code>get_full_extent(raster_info)</code>","text":"<p>Determine the corner coordinates and geotransform for the full extent of a set of rasters</p> <p>Parameters:</p> Name Type Description Default <code>raster_info</code> <code>dict</code> <p>A dictionary of gdal.Info results for the set of rasters</p> required <p>Returns:</p> Name Type Description <code>upper_left</code> <p>The upper left corner of the extent as a tuple</p> <code>upper_right</code> <p>The lower right corner of the extent as a tuple</p> <code>geotransform</code> <p>The geotransform of the extent as a list</p> Source code in <code>asf_tools/composite.py</code> <pre><code>def get_full_extent(raster_info: dict):\n\"\"\"Determine the corner coordinates and geotransform for the full extent of a set of rasters\n\n    Args:\n        raster_info: A dictionary of gdal.Info results for the set of rasters\n\n    Returns:\n        upper_left: The upper left corner of the extent as a tuple\n        upper_right: The lower right corner of the extent as a tuple\n        geotransform: The geotransform of the extent as a list\n    \"\"\"\n    upper_left_corners = [info['cornerCoordinates']['upperLeft'] for info in raster_info.values()]\n    lower_right_corners = [info['cornerCoordinates']['lowerRight'] for info in raster_info.values()]\n\n    ulx = min([ul[0] for ul in upper_left_corners])\n    uly = max([ul[1] for ul in upper_left_corners])\n    lrx = max([lr[0] for lr in lower_right_corners])\n    lry = min([lr[1] for lr in lower_right_corners])\n\n    log.debug(f'Full extent raster upper left: ({ulx, uly}); lower right: ({lrx, lry})')\n\n    trans = []\n    for info in raster_info.values():\n        # Only need info from any one raster\n        trans = info['geoTransform']\n        break\n\n    trans[0] = ulx\n    trans[3] = uly\n\n    return (ulx, uly), (lrx, lry), trans\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.composite.get_target_epsg_code","title":"<code>get_target_epsg_code(codes)</code>","text":"<p>Determine the target UTM EPSG projection for the output composite</p> <p>Parameters:</p> Name Type Description Default <code>codes</code> <code>list[int]</code> <p>List of UTM EPSG codes</p> required <p>Returns:</p> Name Type Description <code>target</code> <code>int</code> <p>UTM EPSG code</p> Source code in <code>asf_tools/composite.py</code> <pre><code>def get_target_epsg_code(codes: list[int]) -&gt; int:\n\"\"\"Determine the target UTM EPSG projection for the output composite\n\n    Args:\n        codes: List of UTM EPSG codes\n\n    Returns:\n        target: UTM EPSG code\n    \"\"\"\n    # use median east/west UTM zone of all files, regardless of hemisphere\n    # UTM EPSG codes for each hemisphere will look like:\n    #   North: 326XX\n    #   South: 327XX\n    valid_codes = list(range(32601, 32661)) + list(range(32701, 32761))\n    if bad_codes := set(codes) - set(valid_codes):\n        raise ValueError(f'Non UTM EPSG code encountered: {bad_codes}')\n\n    hemispheres = [c // 100 * 100 for c in codes]\n    # if even modes, choose lowest (North)\n    target_hemisphere = min(multimode(hemispheres))\n\n    zones = sorted([c % 100 for c in codes])\n    # if even length, choose fist of median two\n    target_zone = zones[(len(zones) - 1) // 2]\n\n    return target_hemisphere + target_zone\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.composite.make_composite","title":"<code>make_composite(out_name, rasters, resolution=None)</code>","text":"<p>Creates a local-resolution-weighted composite from Sentinel-1 RTC products</p> <p>Parameters:</p> Name Type Description Default <code>out_name</code> <code>str</code> <p>The base name of the output GeoTIFFs</p> required <code>rasters</code> <code>list[str]</code> <p>A list of file paths of the images to composite</p> required <code>resolution</code> <code>float | None</code> <p>The pixel size for the output GeoTIFFs</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out_raster</code> <p>Path to the created composite backscatter GeoTIFF</p> <code>out_counts_raster</code> <p>Path to the created GeoTIFF with counts of scenes contributing to each pixel</p> Source code in <code>asf_tools/composite.py</code> <pre><code>def make_composite(out_name: str, rasters: list[str], resolution: float | None = None):\n\"\"\"Creates a local-resolution-weighted composite from Sentinel-1 RTC products\n\n    Args:\n        out_name: The base name of the output GeoTIFFs\n        rasters: A list of file paths of the images to composite\n        resolution: The pixel size for the output GeoTIFFs\n\n    Returns:\n        out_raster: Path to the created composite backscatter GeoTIFF\n        out_counts_raster: Path to the created GeoTIFF with counts of scenes contributing to each pixel\n    \"\"\"\n    if not rasters:\n        raise ValueError('Must specify at least one raster to composite')\n\n    raster_info = {}\n    for raster in rasters:\n        raster_info[raster] = gdal.Info(raster, format='json')\n        # make sure gdal can read the area raster\n        gdal.Info(get_area_raster(raster))\n\n    target_epsg_code = get_target_epsg_code([get_epsg_code(info) for info in raster_info.values()])\n    log.debug(f'Composite projection is EPSG:{target_epsg_code}')\n\n    if resolution is None:\n        resolution = max([info['geoTransform'][1] for info in raster_info.values()])\n    log.debug(f'Composite resolution is {resolution} meters')\n\n    # resample rasters to maximum resolution &amp; common UTM zone\n    with TemporaryDirectory(prefix='reprojected_') as temp_dir:\n        raster_info = reproject_to_target(\n            raster_info,\n            target_epsg_code=target_epsg_code,\n            target_resolution=resolution,\n            directory=temp_dir,\n        )\n\n        # Get extent of union of all images\n        full_ul, full_lr, full_trans = get_full_extent(raster_info)\n\n        nx = int(abs(full_ul[0] - full_lr[0]) // resolution)\n        ny = int(abs(full_ul[1] - full_lr[1]) // resolution)\n\n        outputs = np.zeros((ny, nx))\n        weights = np.zeros(outputs.shape)\n        counts = np.zeros(outputs.shape, dtype=np.int8)\n\n        for raster, info in raster_info.items():\n            log.info(f'Processing raster {raster}')\n            log.debug(\n                f'Raster upper left: {info[\"cornerCoordinates\"][\"upperLeft\"]}; '\n                f'lower right: {info[\"cornerCoordinates\"][\"lowerRight\"]}'\n            )\n\n            values = read_as_array(raster)\n\n            area_raster = get_area_raster(raster)\n            areas = read_as_array(area_raster)\n\n            ulx, uly = info['cornerCoordinates']['upperLeft']\n            y_index_start = int((full_ul[1] - uly) // resolution)\n            y_index_end = y_index_start + values.shape[0]\n\n            x_index_start = int((ulx - full_ul[0]) // resolution)\n            x_index_end = x_index_start + values.shape[1]\n\n            log.debug(\n                f'Placing values in output grid at {y_index_start}:{y_index_end} and {x_index_start}:{x_index_end}'\n            )\n\n            mask = values == 0\n            raster_weights = 1.0 / areas\n            raster_weights[mask] = 0\n\n            outputs[y_index_start:y_index_end, x_index_start:x_index_end] += values * raster_weights\n            weights[y_index_start:y_index_end, x_index_start:x_index_end] += raster_weights\n            counts[y_index_start:y_index_end, x_index_start:x_index_end] += ~mask\n\n    del values, areas, mask, raster_weights\n\n    # Divide by the total weight applied\n    outputs /= weights\n    del weights\n\n    out_raster = write_cog(f'{out_name}.tif', outputs, full_trans, target_epsg_code, nodata_value=0)\n    del outputs\n\n    out_counts_raster = write_cog(\n        f'{out_name}_counts.tif',\n        counts,\n        full_trans,\n        target_epsg_code,\n        dtype=gdal.GDT_Int16,\n    )\n    del counts\n\n    return out_raster, out_counts_raster\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.composite.reproject_to_target","title":"<code>reproject_to_target(raster_info, target_epsg_code, target_resolution, directory)</code>","text":"<p>Reprojects a set of raster images to a common projection and resolution</p> <p>Parameters:</p> Name Type Description Default <code>raster_info</code> <code>dict</code> <p>A dictionary of gdal.Info results for the set of rasters</p> required <code>target_epsg_code</code> <code>int</code> <p>The integer EPSG code for the target projection</p> required <code>target_resolution</code> <code>float</code> <p>The target resolution</p> required <code>directory</code> <code>str</code> <p>The directory in which to create the reprojected files</p> required <p>Returns:</p> Name Type Description <code>target_raster_info</code> <code>dict</code> <p>An updated dictionary of gdal.Info results for the reprojected files</p> Source code in <code>asf_tools/composite.py</code> <pre><code>def reproject_to_target(raster_info: dict, target_epsg_code: int, target_resolution: float, directory: str) -&gt; dict:\n\"\"\"Reprojects a set of raster images to a common projection and resolution\n\n    Args:\n        raster_info: A dictionary of gdal.Info results for the set of rasters\n        target_epsg_code: The integer EPSG code for the target projection\n        target_resolution: The target resolution\n        directory: The directory in which to create the reprojected files\n\n    Returns:\n        target_raster_info: An updated dictionary of gdal.Info results for the reprojected files\n    \"\"\"\n    target_raster_info = {}\n    for raster, info in raster_info.items():\n        epsg_code = get_epsg_code(info)\n        resolution = info['geoTransform'][1]\n        if epsg_code != target_epsg_code or resolution != target_resolution:\n            log.info(f'Reprojecting {raster}')\n            reprojected_raster = os.path.join(directory, os.path.basename(raster))\n            gdal.Warp(\n                reprojected_raster,\n                raster,\n                dstSRS=f'EPSG:{target_epsg_code}',\n                xRes=target_resolution,\n                yRes=target_resolution,\n                targetAlignedPixels=True,\n            )\n\n            area_raster = get_area_raster(raster)\n            log.info(f'Reprojecting {area_raster}')\n            reprojected_area_raster = os.path.join(directory, os.path.basename(area_raster))\n            gdal.Warp(\n                reprojected_area_raster,\n                area_raster,\n                dstSRS=f'EPSG:{target_epsg_code}',\n                xRes=target_resolution,\n                yRes=target_resolution,\n                targetAlignedPixels=True,\n            )\n\n            target_raster_info[reprojected_raster] = gdal.Info(reprojected_raster, format='json')\n        else:\n            log.info(f'No need to reproject {raster}')\n            target_raster_info[raster] = info\n\n    return target_raster_info\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.dem","title":"<code>dem</code>","text":"<p>Prepare a Copernicus GLO-30 DEM virtual raster (VRT) covering a given geometry</p>"},{"location":"tools/asf_tools_api/#asf_tools.dem.prepare_dem_vrt","title":"<code>prepare_dem_vrt(vrt, geometry)</code>","text":"<p>Create a DEM mosaic VRT covering a given geometry</p> <p>The DEM mosaic is assembled from the Copernicus GLO-30 DEM tiles that intersect the geometry.</p> <p>Note: <code>asf_tools</code> does not currently support geometries that cross the antimeridian.</p> <p>Parameters:</p> Name Type Description Default <code>vrt</code> <code>str | Path</code> <p>Path for the output VRT file</p> required <code>geometry</code> <code>Geometry | BaseGeometry</code> <p>Geometry in EPSG:4326 (lon/lat) projection for which to prepare a DEM mosaic</p> required Source code in <code>asf_tools/dem.py</code> <pre><code>def prepare_dem_vrt(vrt: str | Path, geometry: ogr.Geometry | BaseGeometry):\n\"\"\"Create a DEM mosaic VRT covering a given geometry\n\n    The DEM mosaic is assembled from the Copernicus GLO-30 DEM tiles that intersect the geometry.\n\n    Note: `asf_tools` does not currently support geometries that cross the antimeridian.\n\n    Args:\n        vrt: Path for the output VRT file\n        geometry: Geometry in EPSG:4326 (lon/lat) projection for which to prepare a DEM mosaic\n\n    \"\"\"\n    with GDALConfigManager(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n        if isinstance(geometry, BaseGeometry):\n            geometry = ogr.CreateGeometryFromWkb(geometry.wkb)\n\n        assert isinstance(geometry, ogr.Geometry)\n        min_lon, max_lon, _, _ = geometry.GetEnvelope()\n        if min_lon &lt; -160.0 and max_lon &gt; 160.0:\n            raise ValueError(f'asf_tools does not currently support geometries that cross the antimeridian: {geometry}')\n\n        tile_features = vector.get_features(DEM_GEOJSON)\n        if not vector.get_property_values_for_intersecting_features(geometry, tile_features):\n            raise ValueError(f'Copernicus GLO-30 DEM does not intersect this geometry: {geometry}')\n\n        dem_file_paths = vector.intersecting_feature_properties(geometry, tile_features, 'file_path')\n\n        gdal.BuildVRT(str(vrt), dem_file_paths)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar","title":"<code>hydrosar</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.flood_map","title":"<code>flood_map</code>","text":"<p>Generate flood depth map from surface water extent map.</p> <p>Create a flood depth map from a surface water extent map and a HAND image. The HAND image must be pixel-aligned (same extent and size) to the water extent map, and the surface water extent map should be a byte GeoTIFF indicating water (true), not water (false). Flood depth maps are estimated using either a numerical, normalized median absolute deviation, logarithmic or iterative approach.</p>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.flood_map.logstat","title":"<code>logstat(data, func=np.nanstd)</code>","text":"<p>Calculate a function in logarithmic scale and return in linear scale. INF values inside the data array are set to nan.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>array of data</p> required <code>func</code> <code>Callable</code> <p>statistical function to calculate in logarithmic scale</p> <code>nanstd</code> <p>Returns:     statistic: statistic of data in linear scale</p> Source code in <code>asf_tools/hydrosar/flood_map.py</code> <pre><code>def logstat(data: np.ndarray, func: Callable = np.nanstd) -&gt; np.ndarray | float:\n\"\"\"Calculate a function in logarithmic scale and return in linear scale.\n    INF values inside the data array are set to nan.\n\n    Args:\n        data: array of data\n        func: statistical function to calculate in logarithmic scale\n    Returns:\n        statistic: statistic of data in linear scale\n    \"\"\"\n    ld = np.log(data)\n    ld[np.isinf(ld)] = np.nan\n    st = func(ld)\n    return np.exp(st)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.flood_map.make_flood_map","title":"<code>make_flood_map(out_raster, vv_raster, water_raster, hand_raster, estimator='iterative', water_level_sigma=3.0, known_water_threshold=None, iterative_bounds=(0, 15), iterative_min_size=0, minimization_metric='ts')</code>","text":"<p>Create a flood depth map from a surface water extent map.</p> <p>WARNING: This functionality is still under active development and the products created using this function are likely to change in the future.</p> <p>Create a flood depth map from a single surface water extent map and a HAND image. The HAND image must be pixel-aligned to the surface water extent map. The surface water extent map should be a byte GeoTIFF indicating water (true) and not water (false)</p> <p>Known perennial Global Surface-water data are produced under the Copernicus Programme (Pekel et al., 2016), and are included with surface-water detection maps when generating the flood depth product.</p> <p>Flood depth maps are estimated using one of the approaches: Iterative: (Default) Basin hopping optimization method matches flooded areas to flood depth estimates given by the HAND layer. This is the most accurate method but also the most time-intensive. Normalized Median Absolute Deviation (nmad): Uses a median operator to estimate the variation to increase robustness in the presence of outliers. Logstat: Calculates the mean and standard deviation of HAND heights in the logarithmic domain to improve robustness for very non-Gaussian data distributions. Numpy: Calculates statistics on a linear scale. Least robust to outliers and non-Gaussian distributions.</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>Flood depth GeoTIFF to create</p> required <code>vv_raster</code> <code>str | Path</code> <p>Sentinel-1 RTC GeoTIFF, in power scale, with VV polarization</p> required <code>water_raster</code> <code>str | Path</code> <p>Surface water extent GeoTIFF</p> required <code>hand_raster</code> <code>str | Path</code> <p>Height Above Nearest Drainage (HAND) GeoTIFF aligned to the surface water extent raster</p> required <code>estimator</code> <code>str</code> <p>Estimation approach for determining flood depth</p> <code>'iterative'</code> <code>water_level_sigma</code> <code>float</code> <p>Max water height used in logstat, nmad, and numpy estimations</p> <code>3.0</code> <code>known_water_threshold</code> <code>float | None</code> <p>Threshold for extracting the known water area in percent. If <code>None</code>, the threshold is calculated.</p> <code>None</code> <code>iterative_bounds</code> <code>tuple[int, int]</code> <p>Minimum and maximum bound on the flood depths calculated by the basin-hopping algorithm used in the iterative estimator</p> <code>(0, 15)</code> <code>iterative_min_size</code> <code>int</code> <p>Minimum size of a connected waterbody in pixels for calculating flood depths with the iterative estimator. Waterbodies smaller than this wll be skipped.</p> <code>0</code> <code>minimization_metric</code> <code>str</code> <p>Evaluation method to minimize when using the iterative estimator. Options include a Fowlkes-Mallows index (fmi) or a threat score (ts).</p> <code>'ts'</code> References <p>Jean-Francios Pekel, Andrew Cottam, Noel Gorelik, Alan S. Belward. 2016. https://doi:10.1038/nature20584</p> Source code in <code>asf_tools/hydrosar/flood_map.py</code> <pre><code>def make_flood_map(\n    out_raster: str | Path,\n    vv_raster: str | Path,\n    water_raster: str | Path,\n    hand_raster: str | Path,\n    estimator: str = 'iterative',\n    water_level_sigma: float = 3.0,\n    known_water_threshold: float | None = None,\n    iterative_bounds: tuple[int, int] = (0, 15),\n    iterative_min_size: int = 0,\n    minimization_metric: str = 'ts',\n):\n\"\"\"Create a flood depth map from a surface water extent map.\n\n    WARNING: This functionality is still under active development and the products\n    created using this function are likely to change in the future.\n\n    Create a flood depth map from a single surface water extent map and\n    a HAND image. The HAND image must be pixel-aligned to the surface water extent map.\n    The surface water extent map should be a byte GeoTIFF indicating water (true) and\n    not water (false)\n\n    Known perennial Global Surface-water data are produced under the Copernicus Programme (Pekel et al., 2016),\n    and are included with surface-water detection maps when generating the flood depth product.\n\n    Flood depth maps are estimated using one of the approaches:\n    *Iterative: (Default) Basin hopping optimization method matches flooded areas to flood depth\n    estimates given by the HAND layer. This is the most accurate method but also the\n    most time-intensive.\n    *Normalized Median Absolute Deviation (nmad): Uses a median operator to estimate\n    the variation to increase robustness in the presence of outliers.\n    *Logstat: Calculates the mean and standard deviation of HAND heights in the logarithmic\n    domain to improve robustness for very non-Gaussian data distributions.\n    *Numpy: Calculates statistics on a linear scale. Least robust to outliers and non-Gaussian\n    distributions.\n\n    Args:\n        out_raster: Flood depth GeoTIFF to create\n        vv_raster: Sentinel-1 RTC GeoTIFF, in power scale, with VV polarization\n        water_raster: Surface water extent GeoTIFF\n        hand_raster: Height Above Nearest Drainage (HAND) GeoTIFF aligned to the surface water extent raster\n        estimator: Estimation approach for determining flood depth\n        water_level_sigma: Max water height used in logstat, nmad, and numpy estimations\n        known_water_threshold: Threshold for extracting the known water area in percent.\n            If `None`, the threshold is calculated.\n        iterative_bounds: Minimum and maximum bound on the flood depths calculated by the basin-hopping algorithm\n            used in the iterative estimator\n        iterative_min_size: Minimum size of a connected waterbody in pixels for calculating flood depths with the\n            iterative estimator. Waterbodies smaller than this wll be skipped.\n        minimization_metric: Evaluation method to minimize when using the iterative estimator.\n            Options include a Fowlkes-Mallows index (fmi) or a threat score (ts).\n\n    References:\n        Jean-Francios Pekel, Andrew Cottam, Noel Gorelik, Alan S. Belward. 2016. &lt;https://doi:10.1038/nature20584&gt;\n    \"\"\"\n    info = gdal.Info(str(water_raster), format='json')\n    epsg = get_epsg_code(info)\n    geotransform = info['geoTransform']\n    hand_array = gdal.Open(str(hand_raster), gdal.GA_ReadOnly).ReadAsArray()\n\n    log.info('Fetching perennial flood data.')\n    known_water_mask = get_waterbody(info, threshold=known_water_threshold)\n    write_cog(\n        str(out_raster).replace('.tif', f'_{estimator}_PW.tif'),\n        known_water_mask,\n        transform=geotransform,\n        epsg_code=epsg,\n        dtype=gdal.GDT_Byte,\n        nodata_value=False,\n    )\n\n    water_map = gdal.Open(str(water_raster)).ReadAsArray()\n    flood_mask = np.logical_or(water_map, known_water_mask)\n    del water_map\n\n    vv_array = read_as_masked_array(vv_raster)\n    flood_mask[vv_array.mask] = False\n    padding_mask = vv_array.mask\n    del vv_array\n\n    # The return type annotation for ndimage.label is int, which seems incorrect,\n    # since the documentation shows an example of unpacking the return value, like we do here.\n    labeled_flood_mask, num_labels = ndimage.label(flood_mask)  # type: ignore[misc]\n    object_slices = ndimage.find_objects(labeled_flood_mask)\n    log.info(f'Detected {num_labels} waterbodies...')\n    if estimator.lower() == 'iterative':\n        log.info(f'Skipping waterbodies less than {iterative_min_size} pixels.')\n\n    flood_depth = np.zeros(flood_mask.shape)\n\n    for ll in tqdm(range(1, num_labels)):  # Skip first, largest label.\n        slices = object_slices[ll - 1]\n        min0, max0 = slices[0].start, slices[0].stop\n        min1, max1 = slices[1].start, slices[1].stop\n\n        flood_window = labeled_flood_mask[min0:max0, min1:max1]\n        hand_window = hand_array[min0:max0, min1:max1]\n\n        water_height = estimate_flood_depth(\n            ll,\n            hand_window,\n            flood_window,\n            estimator=estimator,\n            water_level_sigma=water_level_sigma,\n            iterative_bounds=iterative_bounds,\n            minimization_metric=minimization_metric,\n            iterative_min_size=iterative_min_size,\n        )\n\n        flood_depth_window = flood_depth[min0:max0, min1:max1]\n        flood_depth_window[flood_window == ll] = water_height - hand_window[flood_window == ll]\n\n    flood_depth[flood_depth &lt; 0] = 0\n\n    nodata = -1\n    flood_depth[padding_mask] = nodata\n\n    floodmask_nodata = np.iinfo(np.uint8).max\n    flood_mask_byte = flood_mask.astype(np.uint8)\n    flood_mask_byte[padding_mask] = floodmask_nodata\n\n    write_cog(\n        str(out_raster).replace('.tif', f'_{estimator}_WaterDepth.tif'),\n        flood_depth,\n        transform=geotransform,\n        epsg_code=epsg,\n        dtype=gdal.GDT_Float64,\n        nodata_value=nodata,\n    )\n    write_cog(\n        str(out_raster).replace('.tif', f'_{estimator}_FloodMask.tif'),\n        flood_mask_byte,\n        transform=geotransform,\n        epsg_code=epsg,\n        dtype=gdal.GDT_Byte,\n        nodata_value=floodmask_nodata,\n    )\n\n    flood_mask[known_water_mask] = False\n    flood_depth[np.logical_not(flood_mask)] = 0\n    flood_depth[padding_mask] = nodata\n    write_cog(\n        str(out_raster).replace('.tif', f'_{estimator}_FloodDepth.tif'),\n        flood_depth,\n        transform=geotransform,\n        epsg_code=epsg,\n        dtype=gdal.GDT_Float64,\n        nodata_value=nodata,\n    )\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand","title":"<code>hand</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate_hand_for_basins","title":"<code>calculate_hand_for_basins(out_raster, geometries, dem_file, acc_thresh=100)</code>","text":"<p>Calculate the Height Above Nearest Drainage (HAND) for watershed boundaries (hydrobasins).</p> <p>For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>HAND GeoTIFF to create</p> required <code>geometries</code> <code>GeometryCollection</code> <p>watershed boundary (hydrobasin) polygons to calculate HAND over</p> required <code>dem_file</code> <code>str | Path</code> <p>DEM raster covering (containing) <code>geometries</code></p> required <code>acc_thresh</code> <code>int | None</code> <p>Accumulation threshold for determining the drainage mask. If <code>None</code>, the mean accumulation value is used</p> <code>100</code> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def calculate_hand_for_basins(\n    out_raster: str | Path,\n    geometries: GeometryCollection,\n    dem_file: str | Path,\n    acc_thresh: int | None = 100,\n):\n\"\"\"Calculate the Height Above Nearest Drainage (HAND) for watershed boundaries (hydrobasins).\n\n    For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins\n\n    Args:\n        out_raster: HAND GeoTIFF to create\n        geometries: watershed boundary (hydrobasin) polygons to calculate HAND over\n        dem_file: DEM raster covering (containing) `geometries`\n        acc_thresh: Accumulation threshold for determining the drainage mask.\n            If `None`, the mean accumulation value is used\n    \"\"\"\n    with rasterio.open(dem_file) as src:\n        basin_mask, basin_affine_tf, basin_window = rasterio.mask.raster_geometry_mask(\n            src, geometries.geoms, all_touched=True, crop=True, pad=True, pad_width=1\n        )\n        basin_array = src.read(1, window=basin_window)\n\n        hand = calculate_hand(basin_array, basin_affine_tf, src.crs, basin_mask, acc_thresh=acc_thresh)\n\n        write_cog(\n            out_raster,\n            hand,\n            transform=basin_affine_tf.to_gdal(),\n            epsg_code=src.crs.to_epsg(),\n            nodata_value=np.nan,\n        )\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.make_copernicus_hand","title":"<code>make_copernicus_hand(out_raster, vector_file, acc_thresh=100)</code>","text":"<p>Copernicus GLO-30 Height Above Nearest Drainage (HAND)</p> <p>Make a Height Above Nearest Drainage (HAND) GeoTIFF from the Copernicus GLO-30 DEM covering the watershed boundaries (hydrobasins) defined in a vector file.</p> <p>For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>HAND GeoTIFF to create</p> required <code>vector_file</code> <code>str | Path</code> <p>Vector file of watershed boundary (hydrobasin) polygons to calculate HAND over</p> required <code>acc_thresh</code> <code>int | None</code> <p>Accumulation threshold for determining the drainage mask. If <code>None</code>, the mean accumulation value is used</p> <code>100</code> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def make_copernicus_hand(\n    out_raster: str | Path,\n    vector_file: str | Path,\n    acc_thresh: int | None = 100,\n):\n\"\"\"Copernicus GLO-30 Height Above Nearest Drainage (HAND)\n\n    Make a Height Above Nearest Drainage (HAND) GeoTIFF from the Copernicus GLO-30 DEM\n    covering the watershed boundaries (hydrobasins) defined in a vector file.\n\n    For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins\n\n    Args:\n        out_raster: HAND GeoTIFF to create\n        vector_file: Vector file of watershed boundary (hydrobasin) polygons to calculate HAND over\n        acc_thresh: Accumulation threshold for determining the drainage mask.\n            If `None`, the mean accumulation value is used\n    \"\"\"\n    with fiona.open(vector_file) as vds:\n        geometries = GeometryCollection([shape(feature['geometry']) for feature in vds])\n\n    with NamedTemporaryFile(suffix='.vrt', delete=False) as dem_vrt:\n        prepare_dem_vrt(dem_vrt.name, geometries)\n        calculate_hand_for_basins(out_raster, geometries, dem_vrt.name, acc_thresh=acc_thresh)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.prepare_hand_vrt","title":"<code>prepare_hand_vrt(vrt, geometry)</code>","text":"<p>Prepare a HAND mosaic VRT covering a given geometry</p> <p>Prepare a Height Above Nearest Drainage (HAND) virtual raster (VRT) covering a given geometry. The Height Above Nearest Drainage (HAND) mosaic is assembled from the HAND tiles that intersect the geometry, using a HAND derived from the Copernicus GLO-30 DEM.</p> <p>Note: <code>asf_tools</code> does not currently support geometries that cross the antimeridian.</p> <p>Parameters:</p> Name Type Description Default <code>vrt</code> <code>str | Path</code> <p>Path for the output VRT file</p> required <code>geometry</code> <code>Geometry | BaseGeometry</code> <p>Geometry in EPSG:4326 (lon/lat) projection for which to prepare a HAND mosaic</p> required Source code in <code>asf_tools/hydrosar/hand/prepare.py</code> <pre><code>def prepare_hand_vrt(vrt: str | Path, geometry: ogr.Geometry | BaseGeometry):\n\"\"\"Prepare a HAND mosaic VRT covering a given geometry\n\n    Prepare a Height Above Nearest Drainage (HAND) virtual raster (VRT) covering a given geometry.\n    The Height Above Nearest Drainage (HAND) mosaic is assembled from the HAND tiles that intersect\n    the geometry, using a HAND derived from the Copernicus GLO-30 DEM.\n\n    Note: `asf_tools` does not currently support geometries that cross the antimeridian.\n\n    Args:\n        vrt: Path for the output VRT file\n        geometry: Geometry in EPSG:4326 (lon/lat) projection for which to prepare a HAND mosaic\n\n    \"\"\"\n    with GDALConfigManager(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n        if isinstance(geometry, BaseGeometry):\n            geometry = ogr.CreateGeometryFromWkb(geometry.wkb)\n\n        assert isinstance(geometry, ogr.Geometry)\n        min_lon, max_lon, _, _ = geometry.GetEnvelope()\n        if min_lon &lt; -160.0 and max_lon &gt; 160.0:\n            raise ValueError(f'asf_tools does not currently support geometries that cross the antimeridian: {geometry}')\n\n        tile_features = vector.get_features(HAND_GEOJSON)\n        if not vector.get_property_values_for_intersecting_features(geometry, tile_features):\n            raise ValueError(f'Copernicus GLO-30 HAND does not intersect this geometry: {geometry}')\n\n        hand_file_paths = vector.intersecting_feature_properties(geometry, tile_features, 'file_path')\n\n        gdal.BuildVRT(str(vrt), hand_file_paths)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate","title":"<code>calculate</code>","text":"<p>Calculate Height Above Nearest Drainage (HAND) from the Copernicus GLO-30 DEM</p>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate.calculate_hand","title":"<code>calculate_hand(dem_array, dem_affine, dem_crs, basin_mask, acc_thresh=100)</code>","text":"<p>Calculate the Height Above Nearest Drainage (HAND)</p> <p>Calculate the Height Above Nearest Drainage (HAND) using pySHEDS library. Because HAND  is tied to watershed boundaries (hydrobasins), clipped/cut basins will produce weird edge  effects, and incomplete basins should be masked out. For watershed boundaries,  see: https://www.hydrosheds.org/page/hydrobasins</p> <p>This involves:     * Filling pits (single-cells lower than their surrounding neighbors)         in the Digital Elevation Model (DEM)     * Filling depressions (regions of cells lower than their surrounding neighbors)         in the Digital Elevation Model (DEM)     * Resolving un-drainable flats     * Determining the flow direction using the ESRI D8 routing scheme     * Determining flow accumulation (number of upstream cells)     * Creating a drainage mask using the accumulation threshold <code>acc_thresh</code>     * Calculating HAND</p> <p>In the HAND calculation, NaNs inside the basin filled using <code>fill_hand</code></p> <p>Parameters:</p> Name Type Description Default <code>dem_array</code> <p>DEM to calculate HAND for</p> required <code>dem_crs</code> <code>CRS</code> <p>DEM Coordinate Reference System (CRS)</p> required <code>dem_affine</code> <code>Affine</code> <p>DEM Affine geotransform</p> required <code>basin_mask</code> <p>Array of booleans indicating wither an element should be masked out (\u00e0 la Numpy Masked Arrays: https://numpy.org/doc/stable/reference/maskedarray.generic.html#what-is-a-masked-array)</p> required <code>acc_thresh</code> <code>int | None</code> <p>Accumulation threshold for determining the drainage mask. If <code>None</code>, the mean accumulation value is used</p> <code>100</code> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def calculate_hand(\n    dem_array,\n    dem_affine: rasterio.Affine,\n    dem_crs: rasterio.crs.CRS,\n    basin_mask,\n    acc_thresh: int | None = 100,\n):\n\"\"\"Calculate the Height Above Nearest Drainage (HAND)\n\n     Calculate the Height Above Nearest Drainage (HAND) using pySHEDS library. Because HAND\n     is tied to watershed boundaries (hydrobasins), clipped/cut basins will produce weird edge\n     effects, and incomplete basins should be masked out. For watershed boundaries,\n     see: https://www.hydrosheds.org/page/hydrobasins\n\n     This involves:\n        * Filling pits (single-cells lower than their surrounding neighbors)\n            in the Digital Elevation Model (DEM)\n        * Filling depressions (regions of cells lower than their surrounding neighbors)\n            in the Digital Elevation Model (DEM)\n        * Resolving un-drainable flats\n        * Determining the flow direction using the ESRI D8 routing scheme\n        * Determining flow accumulation (number of upstream cells)\n        * Creating a drainage mask using the accumulation threshold `acc_thresh`\n        * Calculating HAND\n\n    In the HAND calculation, NaNs inside the basin filled using `fill_hand`\n\n    Args:\n        dem_array: DEM to calculate HAND for\n        dem_crs: DEM Coordinate Reference System (CRS)\n        dem_affine: DEM Affine geotransform\n        basin_mask: Array of booleans indicating wither an element should be masked out (\u00e0 la Numpy Masked Arrays:\n            https://numpy.org/doc/stable/reference/maskedarray.generic.html#what-is-a-masked-array)\n        acc_thresh: Accumulation threshold for determining the drainage mask.\n            If `None`, the mean accumulation value is used\n    \"\"\"\n    nodata_fill_value = np.finfo(float).eps\n    with NamedTemporaryFile() as temp_file:\n        write_cog(\n            temp_file.name,\n            dem_array,\n            transform=dem_affine.to_gdal(),\n            epsg_code=dem_crs.to_epsg(),\n            # Prevents PySheds from assuming using zero as the nodata value\n            nodata_value=nodata_fill_value,\n        )\n\n        # From PySheds; see example usage: http://mattbartos.com/pysheds/\n        grid = sGrid.from_raster(str(temp_file.name))\n        dem = grid.read_raster(str(temp_file.name))\n\n    log.info('Fill pits in DEM')\n    pit_filled_dem = grid.fill_pits(dem)\n\n    log.info('Filling depressions')\n    flooded_dem = grid.fill_depressions(pit_filled_dem)\n    del pit_filled_dem\n\n    log.info('Resolving flats')\n    inflated_dem = grid.resolve_flats(flooded_dem)\n    del flooded_dem\n\n    log.info('Obtaining flow direction')\n    flow_dir = grid.flowdir(inflated_dem, apply_mask=True)\n\n    log.info('Calculating flow accumulation')\n    acc = grid.accumulation(flow_dir)\n\n    if acc_thresh is None:\n        acc_thresh = acc.mean()\n\n    log.info(f'Calculating HAND using accumulation threshold of {acc_thresh}')\n    hand = grid.compute_hand(flow_dir, inflated_dem, acc &gt; acc_thresh, inplace=False)\n\n    if np.isnan(hand).any():\n        log.info('Filling NaNs in the HAND')\n        # mask outside of basin with a not-NaN value to prevent NaN-filling outside of basin (optimization)\n        hand[basin_mask] = nodata_fill_value\n        hand = fill_hand(hand, dem_array)\n\n    # set pixels outside of basin to nodata\n    hand[basin_mask] = np.nan\n\n    # TODO: also mask ocean pixels here?\n\n    return hand\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate.calculate_hand_for_basins","title":"<code>calculate_hand_for_basins(out_raster, geometries, dem_file, acc_thresh=100)</code>","text":"<p>Calculate the Height Above Nearest Drainage (HAND) for watershed boundaries (hydrobasins).</p> <p>For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>HAND GeoTIFF to create</p> required <code>geometries</code> <code>GeometryCollection</code> <p>watershed boundary (hydrobasin) polygons to calculate HAND over</p> required <code>dem_file</code> <code>str | Path</code> <p>DEM raster covering (containing) <code>geometries</code></p> required <code>acc_thresh</code> <code>int | None</code> <p>Accumulation threshold for determining the drainage mask. If <code>None</code>, the mean accumulation value is used</p> <code>100</code> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def calculate_hand_for_basins(\n    out_raster: str | Path,\n    geometries: GeometryCollection,\n    dem_file: str | Path,\n    acc_thresh: int | None = 100,\n):\n\"\"\"Calculate the Height Above Nearest Drainage (HAND) for watershed boundaries (hydrobasins).\n\n    For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins\n\n    Args:\n        out_raster: HAND GeoTIFF to create\n        geometries: watershed boundary (hydrobasin) polygons to calculate HAND over\n        dem_file: DEM raster covering (containing) `geometries`\n        acc_thresh: Accumulation threshold for determining the drainage mask.\n            If `None`, the mean accumulation value is used\n    \"\"\"\n    with rasterio.open(dem_file) as src:\n        basin_mask, basin_affine_tf, basin_window = rasterio.mask.raster_geometry_mask(\n            src, geometries.geoms, all_touched=True, crop=True, pad=True, pad_width=1\n        )\n        basin_array = src.read(1, window=basin_window)\n\n        hand = calculate_hand(basin_array, basin_affine_tf, src.crs, basin_mask, acc_thresh=acc_thresh)\n\n        write_cog(\n            out_raster,\n            hand,\n            transform=basin_affine_tf.to_gdal(),\n            epsg_code=src.crs.to_epsg(),\n            nodata_value=np.nan,\n        )\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate.fill_hand","title":"<code>fill_hand(hand, dem)</code>","text":"<p>Replace NaNs in a HAND array with values interpolated from their neighbor's HOND</p> <p>Replace NaNs in a HAND array with values interpolated from their neighbor's HOND (height of nearest drainage) using a 2D Gaussian kernel. Here, HOND is defined as the DEM value less the HAND value. For the kernel, see: https://docs.astropy.org/en/stable/convolution/#using-astropy-s-convolution-to-replace-bad-data</p> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def fill_hand(hand: np.ndarray, dem: np.ndarray):\n\"\"\"Replace NaNs in a HAND array with values interpolated from their neighbor's HOND\n\n    Replace NaNs in a HAND array with values interpolated from their neighbor's HOND (height of nearest drainage)\n    using a 2D Gaussian kernel. Here, HOND is defined as the DEM value less the HAND value. For the kernel, see:\n    https://docs.astropy.org/en/stable/convolution/#using-astropy-s-convolution-to-replace-bad-data\n    \"\"\"\n    hond = dem - hand\n    hond = fill_nan(hond)\n\n    hand_mask = np.isnan(hand)\n    hand[hand_mask] = dem[hand_mask] - hond[hand_mask]\n    hand[hand &lt; 0] = 0\n\n    return hand\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate.fill_nan","title":"<code>fill_nan(array)</code>","text":"<p>Replace NaNs with values interpolated from their neighbors</p> <p>Replace NaNs with values interpolated from their neighbors using a 2D Gaussian kernel, see: https://docs.astropy.org/en/stable/convolution/#using-astropy-s-convolution-to-replace-bad-data</p> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def fill_nan(array: np.ndarray) -&gt; np.ndarray:\n\"\"\"Replace NaNs with values interpolated from their neighbors\n\n    Replace NaNs with values interpolated from their neighbors using a 2D Gaussian\n    kernel, see: https://docs.astropy.org/en/stable/convolution/#using-astropy-s-convolution-to-replace-bad-data\n    \"\"\"\n    kernel = astropy.convolution.Gaussian2DKernel(x_stddev=3)  # kernel x_size=8*stddev\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        while np.any(np.isnan(array)):\n            array = astropy.convolution.interpolate_replace_nans(array, kernel, convolve=astropy.convolution.convolve)\n\n    return array\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.calculate.make_copernicus_hand","title":"<code>make_copernicus_hand(out_raster, vector_file, acc_thresh=100)</code>","text":"<p>Copernicus GLO-30 Height Above Nearest Drainage (HAND)</p> <p>Make a Height Above Nearest Drainage (HAND) GeoTIFF from the Copernicus GLO-30 DEM covering the watershed boundaries (hydrobasins) defined in a vector file.</p> <p>For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>HAND GeoTIFF to create</p> required <code>vector_file</code> <code>str | Path</code> <p>Vector file of watershed boundary (hydrobasin) polygons to calculate HAND over</p> required <code>acc_thresh</code> <code>int | None</code> <p>Accumulation threshold for determining the drainage mask. If <code>None</code>, the mean accumulation value is used</p> <code>100</code> Source code in <code>asf_tools/hydrosar/hand/calculate.py</code> <pre><code>def make_copernicus_hand(\n    out_raster: str | Path,\n    vector_file: str | Path,\n    acc_thresh: int | None = 100,\n):\n\"\"\"Copernicus GLO-30 Height Above Nearest Drainage (HAND)\n\n    Make a Height Above Nearest Drainage (HAND) GeoTIFF from the Copernicus GLO-30 DEM\n    covering the watershed boundaries (hydrobasins) defined in a vector file.\n\n    For watershed boundaries, see: https://www.hydrosheds.org/page/hydrobasins\n\n    Args:\n        out_raster: HAND GeoTIFF to create\n        vector_file: Vector file of watershed boundary (hydrobasin) polygons to calculate HAND over\n        acc_thresh: Accumulation threshold for determining the drainage mask.\n            If `None`, the mean accumulation value is used\n    \"\"\"\n    with fiona.open(vector_file) as vds:\n        geometries = GeometryCollection([shape(feature['geometry']) for feature in vds])\n\n    with NamedTemporaryFile(suffix='.vrt', delete=False) as dem_vrt:\n        prepare_dem_vrt(dem_vrt.name, geometries)\n        calculate_hand_for_basins(out_raster, geometries, dem_vrt.name, acc_thresh=acc_thresh)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.prepare","title":"<code>prepare</code>","text":"<p>Prepare a Height Above Nearest Drainage (HAND) virtual raster (VRT) covering a given geometry</p>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.prepare.prepare_hand_for_raster","title":"<code>prepare_hand_for_raster(hand_raster, source_raster, resampling_method='lanczos')</code>","text":"<p>Create a HAND raster pixel-aligned to a source raster</p> <p>Parameters:</p> Name Type Description Default <code>hand_raster</code> <code>str | Path</code> <p>Path for the output HAND raster</p> required <code>source_raster</code> <code>str | Path</code> <p>Path for the source raster</p> required <code>resampling_method</code> <code>str</code> <p>Name of the resampling method to use. For available methods, see: https://gdal.org/programs/gdalwarp.html#cmdoption-gdalwarp-r</p> <code>'lanczos'</code> Source code in <code>asf_tools/hydrosar/hand/prepare.py</code> <pre><code>def prepare_hand_for_raster(\n    hand_raster: str | Path,\n    source_raster: str | Path,\n    resampling_method: str = 'lanczos',\n):\n\"\"\"Create a HAND raster pixel-aligned to a source raster\n\n    Args:\n        hand_raster: Path for the output HAND raster\n        source_raster: Path for the source raster\n        resampling_method: Name of the resampling method to use. For available methods, see:\n            https://gdal.org/programs/gdalwarp.html#cmdoption-gdalwarp-r\n    \"\"\"\n    info = gdal.Info(str(source_raster), format='json')\n\n    hand_geometry = shape(info['wgs84Extent'])\n    hand_bounds = [\n        info['cornerCoordinates']['upperLeft'][0],\n        info['cornerCoordinates']['lowerRight'][1],\n        info['cornerCoordinates']['lowerRight'][0],\n        info['cornerCoordinates']['upperLeft'][1],\n    ]\n\n    with NamedTemporaryFile(suffix='.vrt', delete=False) as hand_vrt:\n        prepare_hand_vrt(hand_vrt.name, hand_geometry)\n        gdal.Warp(\n            str(hand_raster),\n            hand_vrt.name,\n            dstSRS=f'EPSG:{get_epsg_code(info)}',\n            outputBounds=hand_bounds,\n            width=info['size'][0],\n            height=info['size'][1],\n            resampleAlg=Resampling[resampling_method].value,\n        )\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.hand.prepare.prepare_hand_vrt","title":"<code>prepare_hand_vrt(vrt, geometry)</code>","text":"<p>Prepare a HAND mosaic VRT covering a given geometry</p> <p>Prepare a Height Above Nearest Drainage (HAND) virtual raster (VRT) covering a given geometry. The Height Above Nearest Drainage (HAND) mosaic is assembled from the HAND tiles that intersect the geometry, using a HAND derived from the Copernicus GLO-30 DEM.</p> <p>Note: <code>asf_tools</code> does not currently support geometries that cross the antimeridian.</p> <p>Parameters:</p> Name Type Description Default <code>vrt</code> <code>str | Path</code> <p>Path for the output VRT file</p> required <code>geometry</code> <code>Geometry | BaseGeometry</code> <p>Geometry in EPSG:4326 (lon/lat) projection for which to prepare a HAND mosaic</p> required Source code in <code>asf_tools/hydrosar/hand/prepare.py</code> <pre><code>def prepare_hand_vrt(vrt: str | Path, geometry: ogr.Geometry | BaseGeometry):\n\"\"\"Prepare a HAND mosaic VRT covering a given geometry\n\n    Prepare a Height Above Nearest Drainage (HAND) virtual raster (VRT) covering a given geometry.\n    The Height Above Nearest Drainage (HAND) mosaic is assembled from the HAND tiles that intersect\n    the geometry, using a HAND derived from the Copernicus GLO-30 DEM.\n\n    Note: `asf_tools` does not currently support geometries that cross the antimeridian.\n\n    Args:\n        vrt: Path for the output VRT file\n        geometry: Geometry in EPSG:4326 (lon/lat) projection for which to prepare a HAND mosaic\n\n    \"\"\"\n    with GDALConfigManager(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n        if isinstance(geometry, BaseGeometry):\n            geometry = ogr.CreateGeometryFromWkb(geometry.wkb)\n\n        assert isinstance(geometry, ogr.Geometry)\n        min_lon, max_lon, _, _ = geometry.GetEnvelope()\n        if min_lon &lt; -160.0 and max_lon &gt; 160.0:\n            raise ValueError(f'asf_tools does not currently support geometries that cross the antimeridian: {geometry}')\n\n        tile_features = vector.get_features(HAND_GEOJSON)\n        if not vector.get_property_values_for_intersecting_features(geometry, tile_features):\n            raise ValueError(f'Copernicus GLO-30 HAND does not intersect this geometry: {geometry}')\n\n        hand_file_paths = vector.intersecting_feature_properties(geometry, tile_features, 'file_path')\n\n        gdal.BuildVRT(str(vrt), hand_file_paths)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.threshold","title":"<code>threshold</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.threshold.expectation_maximization_threshold","title":"<code>expectation_maximization_threshold(tile, number_of_classes=3)</code>","text":"<p>Water threshold Calculation using a multi-mode Expectation Maximization Approach</p> <p>Thresholding works best when backscatter tiles are provided on a decibel scale to get Gaussian distribution that is scaled to a range of 0-255, and performed on a small tile that is likely to have a transition between water and not water.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>ndarray</code> <p>array of backscatter values for a tile from an RTC raster</p> required <code>number_of_classes</code> <code>int</code> <p>classify the tile into this many classes. Typically, three classes capture: (1) urban and bright slopes, (2) average brightness farmland, and (3) water, as is often seen in the US Midwest.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>threshold</code> <code>float</code> <p>threshold value that can be used to create a water extent map</p> Source code in <code>asf_tools/hydrosar/threshold.py</code> <pre><code>def expectation_maximization_threshold(tile: np.ndarray, number_of_classes: int = 3) -&gt; float:\n\"\"\"Water threshold Calculation using a multi-mode Expectation Maximization Approach\n\n    Thresholding works best when backscatter tiles are provided on a decibel scale\n    to get Gaussian distribution that is scaled to a range of 0-255, and performed\n    on a small tile that is likely to have a transition between water and not water.\n\n\n    Args:\n        tile: array of backscatter values for a tile from an RTC raster\n        number_of_classes: classify the tile into this many classes. Typically, three\n            classes capture: (1) urban and bright slopes, (2) average brightness farmland,\n            and (3) water, as is often seen in the US Midwest.\n\n    Returns:\n        threshold: threshold value that can be used to create a water extent map\n    \"\"\"\n    image_copy = tile.copy()\n    image_copy2 = np.ma.filled(tile.astype(float), np.nan)  # needed for valid posterior_lookup keys\n    image = tile.flatten()\n    minimum = np.amin(image)\n    image = image - minimum + 1\n    maximum = np.amax(image)\n\n    histogram = _make_histogram(image)\n    nonzero_indices = np.nonzero(histogram)[0]\n    histogram = histogram[nonzero_indices]\n    histogram = histogram.flatten()\n    class_means = (np.arange(number_of_classes) + 1) * maximum / (number_of_classes + 1)\n    class_variances = np.ones(number_of_classes) * maximum\n    class_proportions = np.ones(number_of_classes) * 1 / number_of_classes\n    sml = np.mean(np.diff(nonzero_indices)) / 1000\n    iteration = 0\n    while True:\n        class_likelihood = _make_distribution(class_means, class_variances, class_proportions, nonzero_indices)\n        sum_likelihood = np.sum(class_likelihood, 1) + np.finfo(class_likelihood[0][0]).eps\n        log_likelihood = np.sum(histogram * np.log(sum_likelihood))\n        for j in range(0, number_of_classes):\n            class_posterior_probability = histogram * class_likelihood[:, j] / sum_likelihood\n            class_proportions[j] = np.sum(class_posterior_probability)\n            class_means[j] = np.sum(nonzero_indices * class_posterior_probability) / class_proportions[j]\n            vr = nonzero_indices - class_means[j]\n            class_variances[j] = np.sum(vr * vr * class_posterior_probability) / class_proportions[j] + sml\n            del class_posterior_probability, vr\n        class_proportions += 1e-3\n        class_proportions /= np.sum(class_proportions)\n        class_likelihood = _make_distribution(class_means, class_variances, class_proportions, nonzero_indices)\n        sum_likelihood = np.sum(class_likelihood, 1) + np.finfo(class_likelihood[0, 0]).eps\n        del class_likelihood\n        new_log_likelihood = np.sum(histogram * np.log(sum_likelihood))\n        del sum_likelihood\n        if (new_log_likelihood - log_likelihood) &lt; 0.000001:\n            break\n        iteration += 1\n    del log_likelihood, new_log_likelihood\n    class_means = class_means + minimum - 1\n    s = image_copy.shape\n    posterior = np.zeros((s[0], s[1], number_of_classes))\n    posterior_lookup: dict = dict()\n    for i in range(0, s[0]):\n        for j in range(0, s[1]):\n            pixel_val = image_copy2[i, j]\n            if pixel_val in posterior_lookup:\n                for n in range(0, number_of_classes):\n                    posterior[i, j, n] = posterior_lookup[pixel_val][n]\n            else:\n                posterior_lookup.update({pixel_val: [0] * number_of_classes})\n                for n in range(0, number_of_classes):\n                    x = _make_distribution(\n                        class_means[n],\n                        class_variances[n],\n                        class_proportions[n],\n                        image_copy[i, j],\n                    )\n                    posterior[i, j, n] = x * class_proportions[n]\n                    posterior_lookup[pixel_val][n] = posterior[i, j, n]\n\n    sorti = np.argsort(class_means)\n    xvec = np.arange(class_means[sorti[0]], class_means[sorti[1]], step=0.05)\n    x1 = _make_distribution(\n        class_means[sorti[0]],\n        class_variances[sorti[0]],\n        class_proportions[sorti[0]],\n        xvec,\n    )\n    x2 = _make_distribution(\n        class_means[sorti[1]],\n        class_variances[sorti[1]],\n        class_proportions[sorti[1]],\n        xvec,\n    )\n    dx = np.abs(x1 - x2)\n\n    return xvec[np.argmin(dx)]\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.water_map","title":"<code>water_map</code>","text":"<p>Generate surface water maps from Sentinel-1 RTC products</p> <p>Create a surface water extent map from a dual-pol Sentinel-1 RTC product and a HAND image. The HAND image must be pixel-aligned (same extent and size) to the RTC images. The water extent maps are created using an adaptive Expectation Maximization thresholding approach and refined using Fuzzy Logic.</p>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.water_map.format_raster_data","title":"<code>format_raster_data(raster, padding_mask=None, nodata=np.iinfo(np.uint8).max)</code>","text":"<p>Ensure raster data is uint8 and set the area outside the valid data to nodata</p> Source code in <code>asf_tools/hydrosar/water_map.py</code> <pre><code>def format_raster_data(raster, padding_mask=None, nodata=np.iinfo(np.uint8).max):\n\"\"\"Ensure raster data is uint8 and set the area outside the valid data to nodata\"\"\"\n    if padding_mask is None:\n        array = read_as_masked_array(raster)\n        padding_mask = array.mask\n    raster = raster.astype(np.uint8)\n    raster[padding_mask] = nodata\n\n    return raster\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.hydrosar.water_map.make_water_map","title":"<code>make_water_map(out_raster, vv_raster, vh_raster, hand_raster=None, tile_shape=(100, 100), max_vv_threshold=-15.5, max_vh_threshold=-23.0, hand_threshold=15.0, hand_fraction=0.8, membership_threshold=0.45)</code>","text":"<p>Creates a surface water extent map from a Sentinel-1 RTC product</p> <p>Create a surface water extent map from a dual-pol Sentinel-1 RTC product and a HAND image. The HAND image must be pixel-aligned (same extent and size) to the RTC images. The water extent maps are created using an adaptive Expectation Maximization thresholding approach and refined with Fuzzy Logic.</p> <p>The input images are broken into a set of corresponding tiles with a shape of <code>tile_shape</code>, and a set of tiles are selected from the VH RTC image that contain water boundaries to determine an appropriate water threshold.  Candidate tiles must meet these criteria: * <code>hand_fraction</code> of pixels within a tile must have HAND pixel values lower   than <code>hand_threshold</code> * The median backscatter value for the tile must be lower than an average tiles'   backscatter values * The tile must have a high variance -- high variance is considered initially to   be a variance in the 95th percentile of the tile variances, but progressively   relaxed to the 5th percentile if there not at least 5 candidate tiles.</p> <p>The 5 VH tiles with the highest variance are selected for thresholding and a water threshold value is determined using an Expectation Maximization approach. If there were not enough candidate tiles or the threshold is too high, <code>max_vh_threshold</code> and/or <code>max_vv_threshold</code> will be used instead.</p> <p>From the initial threshold-based water extent maps, Fuzzy Logic is used to remove spurious false detections and improve the water extent map quality. The fuzzy logic uses these indicators for the presence of water: * radar cross section in a pixel relative to the determined detection threshold * the height above nearest drainage (HAND) * the surface slope, which is derived from the HAND data * the size of the detected water feature</p> <p>For each indicator, a Z-shaped activation function is used to determine pixel membership. The membership maps are combined to form the final water extent map. Pixels classified as water pixels will: * have non-zero membership in all of the indicators, and * have an average membership above the <code>membership_threshold</code> value.</p> <p>Finally, the VV and VH water masks will be combined to include all water pixels from both masks, and the combined water map will be written to <code>out_raster</code>.</p> <p>Parameters:</p> Name Type Description Default <code>out_raster</code> <code>str | Path</code> <p>Water map GeoTIFF to create</p> required <code>vv_raster</code> <code>str | Path</code> <p>Sentinel-1 RTC GeoTIFF, in power scale, with VV polarization</p> required <code>vh_raster</code> <code>str | Path</code> <p>Sentinel-1 RTC GeoTIFF, in power scale, with VH polarization</p> required <code>hand_raster</code> <code>str | Path | None</code> <p>Height Above Nearest Drainage (HAND) GeoTIFF aligned to the RTC rasters</p> <code>None</code> <code>tile_shape</code> <code>tuple[int, int]</code> <p>shape (height, width) in pixels to tile the image to</p> <code>(100, 100)</code> <code>max_vv_threshold</code> <code>float</code> <p>Maximum threshold value to use for <code>vv_raster</code> in decibels (db)</p> <code>-15.5</code> <code>max_vh_threshold</code> <code>float</code> <p>Maximum threshold value to use for <code>vh_raster</code> in decibels (db)</p> <code>-23.0</code> <code>hand_threshold</code> <code>float</code> <p>The maximum height above nearest drainage in meters to consider a pixel valid</p> <code>15.0</code> <code>hand_fraction</code> <code>float</code> <p>The minimum fraction of valid HAND pixels required in a tile for thresholding</p> <code>0.8</code> <code>membership_threshold</code> <code>float</code> <p>The average membership to the fuzzy indicators required for a water pixel</p> <code>0.45</code> Source code in <code>asf_tools/hydrosar/water_map.py</code> <pre><code>def make_water_map(\n    out_raster: str | Path,\n    vv_raster: str | Path,\n    vh_raster: str | Path,\n    hand_raster: str | Path | None = None,\n    tile_shape: tuple[int, int] = (100, 100),\n    max_vv_threshold: float = -15.5,\n    max_vh_threshold: float = -23.0,\n    hand_threshold: float = 15.0,\n    hand_fraction: float = 0.8,\n    membership_threshold: float = 0.45,\n):\n\"\"\"Creates a surface water extent map from a Sentinel-1 RTC product\n\n    Create a surface water extent map from a dual-pol Sentinel-1 RTC product and\n    a HAND image. The HAND image must be pixel-aligned (same extent and size) to\n    the RTC images. The water extent maps are created using an adaptive Expectation\n    Maximization thresholding approach and refined with Fuzzy Logic.\n\n    The input images are broken into a set of corresponding tiles with a shape of\n    `tile_shape`, and a set of tiles are selected from the VH RTC\n    image that contain water boundaries to determine an appropriate water threshold.\n     Candidate tiles must meet these criteria:\n    * `hand_fraction` of pixels within a tile must have HAND pixel values lower\n      than `hand_threshold`\n    * The median backscatter value for the tile must be lower than an average tiles'\n      backscatter values\n    * The tile must have a high variance -- high variance is considered initially to\n      be a variance in the 95th percentile of the tile variances, but progressively\n      relaxed to the 5th percentile if there not at least 5 candidate tiles.\n\n    The 5 VH tiles with the highest variance are selected for thresholding and a\n    water threshold value is determined using an Expectation Maximization approach.\n    If there were not enough candidate tiles or the threshold is too high,\n    `max_vh_threshold` and/or `max_vv_threshold` will be used instead.\n\n    From the initial threshold-based water extent maps, Fuzzy Logic is used to remove\n    spurious false detections and improve the water extent map quality. The fuzzy logic\n    uses these indicators for the presence of water:\n    * radar cross section in a pixel relative to the determined detection threshold\n    * the height above nearest drainage (HAND)\n    * the surface slope, which is derived from the HAND data\n    * the size of the detected water feature\n\n    For each indicator, a Z-shaped activation function is used to determine pixel membership.\n    The membership maps are combined to form the final water extent map. Pixels classified\n    as water pixels will:\n    * have non-zero membership in all of the indicators, and\n    * have an average membership above the `membership_threshold` value.\n\n    Finally, the VV and VH water masks will be combined to include all water pixels\n    from both masks, and the combined water map will be written to `out_raster`.\n\n    Args:\n        out_raster: Water map GeoTIFF to create\n        vv_raster: Sentinel-1 RTC GeoTIFF, in power scale, with VV polarization\n        vh_raster: Sentinel-1 RTC GeoTIFF, in power scale, with VH polarization\n        hand_raster: Height Above Nearest Drainage (HAND) GeoTIFF aligned to the RTC rasters\n        tile_shape: shape (height, width) in pixels to tile the image to\n        max_vv_threshold: Maximum threshold value to use for `vv_raster` in decibels (db)\n        max_vh_threshold:  Maximum threshold value to use for `vh_raster` in decibels (db)\n        hand_threshold: The maximum height above nearest drainage in meters to consider\n            a pixel valid\n        hand_fraction: The minimum fraction of valid HAND pixels required in a tile for\n            thresholding\n        membership_threshold: The average membership to the fuzzy indicators required for a water pixel\n    \"\"\"\n    if tile_shape[0] % 2 or tile_shape[1] % 2:\n        raise ValueError(f'tile_shape {tile_shape} requires even values.')\n\n    info = gdal.Info(str(vh_raster), format='json')\n\n    out_transform = info['geoTransform']\n    out_epsg = get_epsg_code(info)\n\n    if hand_raster is None:\n        hand_raster = str(out_raster).replace('.tif', '_HAND.tif')\n        log.info(f'Extracting HAND data to: {hand_raster}')\n        prepare_hand_for_raster(hand_raster, vh_raster)\n\n    log.info(f'Determining HAND memberships from {hand_raster}')\n    hand_array = read_as_masked_array(hand_raster)\n    hand_tiles = tile_array(hand_array, tile_shape=tile_shape, pad_value=np.nan)\n\n    hand_candidates = select_hand_tiles(hand_tiles, hand_threshold, hand_fraction)\n    log.debug(f'Selected HAND tile candidates {hand_candidates}')\n\n    selected_tiles = None\n    nodata = np.iinfo(np.uint8).max\n    water_extent_maps = []\n    for max_db_threshold, raster, pol in (\n        (max_vh_threshold, vh_raster, 'VH'),\n        (max_vv_threshold, vv_raster, 'VV'),\n    ):\n        log.info(f'Creating initial {pol} water extent map from {raster}')\n        array = read_as_masked_array(raster)\n        padding_mask = array.mask\n        tiles = tile_array(array, tile_shape=tile_shape, pad_value=0.0)\n        # Masking less than zero only necessary for old HyP3/GAMMA products which sometimes returned negative powers\n        tiles = np.ma.masked_less_equal(tiles, 0.0)\n        if selected_tiles is None:\n            selected_tiles = select_backscatter_tiles(tiles, hand_candidates)\n            log.info(f'Selected tiles {selected_tiles} from {raster}')\n\n        with np.testing.suppress_warnings() as sup:\n            sup.filter(RuntimeWarning)  # invalid value and divide by zero encountered in log10\n            tiles = np.log10(tiles) + 30.0  # linear power scale -&gt; Gaussian scale optimized for thresholding\n        max_gaussian_threshold = max_db_threshold / 10.0 + 30.0  # db -&gt; Gaussian scale optimized for thresholding\n        if selected_tiles.size:\n            scaling = 256 / (np.mean(tiles) + 3 * np.std(tiles))\n            gaussian_threshold = determine_em_threshold(tiles[selected_tiles, :, :], scaling)\n            threshold_db = 10.0 * (gaussian_threshold - 30.0)\n            log.info(f'Threshold determined to be {threshold_db} db')\n            if gaussian_threshold &gt; max_gaussian_threshold:\n                log.warning(f'Threshold too high! Using maximum threshold {max_db_threshold} db')\n                gaussian_threshold = max_gaussian_threshold\n        else:\n            log.warning(f'Tile selection did not converge! using default threshold {max_db_threshold} db')\n            gaussian_threshold = max_gaussian_threshold\n\n        gaussian_array = untile_array(tiles, array.shape)\n        water_map = np.ma.masked_less_equal(gaussian_array, gaussian_threshold).mask\n        water_map &amp;= ~array.mask\n\n        write_cog(\n            str(out_raster).replace('.tif', f'_{pol}_initial.tif'),\n            format_raster_data(water_map, padding_mask, nodata),\n            transform=out_transform,\n            epsg_code=out_epsg,\n            dtype=gdal.GDT_Byte,\n            nodata_value=nodata,\n        )\n\n        log.info(f'Refining initial {pol} water extent map using Fuzzy Logic')\n        array = np.ma.masked_where(~water_map, array)\n        gaussian_lower_limit = np.log10(np.ma.median(array)) + 30.0\n\n        water_map = fuzzy_refinement(\n            water_map,\n            gaussian_array,\n            hand_array,\n            pixel_size=out_transform[1],\n            gaussian_thresholds=(gaussian_lower_limit, gaussian_threshold),\n            membership_threshold=membership_threshold,\n        )\n        water_map &amp;= ~array.mask\n\n        write_cog(\n            str(out_raster).replace('.tif', f'_{pol}_fuzzy.tif'),\n            format_raster_data(water_map, padding_mask, nodata),\n            transform=out_transform,\n            epsg_code=out_epsg,\n            dtype=gdal.GDT_Byte,\n            nodata_value=nodata,\n        )\n\n        water_extent_maps.append(water_map)\n\n    log.info('Combining Fuzzy VH and VV extent map')\n    combined_water_map = np.logical_or(*water_extent_maps)\n\n    combined_segments = measure.label(combined_water_map, connectivity=2)\n    combined_water_map = remove_small_segments(combined_segments)\n\n    write_cog(\n        out_raster,\n        format_raster_data(combined_water_map, padding_mask, nodata),\n        transform=out_transform,\n        epsg_code=out_epsg,\n        dtype=gdal.GDT_Byte,\n        nodata_value=nodata,\n    )\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.raster","title":"<code>raster</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.raster.convert_scale","title":"<code>convert_scale(array, in_scale, out_scale)</code>","text":"<p>Convert calibrated raster scale between db, amplitude and power</p> Source code in <code>asf_tools/raster.py</code> <pre><code>def convert_scale(\n    array: np.ndarray | np.ma.MaskedArray,\n    in_scale: Literal['db', 'amplitude', 'power'],\n    out_scale: Literal['db', 'amplitude', 'power'],\n) -&gt; np.ndarray | np.ma.MaskedArray:\n\"\"\"Convert calibrated raster scale between db, amplitude and power\"\"\"\n    if in_scale == out_scale:\n        warnings.warn(f'Nothing to do! {in_scale} is same as {out_scale}.')\n        return array\n\n    log10 = np.ma.log10 if isinstance(array, np.ma.MaskedArray) else np.log10\n\n    if in_scale == 'db':\n        if out_scale == 'power':\n            return 10 ** (array / 10)\n        if out_scale == 'amplitude':\n            return 10 ** (array / 20)\n\n    if in_scale == 'amplitude':\n        if out_scale == 'power':\n            return array**2\n        if out_scale == 'db':\n            return 10 * log10(array**2)\n\n    if in_scale == 'power':\n        if out_scale == 'amplitude':\n            return np.sqrt(array)\n        if out_scale == 'db':\n            return 10 * log10(array)\n\n    raise ValueError(f'Cannot convert raster of scale {in_scale} to {out_scale}')\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.raster.read_as_array","title":"<code>read_as_array(raster, band=1)</code>","text":"<p>Reads data from a raster image into memory</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>The file path to a raster image</p> required <code>band</code> <code>int</code> <p>The raster band to read</p> <code>1</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ndarray</code> <p>The raster pixel data as a numpy array</p> Source code in <code>asf_tools/raster.py</code> <pre><code>def read_as_array(raster: str, band: int = 1) -&gt; np.ndarray:\n\"\"\"Reads data from a raster image into memory\n\n    Args:\n        raster: The file path to a raster image\n        band: The raster band to read\n\n    Returns:\n        data: The raster pixel data as a numpy array\n    \"\"\"\n    log.debug(f'Reading raster values from {raster}')\n    ds = gdal.Open(raster)\n    data = ds.GetRasterBand(band).ReadAsArray()\n    del ds  # How to close w/ gdal\n    return data\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.raster.read_as_masked_array","title":"<code>read_as_masked_array(raster, band=1)</code>","text":"<p>Reads data from a raster image into memory, masking invalid and NoData values</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str | Path</code> <p>The file path to a raster image</p> required <code>band</code> <code>int</code> <p>The raster band to read</p> <code>1</code> <p>Returns:</p> Name Type Description <code>raster_array</code> <code>MaskedArray</code> <p>The raster pixel data as a numpy MaskedArray</p> Source code in <code>asf_tools/raster.py</code> <pre><code>def read_as_masked_array(raster: str | Path, band: int = 1) -&gt; np.ma.MaskedArray:\n\"\"\"Reads data from a raster image into memory, masking invalid and NoData values\n\n    Args:\n        raster: The file path to a raster image\n        band: The raster band to read\n\n    Returns:\n        raster_array: The raster pixel data as a numpy MaskedArray\n    \"\"\"\n    log.debug(f'Reading raster values from {raster}')\n    ds = gdal.Open(str(raster))\n    raster_band = ds.GetRasterBand(band)\n    raster_array = raster_band.ReadAsArray()\n\n    nodata = raster_band.GetNoDataValue()\n    if nodata is not None:\n        raster_array = np.ma.masked_values(raster_array, nodata)\n    raster_array = np.ma.masked_invalid(raster_array)\n\n    del ds  # How to close w/ gdal\n    return raster_array\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.raster.write_cog","title":"<code>write_cog(file_name, data, transform, epsg_code, dtype=gdal.GDT_Float32, nodata_value=None)</code>","text":"<p>Creates a Cloud Optimized GeoTIFF</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path</code> <p>The output file name</p> required <code>data</code> <code>ndarray</code> <p>The raster data</p> required <code>transform</code> <code>list[float]</code> <p>The geotransform for the output GeoTIFF</p> required <code>epsg_code</code> <code>int</code> <p>The integer EPSG code for the output GeoTIFF projection</p> required <code>dtype</code> <p>The pixel data type for the output GeoTIFF</p> <code>GDT_Float32</code> <code>nodata_value</code> <p>The NODATA value for the output Geotiff</p> <code>None</code> <p>Returns:</p> Name Type Description <code>file_name</code> <p>The output file name</p> Source code in <code>asf_tools/raster.py</code> <pre><code>def write_cog(\n    file_name: str | Path,\n    data: np.ndarray,\n    transform: list[float],\n    epsg_code: int,\n    dtype=gdal.GDT_Float32,\n    nodata_value=None,\n):\n\"\"\"Creates a Cloud Optimized GeoTIFF\n\n    Args:\n        file_name: The output file name\n        data: The raster data\n        transform: The geotransform for the output GeoTIFF\n        epsg_code: The integer EPSG code for the output GeoTIFF projection\n        dtype: The pixel data type for the output GeoTIFF\n        nodata_value: The NODATA value for the output Geotiff\n\n    Returns:\n        file_name: The output file name\n    \"\"\"\n    log.info(f'Creating {file_name}')\n\n    with NamedTemporaryFile() as temp_file:\n        driver = gdal.GetDriverByName('GTiff')\n        temp_geotiff = driver.Create(temp_file.name, data.shape[1], data.shape[0], 1, dtype)\n        temp_geotiff.GetRasterBand(1).WriteArray(data)\n        if nodata_value is not None:\n            temp_geotiff.GetRasterBand(1).SetNoDataValue(nodata_value)\n        temp_geotiff.SetGeoTransform(transform)\n        temp_geotiff.SetProjection(epsg_to_wkt(epsg_code))\n\n        driver = gdal.GetDriverByName('COG')\n        options = [\n            'COMPRESS=LZW',\n            'OVERVIEW_RESAMPLING=AVERAGE',\n            'NUM_THREADS=ALL_CPUS',\n            'BIGTIFF=YES',\n        ]\n        driver.CreateCopy(str(file_name), temp_geotiff, options=options)\n\n        del temp_geotiff  # How to close w/ gdal\n    return file_name\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.tile","title":"<code>tile</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.tile.tile_array","title":"<code>tile_array(array, tile_shape=(200, 200), pad_value=None)</code>","text":"<p>Tile a 2D numpy array</p> Turn a 2D numpy array like <p>array = [[0, 0, 1, 1], ...          [0, 0, 1, 1], ...          [2, 2, 3, 3], ...          [2, 2, 3, 3]] array.shape (4, 4)</p> into a tiled array like <p>tiles = tiled_array(array, 2, 2) print(tiles) [[[0, 0],   [0, 0]],  [[1, 1],   [1, 1]],  [[2, 2],   [2, 2]],  [[3, 3],   [3, 3]]] tiles.shape (4, 2, 2)</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray | MaskedArray</code> <p>2D array to tile</p> required <code>tile_shape</code> <code>tuple[int, int]</code> <p>the shape of each tile</p> <code>(200, 200)</code> <code>pad_value</code> <code>float | None</code> <p>right-bottom pad <code>a</code> with <code>pad</code> as needed so <code>a</code> is evenly divisible into tiles</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | MaskedArray</code> <p>the tiled array</p> Source code in <code>asf_tools/tile.py</code> <pre><code>def tile_array(\n    array: np.ndarray | np.ma.MaskedArray,\n    tile_shape: tuple[int, int] = (200, 200),\n    pad_value: float | None = None,\n) -&gt; np.ndarray | np.ma.MaskedArray:\n\"\"\"Tile a 2D numpy array\n\n    Turn a 2D numpy array like:\n        &gt;&gt;&gt; array = [[0, 0, 1, 1],\n        ...          [0, 0, 1, 1],\n        ...          [2, 2, 3, 3],\n        ...          [2, 2, 3, 3]]\n        &gt;&gt;&gt; array.shape\n        (4, 4)\n\n    into a tiled array like:\n        &gt;&gt;&gt; tiles = tiled_array(array, 2, 2)\n        &gt;&gt;&gt; print(tiles)\n        [[[0, 0],\n          [0, 0]],\n         [[1, 1],\n          [1, 1]],\n         [[2, 2],\n          [2, 2]],\n         [[3, 3],\n          [3, 3]]]\n        &gt;&gt;&gt; tiles.shape\n        (4, 2, 2)\n\n    Args:\n        array: 2D array to tile\n        tile_shape: the shape of each tile\n        pad_value: right-bottom pad `a` with `pad` as needed so `a` is evenly divisible into tiles\n\n    Returns:\n        the tiled array\n    \"\"\"\n    array_rows, array_columns = array.shape\n    tile_rows, tile_columns = tile_shape\n\n    # CREDIT: https://twitter.com/LizzUltee/status/1379508448262512641\n    rpad = -array_rows % tile_rows\n    cpad = -array_columns % tile_columns\n\n    if (rpad or cpad) and pad_value is None:\n        raise ValueError(f'Cannot evenly tile a {array.shape} array into ({tile_rows},{tile_columns}) tiles')\n\n    if rpad or cpad:\n        assert pad_value is not None\n        padded_array = np.pad(array, ((0, rpad), (0, cpad)), constant_values=pad_value)\n        if isinstance(array, np.ma.MaskedArray):\n            mask = np.pad(array.mask, ((0, rpad), (0, cpad)), constant_values=True)\n            padded_array = np.ma.MaskedArray(padded_array, mask=mask)\n    else:\n        padded_array = array\n\n    tile_list = []\n    for rows in np.vsplit(padded_array, range(tile_rows, array_rows, tile_rows)):\n        tile_list.extend(np.hsplit(rows, range(tile_columns, array_columns, tile_columns)))\n\n    dstack = np.ma.dstack if isinstance(array, np.ma.MaskedArray) else np.dstack\n    tiled = np.moveaxis(dstack(tile_list), -1, 0)\n\n    return tiled\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.tile.untile_array","title":"<code>untile_array(tiled_array, array_shape)</code>","text":"<p>Untile a tiled array into a 2D numpy array</p> <p>This is the reverse of <code>tile_array</code> and will turn a tiled array like:     &gt;&gt;&gt; tiled_array = [[[0,0],     ...                 [0,0]],     ...                [[1,1],     ...                 [1,1]],     ...                [[2,2],     ...                 [2,2]],     ...                [[3,3],     ...                 [3,3]]]     &gt;&gt;&gt; tiled_array.shape     (4, 2, 2)</p> into a 2D array like <p>array = untile_array(tiled_array) print(array) [[0, 0, 1, 1],  [0, 0, 1, 1],  [2, 2, 3, 3],  [2, 2, 3, 3]] array.shape (4, 4)</p> <p>Parameters:</p> Name Type Description Default <code>tiled_array</code> <code>ndarray | MaskedArray</code> <p>a tiled array</p> required <code>array_shape</code> <code>tuple[int, int]</code> <p>shape to untile the array to. If array_shape's size is smaller than the tiled array, <code>untile_array</code> will subset the tiled array assuming bottom right padding was added when tiling.</p> required <p>Returns:</p> Type Description <code>ndarray | MaskedArray</code> <p>the untiled array</p> Source code in <code>asf_tools/tile.py</code> <pre><code>def untile_array(\n    tiled_array: np.ndarray | np.ma.MaskedArray, array_shape: tuple[int, int]\n) -&gt; np.ndarray | np.ma.MaskedArray:\n\"\"\"Untile a tiled array into a 2D numpy array\n\n    This is the reverse of `tile_array` and will turn a tiled array like:\n        &gt;&gt;&gt; tiled_array = [[[0,0],\n        ...                 [0,0]],\n        ...                [[1,1],\n        ...                 [1,1]],\n        ...                [[2,2],\n        ...                 [2,2]],\n        ...                [[3,3],\n        ...                 [3,3]]]\n        &gt;&gt;&gt; tiled_array.shape\n        (4, 2, 2)\n\n    into a 2D array like:\n        &gt;&gt;&gt; array = untile_array(tiled_array)\n        &gt;&gt;&gt; print(array)\n        [[0, 0, 1, 1],\n         [0, 0, 1, 1],\n         [2, 2, 3, 3],\n         [2, 2, 3, 3]]\n        &gt;&gt;&gt; array.shape\n        (4, 4)\n\n    Args:\n        tiled_array: a tiled array\n        array_shape: shape to untile the array to. If array_shape's size is smaller\n            than the tiled array, `untile_array` will subset the tiled array assuming\n            bottom right padding was added when tiling.\n\n    Returns:\n        the untiled array\n    \"\"\"\n    _, tile_rows, tile_columns = tiled_array.shape\n    array_rows, array_columns = array_shape\n\n    untiled_rows = int(np.ceil(array_rows / tile_rows))\n    untiled_columns = int(np.ceil(array_columns / tile_columns))\n\n    untiled = np.zeros(\n        (untiled_rows * tile_rows, untiled_columns * tile_columns),\n        dtype=tiled_array.dtype,\n    )\n\n    if (array_size := array_rows * array_columns) &gt; tiled_array.size:\n        raise ValueError(\n            f'array_shape {array_shape} will result in an array bigger than the tiled array:'\n            f' {array_size} &gt; {tiled_array.size}'\n        )\n\n    for ii in range(untiled_rows):\n        for jj in range(untiled_columns):\n            untiled[\n                ii * tile_rows : (ii + 1) * tile_rows,\n                jj * tile_columns : (jj + 1) * tile_columns,\n            ] = tiled_array[ii * untiled_columns + jj, :, :]\n\n    if isinstance(tiled_array, np.ma.MaskedArray):\n        assert len(untiled.shape) == 2\n        untiled_mask = untile_array(tiled_array.mask, untiled.shape)  # type: ignore[arg-type]\n        untiled = np.ma.MaskedArray(untiled, mask=untiled_mask)\n\n    return untiled[:array_rows, :array_columns]\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.util","title":"<code>util</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.util.GDALConfigManager","title":"<code>GDALConfigManager</code>","text":"<p>Context manager for setting GDAL config options temporarily</p> Source code in <code>asf_tools/util.py</code> <pre><code>class GDALConfigManager:\n\"\"\"Context manager for setting GDAL config options temporarily\"\"\"\n\n    def __init__(self, **options):\n\"\"\"Args:\n        **options: GDAL Config `option=value` keyword arguments.\n        \"\"\"\n        self.options = options.copy()\n        self._previous_options = {}\n\n    def __enter__(self):\n        for key in self.options:\n            self._previous_options[key] = gdal.GetConfigOption(key)\n\n        for key, value in self.options.items():\n            gdal.SetConfigOption(key, value)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        for key, value in self._previous_options.items():\n            gdal.SetConfigOption(key, value)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.util.GDALConfigManager.__init__","title":"<code>__init__(**options)</code>","text":"<p>Args: **options: GDAL Config <code>option=value</code> keyword arguments.</p> Source code in <code>asf_tools/util.py</code> <pre><code>def __init__(self, **options):\n\"\"\"Args:\n    **options: GDAL Config `option=value` keyword arguments.\n    \"\"\"\n    self.options = options.copy()\n    self._previous_options = {}\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.util.epsg_to_wkt","title":"<code>epsg_to_wkt(epsg_code)</code>","text":"<p>Get the WKT representation of a projection from its EPSG code</p> <p>Parameters:</p> Name Type Description Default <code>epsg_code</code> <code>int</code> <p>The integer EPSG code</p> required <p>Returns:</p> Name Type Description <code>wkt</code> <code>str</code> <p>The WKT representation of the projection</p> Source code in <code>asf_tools/util.py</code> <pre><code>def epsg_to_wkt(epsg_code: int) -&gt; str:\n\"\"\"Get the WKT representation of a projection from its EPSG code\n\n    Args:\n        epsg_code: The integer EPSG code\n\n    Returns:\n        wkt: The WKT representation of the projection\n    \"\"\"\n    srs = osr.SpatialReference()\n    srs.ImportFromEPSG(epsg_code)\n    return srs.ExportToWkt()\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.util.get_coordinates","title":"<code>get_coordinates(info)</code>","text":"<p>Get the corner coordinates from a GDAL Info dictionary</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>dict</code> <p>The dictionary returned by a gdal.Info call</p> required <p>Returns:</p> Type Description <code>(west, south, east, north)</code> <p>the corner coordinates values</p> Source code in <code>asf_tools/util.py</code> <pre><code>def get_coordinates(info: dict) -&gt; tuple[int, int, int, int]:\n\"\"\"Get the corner coordinates from a GDAL Info dictionary\n\n    Args:\n        info: The dictionary returned by a gdal.Info call\n\n    Returns:\n        (west, south, east, north): the corner coordinates values\n    \"\"\"\n    west, south = info['cornerCoordinates']['lowerLeft']\n    east, north = info['cornerCoordinates']['upperRight']\n    return west, south, east, north\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.util.get_epsg_code","title":"<code>get_epsg_code(info)</code>","text":"<p>Get the EPSG code from a GDAL Info dictionary</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>dict</code> <p>The dictionary returned by a gdal.Info call</p> required <p>Returns:</p> Name Type Description <code>epsg_code</code> <code>int</code> <p>The integer EPSG code</p> Source code in <code>asf_tools/util.py</code> <pre><code>def get_epsg_code(info: dict) -&gt; int:\n\"\"\"Get the EPSG code from a GDAL Info dictionary\n\n    Args:\n        info: The dictionary returned by a gdal.Info call\n\n    Returns:\n        epsg_code: The integer EPSG code\n    \"\"\"\n    proj = osr.SpatialReference(info['coordinateSystem']['wkt'])\n    epsg_code = int(proj.GetAttrValue('AUTHORITY', 1))\n    return epsg_code\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking","title":"<code>watermasking</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_osm_tiles","title":"<code>generate_osm_tiles</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_osm_tiles.extract_water","title":"<code>extract_water(water_file, lat, lon, tile_width_deg, tile_height_deg, interior_tile_dir)</code>","text":"<p>Rasterize a water tile from the processed global PBF file.</p> <p>Parameters:</p> Name Type Description Default <code>water_file</code> <p>The path to the processed global PBF file.</p> required <code>lat</code> <p>The minimum latitude of the tile.</p> required <code>lon</code> <p>The minimum longitude of the tile.</p> required <code>tile_width_deg</code> <p>The desired width of the tile in degrees.</p> required <code>tile_height_deg</code> <p>The desired height of the tile in degrees.</p> required Source code in <code>asf_tools/watermasking/generate_osm_tiles.py</code> <pre><code>def extract_water(water_file, lat, lon, tile_width_deg, tile_height_deg, interior_tile_dir):\n\"\"\"Rasterize a water tile from the processed global PBF file.\n\n    Args:\n        water_file: The path to the processed global PBF file.\n        lat: The minimum latitude of the tile.\n        lon: The minimum longitude of the tile.\n        tile_width_deg: The desired width of the tile in degrees.\n        tile_height_deg: The desired height of the tile in degrees.\n    \"\"\"\n    tile = lat_lon_to_tile_string(lat, lon, is_worldcover=False, postfix='')\n    tile_pbf = tile + '.osm.pbf'\n    tile_tif = interior_tile_dir + tile + '.tif'\n    tile_shp = tile + '.shp'\n    tile_geojson = tile + '.geojson'\n\n    # Extract tile from the main pbf, then convert it to a tif.\n    bbox = f'--bbox {lon},{lat},{lon + tile_width_deg},{lat + tile_height_deg}'\n    extract_command = f'osmium extract -s smart -S tags=natural=water {bbox} {water_file} -o {tile_pbf}'.split(' ')\n    export_command = f'osmium export --geometry-types=polygon {tile_pbf} -o {tile_geojson}'.split(' ')\n    subprocess.run(extract_command)\n    subprocess.run(export_command)\n\n    # Islands and Islets can be members of the water features, so they must be removed.\n    water_gdf = gpd.read_file(tile_geojson, engine='pyogrio')\n    try:\n        water_gdf = water_gdf.drop(water_gdf[water_gdf['place'] == 'island'].index)\n        water_gdf = water_gdf.drop(water_gdf[water_gdf['place'] == 'islet'].index)\n    except KeyError:\n        # When there are no islands to remove, an AttributeError should throw, but we don't care about it.\n        pass\n    water_gdf.to_file(tile_shp, mode='w', engine='pyogrio')\n    water_gdf = None\n\n    xmin, xmax, ymin, ymax = lon, lon + tile_width_deg, lat, lat + tile_height_deg\n    pixel_size_x = 0.00009009009\n    pixel_size_y = 0.00009009009\n\n    gdal.Rasterize(\n        tile_tif,\n        tile_shp,\n        xRes=pixel_size_x,\n        yRes=pixel_size_y,\n        burnValues=1,\n        outputBounds=[xmin, ymin, xmax, ymax],\n        outputType=gdal.GDT_Byte,\n        creationOptions=GDAL_OPTIONS,\n    )\n\n    temp_files = [\n        tile + '.dbf',\n        tile + '.cpg',\n        tile + '.prj',\n        tile + '.shx',\n        tile_shp,\n        tile_pbf,\n        tile_geojson,\n    ]\n    remove_temp_files(temp_files)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_osm_tiles.merge_interior_and_ocean","title":"<code>merge_interior_and_ocean(internal_tile_dir, ocean_tile_dir, finished_tile_dir, translate_to_cog=False)</code>","text":"<p>Merge the interior water tiles and ocean water tiles.</p> <p>Parameters:</p> Name Type Description Default <code>interior_tile_dir</code> <p>The path to the directory containing the interior water tiles.</p> required <code>ocean_tile_dir</code> <p>The path to the directory containing the ocean water tiles.</p> required <code>merged_tile_dir</code> <p>The path to the directory containing the merged water tiles.</p> required Source code in <code>asf_tools/watermasking/generate_osm_tiles.py</code> <pre><code>def merge_interior_and_ocean(internal_tile_dir, ocean_tile_dir, finished_tile_dir, translate_to_cog: bool = False):\n\"\"\"Merge the interior water tiles and ocean water tiles.\n\n    Args:\n        interior_tile_dir: The path to the directory containing the interior water tiles.\n        ocean_tile_dir: The path to the directory containing the ocean water tiles.\n        merged_tile_dir: The path to the directory containing the merged water tiles.\n    \"\"\"\n    index = 0\n    num_tiles = len([f for f in os.listdir(internal_tile_dir) if f.endswith('tif')]) - 1\n    for filename in os.listdir(internal_tile_dir):\n        if filename.endswith('.tif'):\n            start_time = time.time()\n\n            internal_tile = internal_tile_dir + filename\n            external_tile = ocean_tile_dir + filename\n            output_tile = finished_tile_dir + filename\n            command = [\n                'gdal_calc.py',\n                '-A',\n                internal_tile,\n                '-B',\n                external_tile,\n                '--format',\n                'GTiff',\n                '--outfile',\n                output_tile,\n                '--calc',\n                '\"logical_or(A, B)\"',\n            ]\n            subprocess.run(command)\n\n            if translate_to_cog:\n                cogs_dir = finished_tile_dir + 'cogs/'\n                try:\n                    os.mkdir(cogs_dir)\n                except FileExistsError:\n                    pass\n                out_file = cogs_dir + filename\n                translate_string = 'gdal_translate -ot Byte -of COG -co NUM_THREADS=all_cpus'\n                command = f'{translate_string} {output_tile} {out_file}'.split(' ')\n                subprocess.run(command)\n                os.remove(output_tile)\n\n            end_time = time.time()\n            total_time = end_time - start_time\n\n            print(f'Elapsed Time: {total_time}(s)')\n            print(f'Completed {index} of {num_tiles}')\n\n            index += 1\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_osm_tiles.process_ocean_tiles","title":"<code>process_ocean_tiles(ocean_polygons_path, lat, lon, tile_width_deg, tile_height_deg, output_dir)</code>","text":"<p>Process and crop OSM ocean polygons into a tif tile.</p> <p>Parameters:</p> Name Type Description Default <code>ocean_polygons_path</code> <p>The path to the global ocean polygons file from OSM.</p> required <code>lat</code> <p>The minimum latitude of the tile.</p> required <code>lon</code> <p>The minimum longitude of the tile.</p> required <code>tile_width_deg</code> <p>The width of the tile in degrees.</p> required <code>tile_height_deg</code> <p>The height of the tile in degrees.</p> required Source code in <code>asf_tools/watermasking/generate_osm_tiles.py</code> <pre><code>def process_ocean_tiles(ocean_polygons_path, lat, lon, tile_width_deg, tile_height_deg, output_dir):\n\"\"\"Process and crop OSM ocean polygons into a tif tile.\n\n    Args:\n        ocean_polygons_path: The path to the global ocean polygons file from OSM.\n        lat: The minimum latitude of the tile.\n        lon: The minimum longitude of the tile.\n        tile_width_deg: The width of the tile in degrees.\n        tile_height_deg: The height of the tile in degrees.\n    \"\"\"\n    tile = lat_lon_to_tile_string(lat, lon, is_worldcover=False, postfix='')\n    tile_tif = output_dir + tile + '.tif'\n\n    xmin, xmax, ymin, ymax = lon, lon + tile_width_deg, lat, lat + tile_height_deg\n    pixel_size_x = 0.00009009009\n    pixel_size_y = 0.00009009009\n\n    clipped_polygons_path = tile + '.shp'\n    command = f'ogr2ogr -clipsrc {xmin} {ymin} {xmax} {ymax} {clipped_polygons_path} {ocean_polygons_path}'.split(' ')\n    subprocess.run(command)\n\n    gdal.Rasterize(\n        tile_tif,\n        clipped_polygons_path,\n        xRes=pixel_size_x,\n        yRes=pixel_size_y,\n        burnValues=1,\n        outputBounds=[xmin, ymin, xmax, ymax],\n        outputType=gdal.GDT_Byte,\n        creationOptions=GDAL_OPTIONS,\n    )\n\n    temp_files = [\n        tile + '.dbf',\n        tile + '.cpg',\n        tile + '.prj',\n        tile + '.shx',\n        tile + '.shp',\n    ]\n    remove_temp_files(temp_files)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_osm_tiles.process_pbf","title":"<code>process_pbf(planet_file, output_file)</code>","text":"<p>Process the global PBF file so that it only contains water features.</p> <p>Parameters:</p> Name Type Description Default <code>planet_file</code> <code>str</code> <p>The path to the OSM Planet PBF file.</p> required <code>output_file</code> <code>str</code> <p>The desired path of the processed PBF file.</p> required Source code in <code>asf_tools/watermasking/generate_osm_tiles.py</code> <pre><code>def process_pbf(planet_file: str, output_file: str):\n\"\"\"Process the global PBF file so that it only contains water features.\n\n    Args:\n        planet_file: The path to the OSM Planet PBF file.\n        output_file: The desired path of the processed PBF file.\n    \"\"\"\n    natural_file = 'planet_natural.pbf'\n    waterways_file = 'planet_waterways.pbf'\n    reservoirs_file = 'planet_reservoirs.pbf'\n\n    natural_water_command = f'osmium tags-filter -o {natural_file} {planet_file} wr/natural=water'.split(' ')\n    waterways_command = f'osmium tags-filter -o {waterways_file} {planet_file} waterway=\"*\"'.split(' ')\n    reservoirs_command = f'osmium tags-filter -o {reservoirs_file} {planet_file} landuse=reservoir'.split(' ')\n    merge_command = f'osmium merge {natural_file} {waterways_file} {reservoirs_file} -o {output_file}'.split(' ')\n\n    subprocess.run(natural_water_command)\n    subprocess.run(waterways_command)\n    subprocess.run(reservoirs_command)\n    subprocess.run(merge_command)\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles","title":"<code>generate_worldcover_tiles</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.build_dataset","title":"<code>build_dataset(worldcover_tile_dir, lat_range, lon_range, tile_width, tile_height)</code>","text":"<p>Main function for generating a dataset with worldcover tiles.</p> <p>Parameters:</p> Name Type Description Default <code>worldcover_tile_dir</code> <p>The directory containing the unprocessed worldcover tiles.</p> required <code>lat_range</code> <p>The range of latitudes the dataset should cover.</p> required <code>lon_range</code> <p>The range of longitudes the dataset should cover.</p> required <code>out_degrees</code> <p>The width of the outputed dataset tiles in degrees.</p> required Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def build_dataset(worldcover_tile_dir, lat_range, lon_range, tile_width, tile_height):\n\"\"\"Main function for generating a dataset with worldcover tiles.\n\n    Args:\n        worldcover_tile_dir: The directory containing the unprocessed worldcover tiles.\n        lat_range: The range of latitudes the dataset should cover.\n        lon_range: The range of longitudes the dataset should cover.\n        out_degrees: The width of the outputed dataset tiles in degrees.\n    \"\"\"\n    for lat in lat_range:\n        for lon in lon_range:\n            start_time = time.time()\n            tile = lat_lon_to_tile_string(lat, lon, is_worldcover=False)\n            tile_filename = UNCROPPED_TILE_DIR + tile\n            worldcover_tiles = lat_lon_to_filenames(worldcover_tile_dir, (lat, lon), WORLDCOVER_TILE_SIZE, tile_width)\n            print(f'Processing: {tile_filename} {worldcover_tiles}')\n            merge_tiles(worldcover_tiles, tile_filename, 'GTiff', compress=True)\n            crop_tile(tile, lat, lon, tile_width, tile_height)\n            end_time = time.time()\n            total_time = end_time - start_time\n            print(f'Time Elapsed: {total_time}s')\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.create_missing_tiles","title":"<code>create_missing_tiles(tile_dir, lat_range, lon_range)</code>","text":"<p>Check for, and build, tiles that may be missing from WorldCover, such as over the ocean.</p> <p>Parameters:</p> Name Type Description Default <code>lat_range</code> <p>The range of latitudes to check.</p> required <code>lon_range</code> <p>The range of longitudes to check.</p> required <p>Returns:</p> Name Type Description <code>current_existing_tiles</code> <p>The list of tiles that exist after the function has completed.</p> Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def create_missing_tiles(tile_dir, lat_range, lon_range):\n\"\"\"Check for, and build, tiles that may be missing from WorldCover, such as over the ocean.\n\n    Args:\n        lat_range: The range of latitudes to check.\n        lon_range: The range of longitudes to check.\n\n    Returns:\n        current_existing_tiles: The list of tiles that exist after the function has completed.\n    \"\"\"\n    current_existing_tiles = [f for f in os.listdir(tile_dir) if f.endswith(FILENAME_POSTFIX)]\n    for lon in lon_range:\n        for lat in lat_range:\n            tile = lat_lon_to_tile_string(lat, lon, is_worldcover=True)\n            print(f'Checking {tile}')\n            if tile not in current_existing_tiles:\n                print(f'Could not find {tile}')\n\n                filename = PREPROCESSED_TILE_DIR + tile\n                x_size, y_size = 36000, 36000\n                x_res, y_res = 8.333333333333333055e-05, -8.333333333333333055e-05\n                ul_lon = lon\n                ul_lat = lat + WORLDCOVER_TILE_SIZE\n                geotransform = (ul_lon, x_res, 0, ul_lat, 0, y_res)\n\n                driver = gdal.GetDriverByName('GTiff')\n                ds = driver.Create(\n                    filename,\n                    xsize=x_size,\n                    ysize=y_size,\n                    bands=1,\n                    eType=gdal.GDT_Byte,\n                    options=GDAL_OPTIONS,\n                )\n                ds.SetProjection('EPSG:4326')\n                ds.SetGeoTransform(geotransform)\n                band = ds.GetRasterBand(1)  # Write ones, as tiles should only be missing over water.\n                band.WriteArray(np.ones((x_size, y_size)))\n\n                del ds\n                del band\n\n                current_existing_tiles.append(tile)\n                print(f'Added {tile}')\n    return current_existing_tiles\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.crop_tile","title":"<code>crop_tile(tile, lat, lon, tile_width, tile_height)</code>","text":"<p>Crop the merged tiles</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <p>The filename of the desired tile to crop.</p> required Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def crop_tile(tile, lat, lon, tile_width, tile_height):\n\"\"\"Crop the merged tiles\n\n    Args:\n        tile: The filename of the desired tile to crop.\n    \"\"\"\n    in_filename = UNCROPPED_TILE_DIR + tile\n    out_filename = CROPPED_TILE_DIR + tile\n    pixel_size_x, pixel_size_y = 0.00009009009, -0.00009009009\n\n    src_ds = gdal.Open(in_filename)\n    gdal.Translate(\n        out_filename,\n        src_ds,\n        projWin=[lon, lat + tile_height, lon + tile_width, lat],\n        xRes=pixel_size_x,\n        yRes=pixel_size_y,\n        outputSRS='EPSG:4326',\n        format='COG',\n        creationOptions=['NUM_THREADS=all_cpus'],\n    )\n    remove_temp_files(['tmp_px_size.tif', 'tmp.shp'])\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.get_tiles","title":"<code>get_tiles(osm_tile_coord, wc_tile_width, tile_width)</code>","text":"<p>Get a list of the worldcover tile locations necessary to fully cover an OSM tile.</p> <p>Parameters:</p> Name Type Description Default <code>osm_tile_coord</code> <code>tuple</code> <p>The lower left corner coordinate (lat, lon) of the desired OSM tile.</p> required <code>wc_tile_width</code> <code>int</code> <p>The width/height of the Worldcover tiles in degrees.</p> required <code>tile_width</code> <code>int</code> <p>The width/height of the OSM tiles in degrees.</p> required <p>Returns:</p> Name Type Description <code>tiles</code> <p>A list of the lower left corner coordinates of the Worldcover tiles that overlap the OSM tile.</p> Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def get_tiles(osm_tile_coord: tuple, wc_tile_width: int, tile_width: int):\n\"\"\"Get a list of the worldcover tile locations necessary to fully cover an OSM tile.\n\n    Args:\n        osm_tile_coord: The lower left corner coordinate (lat, lon) of the desired OSM tile.\n        wc_tile_width: The width/height of the Worldcover tiles in degrees.\n        tile_width: The width/height of the OSM tiles in degrees.\n\n    Returns:\n        tiles: A list of the lower left corner coordinates of the Worldcover tiles that overlap the OSM tile.\n    \"\"\"\n    osm_lat = osm_tile_coord[0]\n    osm_lon = osm_tile_coord[1]\n\n    min_lat = osm_lat - (osm_lat % wc_tile_width)\n    max_lat = osm_lat + tile_width\n    min_lon = osm_lon - (osm_lon % wc_tile_width)\n    max_lon = osm_lon + tile_width\n\n    lats = range(min_lat, max_lat, wc_tile_width)\n    lons = range(min_lon, max_lon, wc_tile_width)\n\n    tiles = []\n    for lat in lats:\n        for lon in lons:\n            tiles.append((lat, lon))\n\n    return tiles\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.lat_lon_to_filenames","title":"<code>lat_lon_to_filenames(worldcover_tile_dir, osm_tile_coord, wc_tile_width, tile_width)</code>","text":"<p>Get a list of the Worldcover tile filenames that are necessary to overlap an OSM tile.</p> <p>Parameters:</p> Name Type Description Default <code>osm_tile</code> <p>The lower left corner (lat, lon) of the desired OSM tile.</p> required <code>wc_tile_width</code> <code>int</code> <p>The width of the Worldcover tiles in degrees.</p> required <code>tile_width</code> <code>int</code> <p>The width of the OSM tiles in degrees.</p> required <p>Returns:</p> Name Type Description <code>filenames</code> <p>The list of Worldcover filenames.</p> Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def lat_lon_to_filenames(worldcover_tile_dir, osm_tile_coord: tuple, wc_tile_width: int, tile_width: int):\n\"\"\"Get a list of the Worldcover tile filenames that are necessary to overlap an OSM tile.\n\n    Args:\n        osm_tile: The lower left corner (lat, lon) of the desired OSM tile.\n        wc_tile_width: The width of the Worldcover tiles in degrees.\n        tile_width: The width of the OSM tiles in degrees.\n\n    Returns:\n        filenames: The list of Worldcover filenames.\n    \"\"\"\n    filenames = []\n    tiles = get_tiles(osm_tile_coord, wc_tile_width, tile_width)\n    for tile in tiles:\n        filenames.append(worldcover_tile_dir + lat_lon_to_tile_string(tile[0], tile[1], is_worldcover=True))\n    return filenames\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.generate_worldcover_tiles.tile_preprocessing","title":"<code>tile_preprocessing(tile_dir, min_lat, max_lat, min_lon, max_lon)</code>","text":"<p>The worldcover tiles have lots of unnecessary classes, these need to be removed first.    Note: make a back-up copy of this directory.</p> <p>Parameters:</p> Name Type Description Default <code>tile_dir</code> <p>The directory containing all of the worldcover tiles.</p> required Source code in <code>asf_tools/watermasking/generate_worldcover_tiles.py</code> <pre><code>def tile_preprocessing(tile_dir, min_lat, max_lat, min_lon, max_lon):\n\"\"\"The worldcover tiles have lots of unnecessary classes, these need to be removed first.\n       Note: make a back-up copy of this directory.\n\n    Args:\n        tile_dir: The directory containing all of the worldcover tiles.\n    \"\"\"\n    filenames = [f for f in os.listdir(tile_dir) if f.endswith('.tif')]\n\n    def filename_filter(filename):\n        latitude = int(filename.split('_')[5][1:3])\n        longitude = int(filename.split('_')[5][4:7])\n        if filename.split('_')[5][3] == 'W':\n            longitude = -longitude\n        mnlat = min_lat - (min_lat % WORLDCOVER_TILE_SIZE)\n        mnlon = min_lon - (min_lon % WORLDCOVER_TILE_SIZE)\n        mxlat = max_lat + (max_lat % WORLDCOVER_TILE_SIZE)\n        mxlon = max_lon + (max_lon % WORLDCOVER_TILE_SIZE)\n        in_lat_range = (latitude &gt;= mnlat) and (latitude &lt;= mxlat)\n        in_lon_range = (longitude &gt;= mnlon) and (longitude &lt;= mxlon)\n        return in_lat_range and in_lon_range\n\n    filenames_filtered = [f for f in filenames if filename_filter(f)]\n\n    index = 0\n    num_tiles = len(filenames_filtered)\n    for filename in filenames_filtered:\n        start_time = time.time()\n\n        tile_name = filename.split('_')[5]\n        filename = str(Path(tile_dir) / filename)\n        dst_filename = PREPROCESSED_TILE_DIR + tile_name + '.tif'\n\n        print(f'Processing: {filename}  ---  {dst_filename}  -- {index} of {num_tiles}')\n\n        src_ds = gdal.Open(filename)\n        src_band = src_ds.GetRasterBand(1)\n        src_arr = src_band.ReadAsArray()\n\n        not_water = np.logical_and(src_arr != 80, src_arr != 0)\n        water_arr = np.ones(src_arr.shape)\n        water_arr[not_water] = 0\n\n        driver = gdal.GetDriverByName('GTiff')\n        dst_ds = driver.Create(\n            dst_filename,\n            water_arr.shape[0],\n            water_arr.shape[1],\n            1,\n            gdal.GDT_Byte,\n            options=GDAL_OPTIONS,\n        )\n        dst_ds.SetGeoTransform(src_ds.GetGeoTransform())\n        dst_ds.SetProjection(src_ds.GetProjection())\n        dst_band = dst_ds.GetRasterBand(1)\n        dst_band.WriteArray(water_arr)\n        dst_band.FlushCache()\n\n        del dst_ds\n        del src_ds\n\n        end_time = time.time()\n        total_time = end_time - start_time\n\n        print(f'Processing {dst_filename} took {total_time} seconds.')\n\n        index += 1\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.utils","title":"<code>utils</code>","text":""},{"location":"tools/asf_tools_api/#asf_tools.watermasking.utils.lat_lon_to_tile_string","title":"<code>lat_lon_to_tile_string(lat, lon, is_worldcover=False, postfix='.tif')</code>","text":"<p>Get the name of the tile with lower left corner (lat, lon).</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <p>The minimum latitude of the tile.</p> required <code>lon</code> <p>The minimum longitude of the tile.</p> required <code>is_worldcover</code> <code>bool</code> <p>Wheter the tile is Worldcover or OSM.</p> <code>False</code> <code>postfix</code> <code>str</code> <p>A postfix to append to the tile name to make it a filename.</p> <code>'.tif'</code> <p>Returns:</p> Type Description <p>The name of the tile.</p> Source code in <code>asf_tools/watermasking/utils.py</code> <pre><code>def lat_lon_to_tile_string(lat, lon, is_worldcover: bool = False, postfix: str = '.tif'):\n\"\"\"Get the name of the tile with lower left corner (lat, lon).\n\n    Args:\n        lat: The minimum latitude of the tile.\n        lon: The minimum longitude of the tile.\n        is_worldcover: Wheter the tile is Worldcover or OSM.\n        postfix: A postfix to append to the tile name to make it a filename.\n\n    Returns:\n        The name of the tile.\n    \"\"\"\n    prefixes = ['N', 'S', 'E', 'W'] if is_worldcover else ['n', 's', 'e', 'w']\n    if lat &gt;= 0:\n        lat_part = prefixes[0] + str(int(lat)).zfill(2)\n    else:\n        lat_part = prefixes[1] + str(int(np.abs(lat))).zfill(2)\n    if lon &gt;= 0:\n        lon_part = prefixes[2] + str(int(lon)).zfill(3)\n    else:\n        lon_part = prefixes[3] + str(int(np.abs(lon))).zfill(3)\n    return lat_part + lon_part + postfix\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.utils.merge_tiles","title":"<code>merge_tiles(tiles, out_filename, out_format, compress=False)</code>","text":"<p>Merge tiles via buildvrt and translate.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <p>The list of tiles to be merged.</p> required <code>out_format</code> <p>The format of the output image.</p> required <code>out_filename</code> <p>The name of the output COG.</p> required Source code in <code>asf_tools/watermasking/utils.py</code> <pre><code>def merge_tiles(tiles, out_filename, out_format, compress=False):\n\"\"\"Merge tiles via buildvrt and translate.\n\n    Args:\n        tiles: The list of tiles to be merged.\n        out_format: The format of the output image.\n        out_filename: The name of the output COG.\n    \"\"\"\n    vrt = 'merged.vrt'\n    build_vrt_command = ['gdalbuildvrt', vrt] + tiles\n    if not compress:\n        translate_command = ['gdal_translate', '-of', out_format, vrt, out_filename]\n    else:\n        translate_command = [\n            'gdal_translate',\n            '-of',\n            out_format,\n            '-co',\n            'COMPRESS=LZW',\n            '-co',\n            'NUM_THREADS=all_cpus',\n            vrt,\n            out_filename,\n        ]\n    subprocess.run(build_vrt_command)\n    subprocess.run(translate_command)\n    remove_temp_files([vrt])\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.utils.remove_temp_files","title":"<code>remove_temp_files(temp_files)</code>","text":"<p>Remove each file in a list of files.</p> <p>Parameters:</p> Name Type Description Default <code>temp_files</code> <code>list</code> <p>The list of temporary files to remove.</p> required Source code in <code>asf_tools/watermasking/utils.py</code> <pre><code>def remove_temp_files(temp_files: list):\n\"\"\"Remove each file in a list of files.\n\n    Args:\n        temp_files: The list of temporary files to remove.\n    \"\"\"\n    for file in temp_files:\n        try:\n            os.remove(file)\n        except FileNotFoundError:\n            print(f'Temp file {file} was not found, skipping removal...')\n</code></pre>"},{"location":"tools/asf_tools_api/#asf_tools.watermasking.utils.setup_directories","title":"<code>setup_directories(dirs)</code>","text":"<p>Setup the directories necessary for running the script.</p> Source code in <code>asf_tools/watermasking/utils.py</code> <pre><code>def setup_directories(dirs: list[str]):\n\"\"\"Setup the directories necessary for running the script.\"\"\"\n    for directory in dirs:\n        try:\n            os.mkdir(directory)\n        except FileExistsError:\n            # Directories already exists.\n            pass\n</code></pre>"},{"location":"tutorials/process-new-granules-for-search-parameters/","title":"Using the HyP3 SDK to process new granules for given search parameters","text":"<p>In the past, ASF offered subscription functionality for HyP3 products.  A user could create a subscription with a particular set of search parameters (date range, area of interest, etc.),  and new Sentinel-1 acquisitions that met these criteria would be automatically submitted for processing.</p> <p>The following Jupyter notebooks demonstrate how to achieve subscription-like functionality.  This workflow is particularly useful for ongoing monitoring of a geographic area of interest.</p> <p>The first notebook demonstrates how to submit RTC jobs using this method, while the second notebook demonstrates  how to submit InSAR jobs. These tutorials can easily be adapted to support other job types.  Please contact us if you need help adapting these tutorials for your particular use case.</p> <ul> <li>Using the HyP3 SDK to generate RTC products for given search parameters</li> <li>Using the HyP3 SDK to generate InSAR products for given search parameters</li> </ul>"},{"location":"using/api/","title":"Using the HyP3 API","text":"<p>HyP3's API is built on OpenAPI  and Swagger.</p> <p>A friendly interface for exploring a HyP3 deployment's API is available at the <code>/ui/</code> endpoint:</p> <ul> <li>HyP3 Basic: https://hyp3-api.asf.alaska.edu/ui/</li> <li>HyP3+: https://hyp3-plus.asf.alaska.edu/ui/</li> </ul> <p>The process of using the HyP3 API is the same for any deployment of HyP3. Importantly, each deployment of HyP3 is  completely independent, so if you submit jobs to one HyP3 deployment, you will not be able to see them in a different  HyP3 deployment. </p>"},{"location":"using/api/#authentication","title":"Authentication","text":"<p>In order to use the API, you'll need an <code>asf-urs</code> session cookie, which you can get by signing in to Vertex.</p> <p></p> <p>Alternatively, you can  generate an Earthdata Login user token,  and enter it in the <code>BearerAuth</code> field by clicking the <code>Authorize</code> button in the Swagger UI. </p> <p>Refer to the Authentication  page for more detailed guidance on authentication methods.</p>"},{"location":"using/api/#confirm-you-are-authenticated","title":"Confirm you are authenticated","text":"<p>To confirm you are authenticated, you can run a <code>GET</code> request to our <code>/user</code> endpoint. Select the blue <code>GET</code> button next to <code>/user</code> and click the <code>Try it out</code> button. </p> <p>Then, execute the request and look at the response </p> <p>If you get a <code>Code 200</code> you should see a JSON dictionary of your user information.</p> <p>Authentication Required</p> <p>If you get a 401 response back you need to sign in to Vertex to get the <code>asf-urs</code> session cookie.</p> <pre><code>{\n\"detail\": \"No authorization token provided\",\n\"status\": 401,\n\"title\": \"Unauthorized\",\n\"type\": \"about:blank\"\n}\n</code></pre>"},{"location":"using/api/#submitting-sentinel-1-rtc-jobs","title":"Submitting Sentinel-1 RTC jobs","text":"<p>Jobs are submitted through the API by providing a JSON payload with a list of job definitions.</p> <p>Sentinel-1 jobs are submitted using  ESA granule IDs. A minimal job list for a single Sentinel-1 RTC job would look like:</p> <pre><code>{\n\"jobs\": [\n{\n\"name\": \"minimal-rtc-example\",\n\"job_type\": \"RTC_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1A_IW_GRDH_1SDV_20210214T154837_20210214T154901_036588_044C54_032E\"\n]\n}\n}\n]\n}\n</code></pre> <p>The job list may contain up to 200 job definitions. You can also provide custom RTC options: <pre><code>{\n\"jobs\": [\n{\n\"name\": \"custom-rtc-example\",\n\"job_type\": \"RTC_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1B_IW_GRDH_1SDV_20210210T153157_20210210T153222_025546_030B48_2901\"\n],\n\"radiometry\": \"gamma0\",\n\"scale\": \"power\",\n\"dem_matching\": false,\n\"include_dem\": true,\n\"include_inc_map\": true,\n\"include_scattering_area\": false,\n\"speckle_filter\": false\n}\n},\n{\n\"name\": \"custom-rtc-example\",\n\"job_type\": \"RTC_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1B_IW_GRDH_1SDV_20210210T153132_20210210T153157_025546_030B48_4E31\"\n],\n\"radiometry\": \"sigma0\",\n\"scale\": \"amplitude\",\n\"dem_matching\": false,\n\"include_dem\": false,\n\"include_inc_map\": false,\n\"include_scattering_area\": true,\n\"speckle_filter\": true\n}\n}\n]\n}\n</code></pre></p>"},{"location":"using/api/#submitting-opera-rtc-s1-jobs","title":"Submitting OPERA-RTC-S1 jobs","text":"<p>The OPERA-RTC-S1 job takes a single co-pol (VV or HH) ESA granule burst ID  from a Sentinel-1 IW SLC acquisition that meets the  date range  and  spatial extent  constraints for processing.</p> <p>Sentinel-1 IW Burst SLCs submitted for processing must have been acquired between April 14, 2016, and December 31, 2021,  and must be north of -60\u00b0 latitude.</p> <pre><code>    {\n\"job_type\": \"OPERA_RTC_S1\",\n\"name\": \"opera-rtc-s1-example\",\n\"job_parameters\": {\n\"granules\": [\"S1_073251_IW2_20200128T020712_VV_2944-BURST\"]\n}\n}\n</code></pre>"},{"location":"using/api/#submitting-sentinel-1-insar-jobs","title":"Submitting Sentinel-1 InSAR jobs","text":"<p>You can also submit InSAR jobs for scene pairs using ESA granule IDs. <pre><code>{\n\"jobs\": [\n{\n\"name\": \"minimal-insar-example\",\n\"job_type\": \"INSAR_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1A_IW_SLC__1SDV_20200203T172103_20200203T172122_031091_03929B_3048\",\n\"S1A_IW_SLC__1SDV_20200110T172104_20200110T172123_030741_03864E_A996\"\n]\n}\n},\n{\n\"name\": \"custom-insar-example\",\n\"job_type\": \"INSAR_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1A_IW_SLC__1SDV_20200527T195012_20200527T195028_032755_03CB56_3D96\",\n\"S1A_IW_SLC__1SDV_20200515T195012_20200515T195027_032580_03C609_4EBA\"\n],\n\"looks\": \"10x2\",\n\"include_look_vectors\": true,\n\"include_los_displacement\": true\n}\n}\n]\n}\n</code></pre></p>"},{"location":"using/api/#submitting-sentinel-1-burst-insar-jobs","title":"Submitting Sentinel-1 Burst InSAR jobs","text":"<p>You can submit InSAR jobs using the <code>INSAR_ISCE_BURST</code> job type, which takes a single pair of ESA granule burst IDs, or use the <code>INSAR_ISCE_MULTI_BURST</code> job type, which accepts lists of up to 15 contiguous along-track burst IDs that will be merged together to produce reference and secondary input SLCs. <pre><code>{\n\"jobs\": [\n{\n\"job_type\": \"INSAR_ISCE_BURST\",\n\"name\": \"single-burst-example\",\n\"job_parameters\": {\n\"granules\": [\n\"S1_136231_IW2_20200604T022312_VV_7C85-BURST\",\n\"S1_136231_IW2_20200616T022313_VV_5D11-BURST\"\n]\n}\n},\n{\n\"job_type\": \"INSAR_ISCE_MULTI_BURST\",\n\"name\": \"multi-burst-example\",\n\"job_parameters\": {\n\"reference\": [\n\"S1_136231_IW2_20200604T022312_VV_7C85-BURST\",\n\"S1_136232_IW2_20200604T022315_VV_7C85-BURST\"\n],\n\"secondary\": [\n\"S1_136231_IW2_20200616T022313_VV_5D11-BURST\",\n\"S1_136232_IW2_20200616T022316_VV_5D11-BURST\"\n],\n\"apply_water_mask\": true,\n\"looks\": \"5x1\"\n}\n}\n]\n}\n</code></pre></p>"},{"location":"using/api/#submitting-aria-s1-gunw-jobs","title":"Submitting ARIA-S1-GUNW jobs","text":"<p>The ARIA-S1-GUNW job type takes a reference date, a secondary date, and an ARIA-S1-GUNW Frame ID as input. See the ARIA-S1-GUNW Product Guide Frame ID section for more details on these inputs. <pre><code>    {\n\"job_type\": \"ARIA_S1_GUNW\",\n\"name\": \"gunw-example\",\n\"job_parameters\": {\n\"reference_date\": \"2019-12-31\",\n\"secondary_date\": \"2018-12-12\",\n\"frame_id\": 11040\n}\n}\n</code></pre></p>"},{"location":"using/api/#submitting-autorift-jobs","title":"Submitting autoRIFT jobs","text":"<p>AutoRIFT supports processing Sentinel-1, Sentinel-2, or Landsat-8 Collection 2 pairs.</p> <ul> <li>Sentinel-1 jobs are submitted using ESA granule IDs</li> <li>Sentinel-2 jobs are submitted using ESA granule IDs</li> <li>Landsat-8 Collection 2 jobs are submitted using USGS scene IDs</li> </ul> <p>To submit an example set of jobs including all supported missions, you could write a job list like:</p> <pre><code>{\n\"jobs\": [\n{\n\"name\": \"autorift-example\",\n\"job_type\": \"AUTORIFT\",\n\"job_parameters\": {\n\"granules\": [\n\"S1A_IW_SLC__1SSH_20170221T204710_20170221T204737_015387_0193F6_AB07\",\n\"S1B_IW_SLC__1SSH_20170227T204628_20170227T204655_004491_007D11_6654\"\n]\n}\n},\n{\n\"name\": \"autorift-example\",\n\"job_type\": \"AUTORIFT\",\n\"job_parameters\": {\n\"granules\": [\n\"S2B_MSIL1C_20200612T150759_N0209_R025_T22WEB_20200612T184700\",\n\"S2A_MSIL1C_20200627T150921_N0209_R025_T22WEB_20200627T170912\"\n]\n}\n},\n{\n\"name\": \"autorift-example\",\n\"job_type\": \"AUTORIFT\",\n\"job_parameters\": {\n\"granules\": [\n\"LC08_L1TP_009011_20200703_20200913_02_T1\",\n\"LC08_L1TP_009011_20200820_20200905_02_T1\"\n]\n}\n}\n]\n}\n</code></pre> <p>With your JSON jobs definition, you can <code>POST</code> to the <code>/jobs</code> endpoint to submit the jobs.</p> <ol> <li>click the green <code>POST</code> button next to <code>/jobs</code></li> <li>click <code>Try it out</code> on the right</li> <li>paste your jobs definition into the <code>Request body</code></li> <li>click <code>execute</code></li> </ol> <p></p> <p>If your jobs were submitted successfully you should see a <code>Code 200</code> and a JSON response of your job list, with some additional job attributes filled in.</p>"},{"location":"using/api/#querying-jobs","title":"Querying jobs","text":"<p>You can <code>GET</code> job information from the <code>/jobs</code> endpoint. You may provide query parameters to filter which jobs are returned: </p> <p>For our above examples, you can get the RTC job that was submitted with the default options by searching for <code>name=minimal-rtc-example</code>. If you provide no query parameters, you'll get a JSON response with a jobs list for every job you've submitted.</p> <p>Within the jobs list, a complete job dictionary will look like: <pre><code>{\n\"jobs\": [\n{\n\"name\": \"minimal-rtc-example\",\n\"job_type\": \"RTC_GAMMA\",\n\"job_parameters\": {\n\"granules\": [\n\"S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8\"\n]\n},\n\"job_id\": \"20c377be-2511-46a8-b908-e015abd3c24e\",\n\"user_id\": \"MY_EDL_USERNAME\",\n\"status_code\": \"SUCCEEDED\",\n\"request_time\": \"2021-02-24T21:30:45+00:00\",\n\"expiration_time\": \"2021-03-11T00:00:00+00:00\",\n\"files\": [\n{\n\"filename\": \"S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\",\n\"s3\": {\n\"bucket\": \"hyp3-contentbucket-fo259f6r6dn6\",\n\"key\": \"20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\"\n},\n\"size\": 28676279,\n\"url\": \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\"\n}\n],\n\"browse_images\": [\n\"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.png\"\n],      \"thumbnail_images\": [\n\"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA_thumb.png\"\n],\n\"logs\": [\n\"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/20c377be-2511-46a8-b908-e015abd3c24e.log\"\n]\n}\n]\n}\n</code></pre></p> <p>Importantly, the <code>files</code> block provides download links for the product files.</p> <p>For large queries results may be truncated. In this case there will be a <code>next</code> key in the response that will contain a URL to continue the query (this response may be similarly truncated and include a <code>next</code> key). <pre><code>{\n\"jobs\": [\n...\n],\n\"next\": \"https://hyp3-api.asf.alaska.edu/jobs?start_token=eyJqb2JfaWQiOiAiYzk1MDUzY2ItYWQzNy00ZGFhLTgxZDItYzA0YmQ4NWZiNDhiIiwgInVzZXJfaWQiOiAiamxyaW5lMiIsICJyZXF1ZXN0X3RpbWUiOiAiMjAyMC0xMC0yOVQxOTo0Mzo0NCswMDowMCJ9\"\n}\n</code></pre></p>"},{"location":"using/authentication/","title":"Authentication with HyP3","text":"<p>Users must authenticate with  Earthdata Login  credentials before they can submit jobs to HyP3 for processing or access information  about the resulting On Demand products.</p> <p>The options available for authentication depend on the interface you are using to interact with HyP3 functionality.</p> <ul> <li>The Vertex search and discovery    interface is a map-based web application that allows users to search for Sentinel-1 acquisitions, submit them for    processing by HyP3, and access the resulting On Demand products.</li> <li>Programmatic access to HyP3 functionality and output products is available through:<ul> <li>The HyP3 API</li> <li>The HyP3 Python SDK</li> </ul> </li> </ul> <p>Authentication is not required when downloading On Demand products generated by HyP3, but you must  authenticate in order to look up the download URLs for these products.</p>"},{"location":"using/authentication/#earthdata-login-edl","title":"Earthdata Login (EDL)","text":"<p>Earthdata Login  (EDL) is the authentication method used across NASA's  Earth Observation System Data Information System (EOSDIS).  These credentials provide access to any of the Earth Science data products served by EOSDIS, regardless of the  identity of the data curator.</p> <p>There is no cost to  register for EDL credentials,  and the process is quick and easy. When creating your profile, make sure to select an item in the Study Area  field, as you may encounter access errors if that field is left blank.</p> <p></p>"},{"location":"using/authentication/#username-and-password","title":"Username and Password","text":"<p>Most authentication workflows involve providing the username and password you set when registering for EDL. You may  be prompted to enter them into a GUI, such as when you are signing in to Vertex, they may be saved in a .netrc file  for automatic use with some programmatic interfaces, or you may be prompted to enter them as part of a scripted  workflow.</p>"},{"location":"using/authentication/#earthdata-login-token","title":"Earthdata Login Token","text":"<p>Earthdata Login also supports the generation of tokens that are valid for 60 days. These user/bearer tokens can be  used for EDL authentication instead of entering a username and password. The  User Token Management  document provides step-by-step guidance for generating an EDL token, which you can do either in the  Earthdata Login web interface  or by using the  User Tokens API.</p>"},{"location":"using/authentication/#authentication-in-vertex","title":"Authentication in Vertex","text":"<p>Click the Sign In button in Vertex,  which opens a  version of the Earthdata Login GUI  customized for ASF applications. Enter your  Earthdata Login credentials  to enable access to all functionality in Vertex linked to EDL credentials, including  On Demand capabilities.</p> <p></p>"},{"location":"using/authentication/#authentication-with-the-hyp3-api","title":"Authentication with the HyP3 API","text":"<p>There are a couple of authentication methods available when using the  HyP3 API.  The most common is using an  Earthdata Login session cookie,  but you can also use an  Earthdata Login bearer token.</p>"},{"location":"using/authentication/#earthdata-login-session-cookie","title":"Earthdata Login Session Cookie","text":"<p>You can authorize the HyP3 API by having a valid ASF Earthdata Login (asf-urs) session cookie in your web environment.  This cookie is generated when you  sign in to Vertex. </p> <p>If you log in directly to the  Earthdata Login GUI,  you will still need to click the  Sign In button in Vertex  to generate the ASF-specific cookie before you can use the HyP3 API. If you have already logged in with EDL in  another application, you may not need to re-enter your credentials, but clicking the Sign In button will  generate the necessary asf-urs session cookie.</p> <p>If you do not have a valid asf-urs session cookie before launching the  HyP3 API Swagger UI,  you will see an error response when submitting an API request: </p> <p></p> <p>You may need to refresh the browser window with the HyP3 API Swagger UI after you sign in to Vertex  before you will be able to submit API requests successfully.</p>"},{"location":"using/authentication/#earthdata-login-bearer-token","title":"Earthdata Login Bearer Token","text":"<p>Users can generate an EDL token, as described in the  Earthdata Login Token  section of this document.</p> <p>Once you have a valid EDL token: </p> <ol> <li>Open the Swagger UI interface for the HyP3 API</li> <li>Click the Authorize button at the top of the Swagger UI page</li> <li>Enter or paste your EDL token in the Value field of the <code>BearerAuth</code> section</li> <li>Click the Authorize button in the <code>Available authorizations</code> window to apply the token</li> </ol> <p></p>"},{"location":"using/authentication/#authentication-with-the-hyp3-python-sdk","title":"Authentication with the HyP3 Python SDK","text":"<p>Authentication to the API occurs during the  initialization of the HyP3 object  when using the HyP3 Python SDK.</p> <p>There are several options for authentication when using this method, including:</p> <ul> <li>Adding your Earthdata Login (EDL) credentials to your local <code>.netrc</code> file</li> <li>Entering your EDL username and password</li> <li>Entering an EDL token</li> </ul> <p>Refer to the  Authenticate HyP3 in the SDK notebook  for guidance on how to implement one of these options using the SDK.</p>"},{"location":"using/credits/","title":"Credits","text":"<p>HyP3 Basic On Demand users are given a free allotment of 8,000 credits per month to use for  processing jobs, and each type of job costs a different number of credits, as shown in the  Credit Cost Table.</p>"},{"location":"using/credits/#credit-allocation","title":"Credit Allocation","text":"<p>The credit cost of a given job is roughly proportional to the computational resources required to process the job, allowing us to distribute our resources more equitably. This supports our mission of making remote-sensing data accessible, with the goal of providing valuable products to the widest breadth of users possible.</p> <p>If your monthly credit allotment is not sufficient for your processing needs, you can purchase additional processing credits through HyP3+. Please contact us if you have questions about additional processing.</p>"},{"location":"using/credits/#credit-cost-table","title":"Credit Cost Table","text":"<ul> <li>The Credit Cost column indicates the number of credits it takes to process a single product of that job type.</li> <li>The HyP3+ Job Cost column indicates the dollar cost to process a single product of that job type if you are using  purchased credits.</li> <li>The HyP3 Basic Max Jobs Per Month column displays the maximum number of jobs that you would be able to run in a single  month if you spent your entire monthly HyP3 Basic credit allotment on jobs of that particular type.</li> </ul> Job Type CreditCost HyP3+Job Cost HyP3 BasicMax Jobs Per Month RTC \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 30-m pixel spacing 5 $0.25 1,600 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 20-m pixel spacing 15 $0.75 533 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10-m pixel spacing 60 $3.00 133 InSAR \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 80-m pixel spacing (20x4 looks) 10 $0.50 800 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 40-m pixel spacing (10x2 looks) 15 $0.75 533 ARIA S1 GUNW \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Standard product (90-m pixel spacing) 60 $3.00 133 Burst InSAR \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 80-m pixel spacing (20x4 looks) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u20134 pairs 1 $0.05 8,000 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5\u201312 pairs 5 $0.25 1,600 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13\u201315 pairs 10 $0.50 800 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 40-m pixel spacing (10x2 looks) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u20133 pairs 1 $0.05 8,000 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4\u20139 pairs 5 $0.25 1,600 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10\u201315 pairs 10 $0.50 800 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 20-m pixel spacing (5x1 looks) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1 pair 1 $0.05 8,000 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2 pairs 5 $0.25 1,600 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3 pairs 10 $0.50 800 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4 pairs 15 $0.75 533 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5 pairs 20 $1.00 400 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6 pairs 25 $1.25 320 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 7 pairs 30 $1.50 266 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8 pairs 35 $1.75 228 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9 pairs 40 $2.00 200 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10 pairs 45 $2.25 177 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11 pairs 90 $4.50 88 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 12 pairs 95 $4.75 84 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13 pairs 100 $5.00 80 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 14 pairs 105 $5.25 76 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 15 pairs 110 $5.50 72 AutoRIFT \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Standard product (120-m pixel spacing) 25 $1.25 320"},{"location":"using/credits/#contact-us","title":"Contact Us","text":"<p>Want to talk about HyP3? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p> <p>You can also reach us by email through ASF User Services: uso@asf.alaska.edu</p>"},{"location":"using/downloading/","title":"Downloading HyP3 Products","text":"<p>There are a number of interfaces available for downloading products generated On Demand using the HyP3 platform. </p> <ul> <li>On Demand Search interface in Vertex</li> <li>Programmatically using the HyP3 API or HyP3 Python SDK</li> </ul>"},{"location":"using/downloading/#on-demand-search-in-vertex","title":"On Demand Search in Vertex","text":"<p>The On Demand Products  Search Type in Vertex allows you to view the status of any job you have submitted for processing and download any  product that has been successfully processed. You will need to  sign in to Vertex with Earthdata Login credentials  to search for On Demand products. </p> <p>Refer to the  On Demand Products section of the Vertex User Manual  for more information about On Demand search functionality. </p>"},{"location":"using/downloading/#downloading-individual-products","title":"Downloading Individual Products","text":"<p>Click on an item in the search results to view download options. You can add the product to the Download Queue using  the cart icon in either the Search Result (left) or File (right) pane, or launch a direct download in your  browser window by clicking the cloud download icon in the File pane. </p> <p></p> <p>To view the download queue, click the Downloads icon in the top right of the Vertex web interface.  The Download Queue interface provides options for direct download of individual items along with bulk download options.</p>"},{"location":"using/downloading/#downloading-multiple-products","title":"Downloading Multiple Products","text":"<p>You can add products to the Download Queue one by one using the cart icon, or you can search your On Demand  products for a specific set of products and add them all to the Download Queue using the Queue button at the  top of the results list. </p> <p>There are a number of options available for filtering your On Demand products, with the most useful being the  Project Name field. You can assign a project name when submitting jobs for processing to easily group items  together that are used for the same project. </p> <p></p> <p>Once you've added products to the Download Queue, either by using the individual cart icons or the bulk Queue  button, click the Downloads icon in the top right of the Vertex web interface. When you open the Download Queue,  you have the option to launch direct downloads of individual items in the list, or you can choose to remove  individual items from the queue. </p> <p>To download all of the products listed in the Download Queue, click the Data Download button at the bottom of  the queue window and choose from the available options: </p>"},{"location":"using/downloading/#download-python-script","title":"Download Python Script","text":"<p>The most robust approach for downloading very long lists of products is the  Download Python Script option. This downloads a python script that you can launch on your computer. </p> <ul> <li>You will be prompted for your Earthdata Login credentials if necessary, then the script will work through the list    of download URLs, downloading them one by one until all of the items have been downloaded. </li> <li>If the script is interrupted during the download, you can simply re-run the same script; it will recognize    any products that have already been successfully downloaded and continue with the remaining items. </li> <li>To use this option, you must have a Python installation available on your computer. </li> </ul>"},{"location":"using/downloading/#download-all","title":"Download All","text":"<p>Chrome users may find the Download All option useful. This option takes advantage of the multi-threading  capability in Chrome to download several items at a time.</p>"},{"location":"using/downloading/#copy-urls","title":"Copy URLs","text":"<p>You can also click on Copy URLs (located next to the Data Download button) to copy a list of the download URLs  for the files in your Download Queue, which you can then paste into your own download script or use with the  <code>wget</code> utility. </p>"},{"location":"using/downloading/#programmatic-access","title":"Programmatic Access","text":"<p>The  HyP3 API  and the  HyP3 Python SDK  provide programmatic access to On Demand products. </p> <p>The <code>name</code> parameter is referred to as \"Project Name\" in the Vertex interface, but is often referred to as \"Job Name\"  in documentation for the programmatic interfaces. This parameter is helpful in grouping together jobs submitted as  part of a particular analysis effort. It facilitates management and download of groups of jobs, makes it easier to  share products with colleagues, and allows users to access their HyP3-generated products using the notebooks available  on ASF's OpenSARLab.</p> <p>The  HyP3 API  allows easy access to job URLs through the Swagger UI, but the  HyP3 Python SDK  is better suited for scripting search and download workflows for On Demand products. </p>"},{"location":"using/downloading/#accessing-products-using-the-hyp3-api","title":"Accessing Products Using the HyP3 API","text":"<p>The HyP3 API  provides the ability to  Query Submitted Jobs. </p>"},{"location":"using/downloading/#authentication","title":"Authentication","text":"<p>To look up your On Demand jobs, you will need to have a valid ASF Earthdata Login (asf-urs) session cookie,  which you can get by  signing in to Vertex with your Earthdata Login Credentials.  You can also authenticate using an  Earthdata Login token.  Refer to  Authentication with HyP3 API  documentation for more information about the available authentication methods.</p>"},{"location":"using/downloading/#entering-search-parameters","title":"Entering Search Parameters","text":"<p>Note that the parameter fields in the UI are populated with defaults. You will need to edit or delete any of the  default values that do not align with your desired search parameters. The start and end date fields reference  the date/time the jobs were submitted, not the date/time of the acquisitions used to generate the products. </p>"},{"location":"using/downloading/#response-json","title":"Response JSON","text":"<p>The response from your  Get Jobs API request  includes download links for the browse images and thumbnails used to display the product contents in Vertex,  as well as the link to the complete product package with a <code>.zip</code> extension. </p> <p>Here is an example of a response JSON: <pre><code>{\n  \"jobs\": [\n   {\n      \"processing_times\": [\n        836.557\n      ],\n      \"browse_images\": [\n        \"https://d3gm2hf49xd6jj.cloudfront.net/6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959.png\",\n        \"https://d3gm2hf49xd6jj.cloudfront.net/6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959_rgb.png\"\n      ],\n      \"credit_cost\": 5,\n      \"priority\": 9960,\n      \"execution_started\": true,\n      \"job_id\": \"6f917fec-9c2f-42fb-a55c-3c4fe2e0520b\",\n      \"name\": \"DEVELOP\",\n      \"thumbnail_images\": [\n        \"https://d3gm2hf49xd6jj.cloudfront.net/6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959_thumb.png\",\n        \"https://d3gm2hf49xd6jj.cloudfront.net/6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959_rgb_thumb.png\"\n      ],\n      \"request_time\": \"2025-06-10T07:47:28+00:00\",\n      \"logs\": [],\n      \"user_id\": \"hjkristenson\",\n      \"status_code\": \"SUCCEEDED\",\n      \"job_parameters\": {\n        \"speckle_filter\": false,\n        \"include_inc_map\": false,\n        \"dem_name\": \"copernicus\",\n        \"radiometry\": \"gamma0\",\n        \"granules\": [\n          \"S1A_IW_GRDH_1SDV_20250420T135654_20250420T135723_058841_074ABD_E7BB\"\n        ],\n        \"scale\": \"power\",\n        \"dem_matching\": false,\n        \"resolution\": 30,\n        \"include_rgb\": true,\n        \"include_dem\": false,\n        \"include_scattering_area\": false\n      },\n      \"files\": [\n        {\n          \"s3\": {\n            \"bucket\": \"hyp3-edc-prod-contentbucket-1fv14ed36ifj6\",\n            \"key\": \"6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959.zip\"\n          },\n          \"filename\": \"S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959.zip\",\n          \"size\": 656581903,\n          \"url\": \"https://d3gm2hf49xd6jj.cloudfront.net/6f917fec-9c2f-42fb-a55c-3c4fe2e0520b/S1A_IW_20250420T135654_DVP_RTC30_G_gpuned_7959.zip\"\n        }\n      ],\n      \"expiration_time\": \"2025-06-25T00:00:00+00:00\",\n      \"job_type\": \"RTC_GAMMA\"\n    },\n   ],\n}   \n</code></pre></p> <p>The download URL for the full product package is provided near the end of the response for each job in the <code>jobs</code>  array of the response JSON. The value is paired with the <code>url</code> key in the <code>files</code> array. </p> <p>You can copy and paste each product URL directly into a browser window, or script a workflow to pull all the  product URLs from the response JSON into a bulk download function or text file. It may be more convenient to use the  HyP3 Python SDK  to script bulk download functionality.</p>"},{"location":"using/downloading/#accessing-products-using-the-hyp3-python-sdk","title":"Accessing Products using the HyP3 Python SDK","text":"<p>The HyP3 Python SDK is a wrapper around the HyP3 API,  and provides convenient search and download functionality for On Demand products (HyP3 jobs).  This example notebook  demonstrates how to use the SDK for a range of workflows. </p>"},{"location":"using/downloading/#authentication_1","title":"Authentication","text":"<p>To access product information using the SDK, you will need to  authenticate when initializing the HyP3 object.  You can add Earthdata Login (EDL) credentials to your local <code>.netrc</code> file, or use a prompt to enter either  EDL credentials or an EDL token manually. Refer to the  Authenticate HyP3 in the SDK notebook  for authentication guidance and sample code.</p>"},{"location":"using/downloading/#search-for-jobs","title":"Search for Jobs","text":"<p>Use the <code>find_jobs</code> method from the <code>HyP3</code> class to generate a list of products to download (batch), then use the  <code>download_files</code> method from the <code>Batch</code> class to download all the products in the list. Refer to the  Finding Existing Jobs section and the  HyP3 SDK API Reference  for more information. </p>"},{"location":"using/downloading/#product-packaging-and-extraction","title":"Product Packaging and Extraction","text":"<p>On Demand products from ASF are delivered as zip files. The files contained in the zip archive vary by product type,  but the zip archive always includes an internal directory containing all the individual files. The directory names  can be quite long, and some users (particularly those using a Windows operating system) will need to make  accommodations in order to successfully extract the contents.</p>"},{"location":"using/downloading/#extracting-product-packages","title":"Extracting Product Packages","text":"<p>When extracting the contents of a HyP3-generated zip file, you may need to specify a destination directory to prevent  the extraction of the internal directory to a directory named with the full zip file name. For many of the products,  this combination of directories would result in paths that are longer than can be used with Windows operating systems. </p>"},{"location":"using/downloading/#downloading-individual-files","title":"Downloading Individual Files","text":"<p>Downloading the full zip file ensures that you have all of the data products as well as auxiliary files and  relevant metadata, but some users may not require all of the files included in the product zip archive. </p> <p>The contents of the zip files can all be accessed directly. Simply replace the <code>.zip</code> at the end of the download URL  with the tag for the specific file you want to download. </p> <p>For example, for the following download URL for an RTC On Demand product: </p> <pre><code>https://d3gm2hf49xd6jj.cloudfront.net/76b1a849-c826-428a-966c-55f8bb88f814/\nS1A_IW_20250502T135654_DVP_RTC30_G_gpuned_70DD.zip\n</code></pre> <p>simply replace the <code>.zip</code> with the desired product extension, such as <code>_VV.tif</code> for the RTC GeoTIFF in VV polarization: </p> <pre><code>https://d3gm2hf49xd6jj.cloudfront.net/76b1a849-c826-428a-966c-55f8bb88f814/\nS1A_IW_20250502T135654_DVP_RTC30_G_gpuned_70DD_VV.tif\n</code></pre> <p>You can then paste that URL into a browser window, or use it in a download script, to download only the designated  product rather than the full zip archive. </p> <p>The zip archive contains valuable metadata products, including a readme file that provides information  about the workflow used to generate the product and the files included in the product package. New users are advised to download the full archive to ensure they have access to this information  and can determine what individual products are required for their application. </p>"},{"location":"using/downloading/#downloading-products-submitted-by-other-users","title":"Downloading Products Submitted by Other Users","text":"<p>You can search for On Demand products processed under a different username. This functionality is a convenient  way to share products when collaborating with others, and can be accessed both in  Vertex and the  HyP3 Python SDK. </p>"},{"location":"using/downloading/#using-vertex","title":"Using Vertex","text":"<p>Click the Filters button to open the On Demand Search Filters window, and use  the User ID filter to enter the username under which the desired job was submitted. </p> <p></p> <p>If the user who submitted the job also provides you with the Project Name, you can apply that search filter,  as well. The drop-down list in the Project Name field only displays the list for the user who is logged in, so you  will not be able to look up another user's list of Project Names using this interface. </p>"},{"location":"using/downloading/#using-the-hyp3-python-sdk","title":"Using the HyP3 Python SDK","text":"<p>The HyP3 Python SDK provides the capability to search for products submitted by other users. Refer to  this notebook  to learn how.</p>"},{"location":"using/downloading/#downloading-with-wget","title":"Downloading with Wget","text":"<p>While authentication is required to look up download URLs  for On Demand products, it is not required for actually using the download URLs. This makes it easy to use any  method you prefer for downloading the products.</p> <p>Some users may find the  Wget  utility useful for downloading a long list of products.</p> <p>Create a text file containing a list of download URLs (i.e <code>urls.txt</code>). This list can be generated a number of  ways, including but not limited to: </p> <ul> <li>the Copy URLs functionality in Vertex</li> <li>extracting URLs from an API response JSON</li> <li>converting a list of download URLs for    On Demand product zip files    to a list of    individual file URLs</li> </ul> <p>You can then use the  input file  option with Wget to reference the text file containing a list of download URLs. </p> <p>The command is simply the following: <pre><code>wget -i urls.txt\n</code></pre></p>"},{"location":"using/requesting_access/","title":"Requesting Access","text":"<p>Starting on TODO: date, new users will be required to request access before submitting jobs for On Demand processing using the HyP3 platform. HyP3 accounts are limited to one per person. If you have used HyP3 for On Demand processing in the past, you will be able to continue using our service without requesting access.</p> <p>You will need an Earthdata Login  (EDL) account before requesting access. You can register here if you do not already have an account.</p> <p>After submitting your request, you will receive an email with your approval status within two business days. You can expect to be approved if you have not already registered for HyP3 access using a different Earthdata Login account.</p> <p>Once your access request has been approved, you can submit jobs for On Demand processing using your EDL credentials.  You only need to complete this approval process once. </p>"},{"location":"using/requesting_access/#workshops-and-tutorials","title":"Workshops and Tutorials","text":"<p>If you are leading a workshop or tutorial and your participants will require access to HyP3,  we can provide you with a unique access code to streamline the access request process. Your participants can enter  this code in the access request form to receive immediate, automatic approval.</p> <p>If you would like to request a unique access code, please email  uso@asf.alaska.edu with a description of your workshop or tutorial.</p>"},{"location":"using/requesting_access/#contact-us","title":"Contact Us","text":"<p>Email ASF User Services at uso@asf.alaska.edu with any questions  regarding access to HyP3. </p>"},{"location":"using/sdk/","title":"HyP3 SDK","text":"<p>A python wrapper around the HyP3 API</p> <pre><code>&gt;&gt;&gt; from hyp3_sdk import HyP3\n&gt;&gt;&gt; hyp3 = HyP3(username='MyUsername', password='MyPassword')\n\n&gt;&gt;&gt; granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8'\n&gt;&gt;&gt; job = hyp3.submit_rtc_job(granule=granule, name='MyNewJob')\n&gt;&gt;&gt; job = hyp3.watch(job)\n&gt;&gt;&gt; job.download_files()\n</code></pre>"},{"location":"using/sdk/#install","title":"Install","text":"<p>In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda or Python virtual environments.</p> <p>The HyP3 SDK can be installed into a conda environment with</p> <pre><code>conda install -c conda-forge hyp3_sdk\n</code></pre> <p>or into a virtual environment with</p> <pre><code>python -m pip install hyp3_sdk\n</code></pre>"},{"location":"using/sdk/#quick-usage","title":"Quick Usage","text":"<p>There are 3 main classes that the SDK exposes:</p> <ul> <li><code>HyP3</code> to perform HyP3 operations (find jobs, refresh job information, submitting new jobs)</li> <li><code>Job</code> to perform operations on single jobs (downloading products, check status)</li> <li><code>Batch</code> to perform operations on multiple jobs at once (downloading products, check status)</li> </ul> <p>An instance of the <code>HyP3</code> class will be needed to interact with the external HyP3 API.</p> <pre><code>&gt;&gt;&gt; from hyp3_sdk import HyP3\n&gt;&gt;&gt; hyp3 = HyP3(username='MyUsername', password='MyPassword')\n\n&gt;&gt;&gt; granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8'\n&gt;&gt;&gt; job = hyp3.submit_rtc_job(granule=granule, name='MyNewJob')\n&gt;&gt;&gt; job = hyp3.watch(job)\n&gt;&gt;&gt; job.download_files()\n</code></pre>"},{"location":"using/sdk/#connect-to-the-hyp3-api","title":"Connect to the HyP3 API","text":"<p>Create a connection to the HyP3 API using the <code>HyP3</code> class: <pre><code>import hyp3_sdk as sdk\nhyp3 = sdk.HyP3()\n</code></pre></p> <p>This connects to the HyP3 Basic deployment by default. To connect to an alternate deployment such as HyP3+, specify the corresponding <code>api_url</code>: <pre><code>hyp3_plus = sdk.HyP3(api_url='https://hyp3-plus.asf.alaska.edu')\n</code></pre></p>"},{"location":"using/sdk/#submitting-jobs","title":"Submitting Jobs","text":"<p><code>hyp3</code> has member functions for submitting new jobs: <pre><code>rtc_job = hyp3.submit_rtc_job('granule_id', 'job_name')\ninsar_job = hyp3.submit_insar_job('reference_granule_id', 'secondary_granule_id', 'job_name')\ninsar_burst_job = hyp3.submit_insar_isce_burst_job('reference_granule_id', 'secondary_granule_id', 'job_name')\ninsar_multi_burst_job = hyp3.submit_insar_isce_multi_burst_job(['ref_id_1', 'ref_id_2'], ['sec_id_1', 'sec_id_2'], 'job_name')\nautorift_job = hyp3.submit_autorift_job('reference_granule_id', 'secondary_granule_id', 'job_name')\naria_s1_gunw_job = hyp3.submit_aria_s1_gunw_job('ref_date', 'sec_date', 'frame_id', 'job_name')\nopera_rtc_s1_job = hyp3.submit_opera_rtc_s1_job('granule_id')\n</code></pre> Each of these functions will return an instance of the <code>Job</code> class that represents a new HyP3 job request.</p>"},{"location":"using/sdk/#finding-existing-jobs","title":"Finding Existing Jobs","text":"<p>To find HyP3 jobs that were run previously, you can use the <code>hyp3.find_jobs()</code> <pre><code>batch = hyp3.find_jobs()\n</code></pre> This will return a <code>Batch</code> instance representing all jobs owned by you. You can also pass parameters to query to a specific set of jobs</p>"},{"location":"using/sdk/#operations-on-job-and-batch","title":"Operations on Job and Batch","text":"<p>If your jobs are not complete you can use the HyP3 instance to update them, and wait from completion <pre><code>batch = hyp3.find_jobs()\nif not batch.complete():\n    # to get updated information\n    batch = hyp3.refresh(batch)\n    # or to wait until completion and get updated information (which will take a fair bit)\n    batch = hyp3.watch(batch)\n</code></pre></p> <p>Once you have complete jobs you can download the products to your machine <pre><code>batch.download_files()\n</code></pre></p> <p>These operations also work on <code>Job</code> objects <pre><code>job = hyp3.submit_rtc_job('S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8', 'MyJobName')\njob = hyp3.watch(job)\njob.download_files()\n</code></pre></p>"},{"location":"using/sdk/#documentation","title":"Documentation","text":"<p>For the full SDK API Reference, see the HyP3 documentation</p>"},{"location":"using/sdk/#contact-us","title":"Contact Us","text":"<p>Want to talk about the HyP3 SDK? We would love to hear from you!</p> <p>Found a bug? Want to request a feature? Open an issue</p> <p>General questions? Suggestions? Or just want to talk to the team? Chat with us on Gitter</p>"},{"location":"using/sdk_api/","title":"<code>hyp3_sdk</code> v7.7.5 API Reference","text":""},{"location":"using/sdk_api/#hyp3_sdk","title":"<code>hyp3_sdk</code>","text":"<p>A python wrapper around the HyP3 API</p>"},{"location":"using/sdk_api/#hyp3_sdk.Batch","title":"<code>Batch</code>","text":"Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>class Batch:\n    def __init__(self, jobs: list[Job] | None = None):\n        if jobs is None:\n            jobs = []\n        self.jobs = jobs\n\n    def __add__(self, other: Union[Job, 'Batch']):\n        if isinstance(other, Batch):\n            return Batch(self.jobs + other.jobs)\n        elif isinstance(other, Job):\n            return Batch(self.jobs + [other])\n        else:\n            raise TypeError(f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\")\n\n    def __iadd__(self, other: Union[Job, 'Batch']):\n        if isinstance(other, Batch):\n            self.jobs += other.jobs\n        elif isinstance(other, Job):\n            self.jobs += [other]\n        else:\n            raise TypeError(f\"unsupported operand type(s) for +=: '{type(self)}' and '{type(other)}'\")\n        return self\n\n    def __iter__(self):\n        return iter(self.jobs)\n\n    def __len__(self):\n        return len(self.jobs)\n\n    def __contains__(self, job: Job):\n        return job in self.jobs\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Batch):\n            raise TypeError('`__eq__` can only compare a Batch object with another Batch object.')\n        return self.jobs == other.jobs\n\n    def __delitem__(self, job: int):\n        self.jobs.pop(job)\n        return self\n\n    def __getitem__(self, index: int | slice):\n        if isinstance(index, slice):\n            return Batch(self.jobs[index])\n        return self.jobs[index]\n\n    def __setitem__(self, index: int, job: Job):\n        self.jobs[index] = job\n        return self\n\n    def __repr__(self):\n        reprs = ', '.join([job.__repr__() for job in self.jobs])\n        return f'Batch([{reprs}])'\n\n    def __str__(self):\n        count = self._count_statuses()\n        return (\n            f'{len(self)} HyP3 Jobs: '\n            f'{count[\"SUCCEEDED\"]} succeeded, '\n            f'{count[\"FAILED\"]} failed, '\n            f'{count[\"RUNNING\"]} running, '\n            f'{count[\"PENDING\"]} pending.'\n        )\n\n    def _count_statuses(self):\n        return Counter([job.status_code for job in self.jobs])\n\n    def complete(self) -&gt; bool:\n\"\"\"Returns: True if all jobs are complete, otherwise returns False\"\"\"\n        for job in self.jobs:\n            if not job.complete():\n                return False\n        return True\n\n    def succeeded(self) -&gt; bool:\n\"\"\"Returns: True if all jobs have succeeded, otherwise returns False\"\"\"\n        for job in self.jobs:\n            if not job.succeeded():\n                return False\n        return True\n\n    def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n            location: Directory location to put files into\n            create: Create `location` if it does not point to an existing directory\n\n        Returns: list of Path objects to downloaded files\n        \"\"\"\n        downloaded_files = []\n        tqdm = get_tqdm_progress_bar()\n        for job in tqdm(self.jobs):\n            try:\n                downloaded_files.extend(job.download_files(location, create))\n            except HyP3SDKError as e:\n                print(f'Warning: {e} Skipping download for {job}.')\n        return downloaded_files\n\n    def any_expired(self) -&gt; bool:\n\"\"\"Check succeeded jobs for expiration\"\"\"\n        for job in self.jobs:\n            try:\n                if job.expired():\n                    return True\n            except HyP3SDKError:\n                continue\n        return False\n\n    def filter_jobs(\n        self,\n        succeeded: bool = True,\n        pending: bool = True,\n        running: bool = True,\n        failed: bool = False,\n        include_expired: bool = True,\n    ) -&gt; 'Batch':\n\"\"\"Filter jobs by status. By default, only succeeded, pending,\n        and still running jobs will be in the returned batch.\n\n        Args:\n            succeeded: Include all succeeded jobs\n            pending: Include all pending jobs\n            running: Include all running jobs\n            failed: Include all failed jobs\n            include_expired: Include expired jobs in the result\n\n\n        Returns:\n             batch: A batch object containing jobs matching all the selected statuses\n        \"\"\"\n        filtered_jobs = []\n\n        for job in self.jobs:\n            if job.succeeded() and succeeded:\n                if include_expired or not job.expired():\n                    filtered_jobs.append(job)\n\n            elif job.running() and running:\n                filtered_jobs.append(job)\n\n            elif job.pending() and pending:\n                filtered_jobs.append(job)\n\n            elif job.failed() and failed:\n                filtered_jobs.append(job)\n\n        return Batch(filtered_jobs)\n\n    def total_credit_cost(self):\n        return sum(job.credit_cost for job in self.jobs if job.credit_cost is not None)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Batch.any_expired","title":"<code>any_expired()</code>","text":"<p>Check succeeded jobs for expiration</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def any_expired(self) -&gt; bool:\n\"\"\"Check succeeded jobs for expiration\"\"\"\n    for job in self.jobs:\n        try:\n            if job.expired():\n                return True\n        except HyP3SDKError:\n            continue\n    return False\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Batch.complete","title":"<code>complete()</code>","text":"<p>Returns: True if all jobs are complete, otherwise returns False</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def complete(self) -&gt; bool:\n\"\"\"Returns: True if all jobs are complete, otherwise returns False\"\"\"\n    for job in self.jobs:\n        if not job.complete():\n            return False\n    return True\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Batch.download_files","title":"<code>download_files(location='.', create=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>location</code> <code>Path | str</code> <p>Directory location to put files into</p> <code>'.'</code> <code>create</code> <code>bool</code> <p>Create <code>location</code> if it does not point to an existing directory</p> <code>True</code> <p>Returns: list of Path objects to downloaded files</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n        location: Directory location to put files into\n        create: Create `location` if it does not point to an existing directory\n\n    Returns: list of Path objects to downloaded files\n    \"\"\"\n    downloaded_files = []\n    tqdm = get_tqdm_progress_bar()\n    for job in tqdm(self.jobs):\n        try:\n            downloaded_files.extend(job.download_files(location, create))\n        except HyP3SDKError as e:\n            print(f'Warning: {e} Skipping download for {job}.')\n    return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Batch.filter_jobs","title":"<code>filter_jobs(succeeded=True, pending=True, running=True, failed=False, include_expired=True)</code>","text":"<p>Filter jobs by status. By default, only succeeded, pending, and still running jobs will be in the returned batch.</p> <p>Parameters:</p> Name Type Description Default <code>succeeded</code> <code>bool</code> <p>Include all succeeded jobs</p> <code>True</code> <code>pending</code> <code>bool</code> <p>Include all pending jobs</p> <code>True</code> <code>running</code> <code>bool</code> <p>Include all running jobs</p> <code>True</code> <code>failed</code> <code>bool</code> <p>Include all failed jobs</p> <code>False</code> <code>include_expired</code> <code>bool</code> <p>Include expired jobs in the result</p> <code>True</code> <p>Returns:</p> Name Type Description <code>batch</code> <code>Batch</code> <p>A batch object containing jobs matching all the selected statuses</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def filter_jobs(\n    self,\n    succeeded: bool = True,\n    pending: bool = True,\n    running: bool = True,\n    failed: bool = False,\n    include_expired: bool = True,\n) -&gt; 'Batch':\n\"\"\"Filter jobs by status. By default, only succeeded, pending,\n    and still running jobs will be in the returned batch.\n\n    Args:\n        succeeded: Include all succeeded jobs\n        pending: Include all pending jobs\n        running: Include all running jobs\n        failed: Include all failed jobs\n        include_expired: Include expired jobs in the result\n\n\n    Returns:\n         batch: A batch object containing jobs matching all the selected statuses\n    \"\"\"\n    filtered_jobs = []\n\n    for job in self.jobs:\n        if job.succeeded() and succeeded:\n            if include_expired or not job.expired():\n                filtered_jobs.append(job)\n\n        elif job.running() and running:\n            filtered_jobs.append(job)\n\n        elif job.pending() and pending:\n            filtered_jobs.append(job)\n\n        elif job.failed() and failed:\n            filtered_jobs.append(job)\n\n    return Batch(filtered_jobs)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Batch.succeeded","title":"<code>succeeded()</code>","text":"<p>Returns: True if all jobs have succeeded, otherwise returns False</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def succeeded(self) -&gt; bool:\n\"\"\"Returns: True if all jobs have succeeded, otherwise returns False\"\"\"\n    for job in self.jobs:\n        if not job.succeeded():\n            return False\n    return True\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3","title":"<code>HyP3</code>","text":"<p>A python wrapper around the HyP3 API.</p> <p>Warning: All jobs submitted to HyP3 are publicly visible. For more information, see https://hyp3-docs.asf.alaska.edu/#public-visibility-of-jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>class HyP3:\n\"\"\"A python wrapper around the HyP3 API.\n\n    Warning: All jobs submitted to HyP3 are publicly visible. For more information, see\n    https://hyp3-docs.asf.alaska.edu/#public-visibility-of-jobs\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str = PROD_API,\n        username: str | None = None,\n        password: str | None = None,\n        token: str | None = None,\n        prompt: Literal['password', 'token'] | bool | None = None,\n    ):\n\"\"\"If username and password are not provided, attempts to use credentials from a `.netrc` file.\n\n        Args:\n            api_url: Address of the HyP3 API\n            username: Username for authenticating to `urs.earthdata.nasa.gov`.\n                Both username and password must be provided if either is provided.\n            password: Password for authenticating to `urs.earthdata.nasa.gov`.\n                Both username and password must be provided if either is provided.\n            token: Earthdata Login Bearer Token for authenticating to `urs.earthdata.nasa.gov`\n            prompt: Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.\n        \"\"\"\n        self.url = api_url\n\n        if prompt not in (True, False, 'password', 'token', None):\n            raise ValueError(f'Unexpected value {prompt} for `prompt`')\n\n        if prompt is True:\n            warnings.warn(\n                'Passing `prompt=True` is deprecated. Please use either `prompt=\"password\"` or `prompt=\"token\"`',\n                UserWarning,\n            )\n            prompt = 'password'\n\n        if prompt == 'password':\n            if username is None:\n                username = input('NASA Earthdata Login username: ')\n            if password is None:\n                password = getpass('NASA Earthdata Login password: ')\n\n        if prompt == 'token' and token is None:\n            token = getpass('NASA Earthdata Login bearer token: ')\n\n        self.session = hyp3_sdk.util.get_authenticated_session(username, password, token)\n        self.session.headers.update({'User-Agent': f'{hyp3_sdk.__name__}/{hyp3_sdk.__version__}'})\n\n        hostname = urlsplit(self.url).hostname\n        assert hostname is not None\n        if not hostname.endswith('.asf.alaska.edu') and 'asf-urs' in self.session.cookies:\n            self.session.cookies.set('asf-urs', self.session.cookies['asf-urs'], domain=hostname)\n\n    def _get_endpoint_url(self, endpoint: str) -&gt; str:\n        parts = urlsplit(self.url)\n        path = '/'.join([parts.path.strip('/'), endpoint.strip('/')])\n        return urlunsplit(SplitResult(scheme=parts.scheme, netloc=parts.netloc, path=path, query='', fragment=''))\n\n    def find_jobs(\n        self,\n        start: datetime | None = None,\n        end: datetime | None = None,\n        status_code: str | None = None,\n        name: str | None = None,\n        job_type: str | None = None,\n        user_id: str | None = None,\n    ) -&gt; Batch:\n\"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria\n\n        Args:\n            start: only jobs submitted after given time\n            end: only jobs submitted before given time\n            status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)\n            name: only jobs with this name\n            job_type: only jobs with this job_type\n            user_id: only jobs submitted by this user (defaults to the current user)\n\n        Returns:\n            A Batch object containing the found jobs\n        \"\"\"\n        params = {}\n        for param_name in ('start', 'end', 'status_code', 'name', 'job_type', 'user_id'):\n            param_value = locals().get(param_name)\n            if param_value is not None:\n                if isinstance(param_value, datetime):\n                    if param_value.tzinfo is None:\n                        param_value = param_value.replace(tzinfo=timezone.utc)\n                    param_value = param_value.isoformat(timespec='seconds')\n\n                params[param_name] = param_value\n\n        response = self.session.get(self._get_endpoint_url('/jobs'), params=params)\n        _raise_for_hyp3_status(response)\n        jobs = [Job.from_dict(job) for job in response.json()['jobs']]\n\n        while 'next' in response.json():\n            next_url = response.json()['next']\n            response = self.session.get(next_url)\n            _raise_for_hyp3_status(response)\n            jobs.extend([Job.from_dict(job) for job in response.json()['jobs']])\n\n        return Batch(jobs)\n\n    def get_job_by_id(self, job_id: str) -&gt; Job:\n\"\"\"Get job by job ID\n\n        Args:\n            job_id: A job ID\n\n        Returns:\n            A Job object\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url(f'/jobs/{job_id}'))\n        _raise_for_hyp3_status(response)\n\n        return Job.from_dict(response.json())\n\n    @singledispatchmethod\n    def watch(self, job_or_batch: Batch | Job, timeout: int = 10800, interval: int | float = 60) -&gt; Batch | Job:\n\"\"\"Watch jobs until they complete\n\n        Args:\n            job_or_batch: A Batch or Job object of jobs to watch\n            timeout: How long to wait until exiting in seconds\n            interval: How often to check for updates in seconds\n\n        Returns:\n            A Batch or Job object with refreshed watched jobs\n        \"\"\"\n        raise NotImplementedError(f'Cannot watch {type(job_or_batch)} type object')\n\n    @watch.register\n    def _watch_batch(self, batch: Batch, timeout: int = 10800, interval: int | float = 60) -&gt; Batch:\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        iterations_until_timeout = math.ceil(timeout / interval)\n        bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{postfix[0]}]'\n        with tqdm(total=len(batch), bar_format=bar_format, postfix=[f'timeout in {timeout} s']) as progress_bar:\n            for ii in range(iterations_until_timeout):\n                batch = self.refresh(batch)  # type: ignore [assignment]\n\n                counts = batch._count_statuses()\n                complete = counts['SUCCEEDED'] + counts['FAILED']\n\n                progress_bar.postfix = [f'timeout in {timeout - ii * interval}s']\n                # to control n/total manually; update is n += value\n                progress_bar.n = complete\n                progress_bar.update(0)\n\n                if batch.complete():\n                    return batch\n                time.sleep(interval)\n        raise HyP3Error(f'Timeout occurred while waiting for {batch}')\n\n    @watch.register\n    def _watch_job(self, job: Job, timeout: int = 10800, interval: int | float = 60) -&gt; Job:\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        iterations_until_timeout = math.ceil(timeout / interval)\n        bar_format = '{n_fmt}/{total_fmt} [{postfix[0]}]'\n        with tqdm(total=1, bar_format=bar_format, postfix=[f'timeout in {timeout} s']) as progress_bar:\n            for ii in range(iterations_until_timeout):\n                job = self.refresh(job)  # type: ignore [assignment]\n                progress_bar.postfix = [f'timeout in {timeout - ii * interval}s']\n                progress_bar.update(int(job.complete()))\n\n                if job.complete():\n                    return job\n                time.sleep(interval)\n        raise HyP3Error(f'Timeout occurred while waiting for {job}')\n\n    @singledispatchmethod\n    def refresh(self, job_or_batch: Batch | Job) -&gt; Batch | Job:\n\"\"\"Refresh each jobs' information\n\n        Args:\n            job_or_batch: A Batch of Job object to refresh\n\n        Returns:\n            A Batch or Job object with refreshed information\n        \"\"\"\n        raise NotImplementedError(f'Cannot refresh {type(job_or_batch)} type object')\n\n    @refresh.register\n    def _refresh_batch(self, batch: Batch):\n        jobs = []\n        for job in batch.jobs:\n            jobs.append(self.refresh(job))\n        return Batch(jobs)  # type: ignore [arg-type]\n\n    @refresh.register\n    def _refresh_job(self, job: Job):\n        return self.get_job_by_id(job.job_id)\n\n    def submit_prepared_jobs(self, prepared_jobs: dict | list[dict]) -&gt; Batch:\n\"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries\n\n        Args:\n            prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries\n\n        Returns:\n            A Batch object containing the submitted job(s)\n        \"\"\"\n        if isinstance(prepared_jobs, dict):\n            payload = {'jobs': [prepared_jobs]}\n        else:\n            payload = {'jobs': prepared_jobs}\n\n        response = self.session.post(self._get_endpoint_url('/jobs'), json=payload)\n        _raise_for_hyp3_status(response)\n\n        batch = Batch()\n        for job in response.json()['jobs']:\n            batch += Job.from_dict(job)\n        return batch\n\n    def submit_autorift_job(self, granule1: str, granule2: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an autoRIFT job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n\n        Returns:\n            A Batch object containing the autoRIFT job\n        \"\"\"\n        job_dict = self.prepare_autorift_job(granule1, granule2, name=name)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_autorift_job(cls, granule1: str, granule2: str, name: str | None = None) -&gt; dict:\n\"\"\"Submit an autoRIFT job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared autoRIFT job\n        \"\"\"\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2]},\n            'job_type': 'AUTORIFT',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_rtc_job(\n        self,\n        granule: str,\n        name: str | None = None,\n        dem_matching: bool = False,\n        include_dem: bool = False,\n        include_inc_map: bool = False,\n        include_rgb: bool = False,\n        include_scattering_area: bool = False,\n        radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n        resolution: Literal[10, 20, 30] = 30,\n        scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n        speckle_filter: bool = False,\n        dem_name: Literal['copernicus'] = 'copernicus',\n    ) -&gt; Batch:\n\"\"\"Submit an RTC job\n\n        Args:\n            granule: The granule (scene) to use\n            name: A name for the job\n            dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n            include_dem: Include the DEM file in the product package\n            include_inc_map: Include the local incidence angle map in the product package\n            include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n                (ignored for single-pol granules)\n            include_scattering_area: Include the scattering area in the product package\n            radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n                projected into the look direction (gamma0)\n            resolution: Desired output pixel spacing in meters\n            scale: Scale of output image; power, decibel or amplitude\n            speckle_filter: Apply an Enhanced Lee speckle filter\n            dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n            the Copernicus GLO-30 Public DEM.\n\n        Returns:\n            A Batch object containing the RTC job\n        \"\"\"\n        arguments = locals()\n        arguments.pop('self')\n        job_dict = self.prepare_rtc_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_rtc_job(\n        cls,\n        granule: str,\n        name: str | None = None,\n        dem_matching: bool = False,\n        include_dem: bool = False,\n        include_inc_map: bool = False,\n        include_rgb: bool = False,\n        include_scattering_area: bool = False,\n        radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n        resolution: Literal[10, 20, 30] = 30,\n        scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n        speckle_filter: bool = False,\n        dem_name: Literal['copernicus'] = 'copernicus',\n    ) -&gt; dict:\n\"\"\"Submit an RTC job\n\n        Args:\n            granule: The granule (scene) to use\n            name: A name for the job\n            dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n            include_dem: Include the DEM file in the product package\n            include_inc_map: Include the local incidence angle map in the product package\n            include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n                (ignored for single-pol granules)\n            include_scattering_area: Include the scattering area in the product package\n            radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n                projected into the look direction (gamma0)\n            resolution: Desired output pixel spacing in meters\n            scale: Scale of output image; power, decibel or amplitude\n            speckle_filter: Apply an Enhanced Lee speckle filter\n            dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n            the Copernicus GLO-30 Public DEM.\n\n        Returns:\n            A dictionary containing the prepared RTC job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['granule', 'name', 'cls']:\n            job_parameters.pop(key, None)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule], **job_parameters},\n            'job_type': 'RTC_GAMMA',\n        }\n\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_job(\n        self,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        include_look_vectors: bool = False,\n        include_los_displacement: bool = False,\n        include_inc_map: bool = False,\n        looks: Literal['20x4', '10x2'] = '20x4',\n        include_dem: bool = False,\n        include_wrapped_phase: bool = False,\n        apply_water_mask: bool = False,\n        include_displacement_maps: bool = False,\n        phase_filter_parameter: float = 0.6,\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            include_look_vectors: Include the look vector theta and phi files in the product package\n            include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n                along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n                `include_displacement_maps`, and will be removed in a future release.\n            include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n            looks: Number of looks to take in range and azimuth\n            include_dem: Include the digital elevation model GeoTIFF in the product package\n            include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n            phase_filter_parameter: Adaptive phase filter parameter.\n                Useful values fall in the range 0.2 to 1.\n                Larger values result in stronger filtering.\n                If zero, adaptive phase filter will be skipped.\n\n        Returns:\n            A Batch object containing the InSAR job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_job(\n        cls,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        include_look_vectors: bool = False,\n        include_los_displacement: bool = False,\n        include_inc_map: bool = False,\n        looks: Literal['20x4', '10x2'] = '20x4',\n        include_dem: bool = False,\n        include_wrapped_phase: bool = False,\n        apply_water_mask: bool = False,\n        include_displacement_maps: bool = False,\n        phase_filter_parameter: float = 0.6,\n    ) -&gt; dict:\n\"\"\"Submit an InSAR job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            include_look_vectors: Include the look vector theta and phi files in the product package\n            include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n                along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n                `include_displacement_maps`, and will be removed in a future release.\n            include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n            looks: Number of looks to take in range and azimuth\n            include_dem: Include the digital elevation model GeoTIFF in the product package\n            include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n            phase_filter_parameter: Adaptive phase filter parameter.\n                Useful values fall in the range 0.2 to 1.\n                Larger values result in stronger filtering.\n                If zero, adaptive phase filter will be skipped.\n\n        Returns:\n            A dictionary containing the prepared InSAR job\n        \"\"\"\n        if include_los_displacement:\n            warnings.warn(\n                'The include_los_displacement parameter has been deprecated in favor of '\n                'include_displacement_maps, and will be removed in a future release.',\n                FutureWarning,\n            )\n\n        job_parameters = locals().copy()\n        for key in ['cls', 'granule1', 'granule2', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n            'job_type': 'INSAR_GAMMA',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_isce_multi_burst_job(\n        self,\n        reference: list[str],\n        secondary: list[str],\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE multi burst job.\n\n        Args:\n            reference: A list of reference granules (scenes) to use\n            secondary: A list of secondary granules (scenes) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A Batch object containing the InSAR ISCE multi burst job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_isce_multi_burst_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_isce_multi_burst_job(\n        cls,\n        reference: list[str],\n        secondary: list[str],\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE multi burst job.\n\n        Args:\n            reference: A list of reference granules (scenes) to use\n            secondary: A list of secondary granules (scenes) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A dictionary containing the prepared InSAR ISCE multi burst job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {**job_parameters},\n            'job_type': 'INSAR_ISCE_MULTI_BURST',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_isce_burst_job(\n        self,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE burst job.\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A Batch object containing the InSAR ISCE burst job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_isce_burst_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_isce_burst_job(\n        cls,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE burst job.\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A dictionary containing the prepared InSAR ISCE burst job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'granule1', 'granule2', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n            'job_type': 'INSAR_ISCE_BURST',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_aria_s1_gunw_job(\n        self, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n    ) -&gt; Batch:\n\"\"\"Submit an ARIA S1 GUNW job.\n\n        Args:\n            reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n            secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n            frame_id: Subset GUNW products to this frame\n            name: A name for the job (optional)\n\n        Returns:\n            A Batch object containing the ARIA S1 GUNW job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_aria_s1_gunw_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_aria_s1_gunw_job(\n        cls, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n    ) -&gt; dict:\n\"\"\"Prepare an ARIA S1 GUNW job.\n\n        Args:\n            reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n            secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n            frame_id: Subset GUNW products to this frame\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared ARIA S1 GUNW job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': job_parameters,\n            'job_type': 'ARIA_S1_GUNW',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_opera_rtc_s1_job(self, granule: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an OPERA RTC-S1 job.\n\n        Args:\n            granule: The name of the S1 burst to use\n            name: A name for the job (optional)\n\n        Returns:\n            A Batch object containing the OPERA RTC-S1 job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_opera_rtc_s1_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_opera_rtc_s1_job(cls, granule: str, name: str | None = None) -&gt; dict:\n\"\"\"Prepare an OPERA RTC-S1 job.\n\n        Args:\n            granule: The name of the S1 burst to use\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared OPERA RTC-S1 job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name', 'granule']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule], **job_parameters},\n            'job_type': 'OPERA_RTC_S1',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def my_info(self) -&gt; dict:\n\"\"\"Returns:\n        Your user information\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url('/user'))\n        _raise_for_hyp3_status(response)\n        return response.json()\n\n    def check_credits(self) -&gt; float | int | None:\n\"\"\"Returns:\n        Your remaining processing credits, or None if you have no processing limit\n        \"\"\"\n        info = self.my_info()\n        return info['remaining_credits']\n\n    def check_quota(self) -&gt; float | int | None:\n\"\"\"Deprecated method for checking your remaining processing credits; replaced by `HyP3.check_credits`\n\n        Returns:\n            Your remaining processing credits, or None if you have no processing limit\n        \"\"\"\n        warn(\n            'This method is deprecated and will be removed in a future release.\\n'\n            'Please use `HyP3.check_credits` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.check_credits()\n\n    def costs(self) -&gt; dict:\n\"\"\"Returns:\n        Table of job costs\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url('/costs'))\n        _raise_for_hyp3_status(response)\n        return response.json()\n\n    def update_jobs(self, jobs: Batch | Job, **kwargs: object) -&gt; Batch | Job:\n\"\"\"Update the name of one or more previously-submitted jobs.\n\n        Args:\n            jobs: The job(s) to update\n            kwargs:\n                name: The new name, or None to remove the name\n\n        Returns:\n            The updated job(s)\n        \"\"\"\n        if isinstance(jobs, Batch):\n            batch = hyp3_sdk.Batch()\n            tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n            for job in tqdm(jobs):\n                batch += self.update_jobs(job, **kwargs)\n            return batch\n\n        if not isinstance(jobs, Job):\n            raise TypeError(f\"'jobs' has type {type(jobs)}, must be {Batch} or {Job}\")\n\n        response = self.session.patch(self._get_endpoint_url(f'/jobs/{jobs.job_id}'), json=kwargs)\n        _raise_for_hyp3_status(response)\n        return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.__init__","title":"<code>__init__(api_url=PROD_API, username=None, password=None, token=None, prompt=None)</code>","text":"<p>If username and password are not provided, attempts to use credentials from a <code>.netrc</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>Address of the HyP3 API</p> <code>PROD_API</code> <code>username</code> <code>str | None</code> <p>Username for authenticating to <code>urs.earthdata.nasa.gov</code>. Both username and password must be provided if either is provided.</p> <code>None</code> <code>password</code> <code>str | None</code> <p>Password for authenticating to <code>urs.earthdata.nasa.gov</code>. Both username and password must be provided if either is provided.</p> <code>None</code> <code>token</code> <code>str | None</code> <p>Earthdata Login Bearer Token for authenticating to <code>urs.earthdata.nasa.gov</code></p> <code>None</code> <code>prompt</code> <code>Literal['password', 'token'] | bool | None</code> <p>Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.</p> <code>None</code> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def __init__(\n    self,\n    api_url: str = PROD_API,\n    username: str | None = None,\n    password: str | None = None,\n    token: str | None = None,\n    prompt: Literal['password', 'token'] | bool | None = None,\n):\n\"\"\"If username and password are not provided, attempts to use credentials from a `.netrc` file.\n\n    Args:\n        api_url: Address of the HyP3 API\n        username: Username for authenticating to `urs.earthdata.nasa.gov`.\n            Both username and password must be provided if either is provided.\n        password: Password for authenticating to `urs.earthdata.nasa.gov`.\n            Both username and password must be provided if either is provided.\n        token: Earthdata Login Bearer Token for authenticating to `urs.earthdata.nasa.gov`\n        prompt: Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.\n    \"\"\"\n    self.url = api_url\n\n    if prompt not in (True, False, 'password', 'token', None):\n        raise ValueError(f'Unexpected value {prompt} for `prompt`')\n\n    if prompt is True:\n        warnings.warn(\n            'Passing `prompt=True` is deprecated. Please use either `prompt=\"password\"` or `prompt=\"token\"`',\n            UserWarning,\n        )\n        prompt = 'password'\n\n    if prompt == 'password':\n        if username is None:\n            username = input('NASA Earthdata Login username: ')\n        if password is None:\n            password = getpass('NASA Earthdata Login password: ')\n\n    if prompt == 'token' and token is None:\n        token = getpass('NASA Earthdata Login bearer token: ')\n\n    self.session = hyp3_sdk.util.get_authenticated_session(username, password, token)\n    self.session.headers.update({'User-Agent': f'{hyp3_sdk.__name__}/{hyp3_sdk.__version__}'})\n\n    hostname = urlsplit(self.url).hostname\n    assert hostname is not None\n    if not hostname.endswith('.asf.alaska.edu') and 'asf-urs' in self.session.cookies:\n        self.session.cookies.set('asf-urs', self.session.cookies['asf-urs'], domain=hostname)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.check_credits","title":"<code>check_credits()</code>","text":"<p>Returns: Your remaining processing credits, or None if you have no processing limit</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def check_credits(self) -&gt; float | int | None:\n\"\"\"Returns:\n    Your remaining processing credits, or None if you have no processing limit\n    \"\"\"\n    info = self.my_info()\n    return info['remaining_credits']\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.check_quota","title":"<code>check_quota()</code>","text":"<p>Deprecated method for checking your remaining processing credits; replaced by <code>HyP3.check_credits</code></p> <p>Returns:</p> Type Description <code>float | int | None</code> <p>Your remaining processing credits, or None if you have no processing limit</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def check_quota(self) -&gt; float | int | None:\n\"\"\"Deprecated method for checking your remaining processing credits; replaced by `HyP3.check_credits`\n\n    Returns:\n        Your remaining processing credits, or None if you have no processing limit\n    \"\"\"\n    warn(\n        'This method is deprecated and will be removed in a future release.\\n'\n        'Please use `HyP3.check_credits` instead.',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return self.check_credits()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.costs","title":"<code>costs()</code>","text":"<p>Returns: Table of job costs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def costs(self) -&gt; dict:\n\"\"\"Returns:\n    Table of job costs\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url('/costs'))\n    _raise_for_hyp3_status(response)\n    return response.json()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.find_jobs","title":"<code>find_jobs(start=None, end=None, status_code=None, name=None, job_type=None, user_id=None)</code>","text":"<p>Gets a Batch of jobs from HyP3 matching the provided search criteria</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime | None</code> <p>only jobs submitted after given time</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>only jobs submitted before given time</p> <code>None</code> <code>status_code</code> <code>str | None</code> <p>only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)</p> <code>None</code> <code>name</code> <code>str | None</code> <p>only jobs with this name</p> <code>None</code> <code>job_type</code> <code>str | None</code> <p>only jobs with this job_type</p> <code>None</code> <code>user_id</code> <code>str | None</code> <p>only jobs submitted by this user (defaults to the current user)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the found jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def find_jobs(\n    self,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    status_code: str | None = None,\n    name: str | None = None,\n    job_type: str | None = None,\n    user_id: str | None = None,\n) -&gt; Batch:\n\"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria\n\n    Args:\n        start: only jobs submitted after given time\n        end: only jobs submitted before given time\n        status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)\n        name: only jobs with this name\n        job_type: only jobs with this job_type\n        user_id: only jobs submitted by this user (defaults to the current user)\n\n    Returns:\n        A Batch object containing the found jobs\n    \"\"\"\n    params = {}\n    for param_name in ('start', 'end', 'status_code', 'name', 'job_type', 'user_id'):\n        param_value = locals().get(param_name)\n        if param_value is not None:\n            if isinstance(param_value, datetime):\n                if param_value.tzinfo is None:\n                    param_value = param_value.replace(tzinfo=timezone.utc)\n                param_value = param_value.isoformat(timespec='seconds')\n\n            params[param_name] = param_value\n\n    response = self.session.get(self._get_endpoint_url('/jobs'), params=params)\n    _raise_for_hyp3_status(response)\n    jobs = [Job.from_dict(job) for job in response.json()['jobs']]\n\n    while 'next' in response.json():\n        next_url = response.json()['next']\n        response = self.session.get(next_url)\n        _raise_for_hyp3_status(response)\n        jobs.extend([Job.from_dict(job) for job in response.json()['jobs']])\n\n    return Batch(jobs)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.get_job_by_id","title":"<code>get_job_by_id(job_id)</code>","text":"<p>Get job by job ID</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>A job ID</p> required <p>Returns:</p> Type Description <code>Job</code> <p>A Job object</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def get_job_by_id(self, job_id: str) -&gt; Job:\n\"\"\"Get job by job ID\n\n    Args:\n        job_id: A job ID\n\n    Returns:\n        A Job object\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url(f'/jobs/{job_id}'))\n    _raise_for_hyp3_status(response)\n\n    return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.my_info","title":"<code>my_info()</code>","text":"<p>Returns: Your user information</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def my_info(self) -&gt; dict:\n\"\"\"Returns:\n    Your user information\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url('/user'))\n    _raise_for_hyp3_status(response)\n    return response.json()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_aria_s1_gunw_job","title":"<code>prepare_aria_s1_gunw_job(reference_date, secondary_date, frame_id, name=None)</code>  <code>classmethod</code>","text":"<p>Prepare an ARIA S1 GUNW job.</p> <p>Parameters:</p> Name Type Description Default <code>reference_date</code> <code>str</code> <p>Date of reference scenes for InSAR processing in YYYY-MM-DD format</p> required <code>secondary_date</code> <code>str</code> <p>Date of secondary scenes for InSAR processing in YYYY-MM-DD format</p> required <code>frame_id</code> <code>int</code> <p>Subset GUNW products to this frame</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared ARIA S1 GUNW job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_aria_s1_gunw_job(\n    cls, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n) -&gt; dict:\n\"\"\"Prepare an ARIA S1 GUNW job.\n\n    Args:\n        reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n        secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n        frame_id: Subset GUNW products to this frame\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared ARIA S1 GUNW job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': job_parameters,\n        'job_type': 'ARIA_S1_GUNW',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_autorift_job","title":"<code>prepare_autorift_job(granule1, granule2, name=None)</code>  <code>classmethod</code>","text":"<p>Submit an autoRIFT job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared autoRIFT job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_autorift_job(cls, granule1: str, granule2: str, name: str | None = None) -&gt; dict:\n\"\"\"Submit an autoRIFT job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared autoRIFT job\n    \"\"\"\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2]},\n        'job_type': 'AUTORIFT',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_insar_isce_burst_job","title":"<code>prepare_insar_isce_burst_job(granule1, granule2, name=None, apply_water_mask=False, looks='20x4')</code>  <code>classmethod</code>","text":"<p>Prepare an InSAR ISCE burst job.</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR ISCE burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_isce_burst_job(\n    cls,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE burst job.\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A dictionary containing the prepared InSAR ISCE burst job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'granule1', 'granule2', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n        'job_type': 'INSAR_ISCE_BURST',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_insar_isce_multi_burst_job","title":"<code>prepare_insar_isce_multi_burst_job(reference, secondary, name=None, apply_water_mask=False, looks='20x4')</code>  <code>classmethod</code>","text":"<p>Prepare an InSAR ISCE multi burst job.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>list[str]</code> <p>A list of reference granules (scenes) to use</p> required <code>secondary</code> <code>list[str]</code> <p>A list of secondary granules (scenes) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR ISCE multi burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_isce_multi_burst_job(\n    cls,\n    reference: list[str],\n    secondary: list[str],\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE multi burst job.\n\n    Args:\n        reference: A list of reference granules (scenes) to use\n        secondary: A list of secondary granules (scenes) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A dictionary containing the prepared InSAR ISCE multi burst job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {**job_parameters},\n        'job_type': 'INSAR_ISCE_MULTI_BURST',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_insar_job","title":"<code>prepare_insar_job(granule1, granule2, name=None, include_look_vectors=False, include_los_displacement=False, include_inc_map=False, looks='20x4', include_dem=False, include_wrapped_phase=False, apply_water_mask=False, include_displacement_maps=False, phase_filter_parameter=0.6)</code>  <code>classmethod</code>","text":"<p>Submit an InSAR job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>include_look_vectors</code> <code>bool</code> <p>Include the look vector theta and phi files in the product package</p> <code>False</code> <code>include_los_displacement</code> <code>bool</code> <p>Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of <code>include_displacement_maps</code>, and will be removed in a future release.</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local and ellipsoidal incidence angle maps in the product package</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <code>include_dem</code> <code>bool</code> <p>Include the digital elevation model GeoTIFF in the product package</p> <code>False</code> <code>include_wrapped_phase</code> <code>bool</code> <p>Include the wrapped phase GeoTIFF in the product package</p> <code>False</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>include_displacement_maps</code> <code>bool</code> <p>Include displacement maps (line-of-sight and vertical) in the product package</p> <code>False</code> <code>phase_filter_parameter</code> <code>float</code> <p>Adaptive phase filter parameter. Useful values fall in the range 0.2 to 1. Larger values result in stronger filtering. If zero, adaptive phase filter will be skipped.</p> <code>0.6</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_job(\n    cls,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    include_look_vectors: bool = False,\n    include_los_displacement: bool = False,\n    include_inc_map: bool = False,\n    looks: Literal['20x4', '10x2'] = '20x4',\n    include_dem: bool = False,\n    include_wrapped_phase: bool = False,\n    apply_water_mask: bool = False,\n    include_displacement_maps: bool = False,\n    phase_filter_parameter: float = 0.6,\n) -&gt; dict:\n\"\"\"Submit an InSAR job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        include_look_vectors: Include the look vector theta and phi files in the product package\n        include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n            along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n            `include_displacement_maps`, and will be removed in a future release.\n        include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n        looks: Number of looks to take in range and azimuth\n        include_dem: Include the digital elevation model GeoTIFF in the product package\n        include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n        phase_filter_parameter: Adaptive phase filter parameter.\n            Useful values fall in the range 0.2 to 1.\n            Larger values result in stronger filtering.\n            If zero, adaptive phase filter will be skipped.\n\n    Returns:\n        A dictionary containing the prepared InSAR job\n    \"\"\"\n    if include_los_displacement:\n        warnings.warn(\n            'The include_los_displacement parameter has been deprecated in favor of '\n            'include_displacement_maps, and will be removed in a future release.',\n            FutureWarning,\n        )\n\n    job_parameters = locals().copy()\n    for key in ['cls', 'granule1', 'granule2', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n        'job_type': 'INSAR_GAMMA',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_opera_rtc_s1_job","title":"<code>prepare_opera_rtc_s1_job(granule, name=None)</code>  <code>classmethod</code>","text":"<p>Prepare an OPERA RTC-S1 job.</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The name of the S1 burst to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared OPERA RTC-S1 job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_opera_rtc_s1_job(cls, granule: str, name: str | None = None) -&gt; dict:\n\"\"\"Prepare an OPERA RTC-S1 job.\n\n    Args:\n        granule: The name of the S1 burst to use\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared OPERA RTC-S1 job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name', 'granule']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule], **job_parameters},\n        'job_type': 'OPERA_RTC_S1',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.prepare_rtc_job","title":"<code>prepare_rtc_job(granule, name=None, dem_matching=False, include_dem=False, include_inc_map=False, include_rgb=False, include_scattering_area=False, radiometry='gamma0', resolution=30, scale='power', speckle_filter=False, dem_name='copernicus')</code>  <code>classmethod</code>","text":"<p>Submit an RTC job</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>dem_matching</code> <code>bool</code> <p>Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files</p> <code>False</code> <code>include_dem</code> <code>bool</code> <p>Include the DEM file in the product package</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local incidence angle map in the product package</p> <code>False</code> <code>include_rgb</code> <code>bool</code> <p>Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules)</p> <code>False</code> <code>include_scattering_area</code> <code>bool</code> <p>Include the scattering area in the product package</p> <code>False</code> <code>radiometry</code> <code>Literal['sigma0', 'gamma0']</code> <p>Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0)</p> <code>'gamma0'</code> <code>resolution</code> <code>Literal[10, 20, 30]</code> <p>Desired output pixel spacing in meters</p> <code>30</code> <code>scale</code> <code>Literal['amplitude', 'decibel', 'power']</code> <p>Scale of output image; power, decibel or amplitude</p> <code>'power'</code> <code>speckle_filter</code> <code>bool</code> <p>Apply an Enhanced Lee speckle filter</p> <code>False</code> <code>dem_name</code> <code>Literal['copernicus']</code> <p>Name of the DEM to use for processing. <code>copernicus</code> is the only option, and it will use</p> <code>'copernicus'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared RTC job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_rtc_job(\n    cls,\n    granule: str,\n    name: str | None = None,\n    dem_matching: bool = False,\n    include_dem: bool = False,\n    include_inc_map: bool = False,\n    include_rgb: bool = False,\n    include_scattering_area: bool = False,\n    radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n    resolution: Literal[10, 20, 30] = 30,\n    scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n    speckle_filter: bool = False,\n    dem_name: Literal['copernicus'] = 'copernicus',\n) -&gt; dict:\n\"\"\"Submit an RTC job\n\n    Args:\n        granule: The granule (scene) to use\n        name: A name for the job\n        dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n        include_dem: Include the DEM file in the product package\n        include_inc_map: Include the local incidence angle map in the product package\n        include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n            (ignored for single-pol granules)\n        include_scattering_area: Include the scattering area in the product package\n        radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n            projected into the look direction (gamma0)\n        resolution: Desired output pixel spacing in meters\n        scale: Scale of output image; power, decibel or amplitude\n        speckle_filter: Apply an Enhanced Lee speckle filter\n        dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n        the Copernicus GLO-30 Public DEM.\n\n    Returns:\n        A dictionary containing the prepared RTC job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['granule', 'name', 'cls']:\n        job_parameters.pop(key, None)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule], **job_parameters},\n        'job_type': 'RTC_GAMMA',\n    }\n\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.refresh","title":"<code>refresh(job_or_batch)</code>","text":"<p>Refresh each jobs' information</p> <p>Parameters:</p> Name Type Description Default <code>job_or_batch</code> <code>Batch | Job</code> <p>A Batch of Job object to refresh</p> required <p>Returns:</p> Type Description <code>Batch | Job</code> <p>A Batch or Job object with refreshed information</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@singledispatchmethod\ndef refresh(self, job_or_batch: Batch | Job) -&gt; Batch | Job:\n\"\"\"Refresh each jobs' information\n\n    Args:\n        job_or_batch: A Batch of Job object to refresh\n\n    Returns:\n        A Batch or Job object with refreshed information\n    \"\"\"\n    raise NotImplementedError(f'Cannot refresh {type(job_or_batch)} type object')\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_aria_s1_gunw_job","title":"<code>submit_aria_s1_gunw_job(reference_date, secondary_date, frame_id, name=None)</code>","text":"<p>Submit an ARIA S1 GUNW job.</p> <p>Parameters:</p> Name Type Description Default <code>reference_date</code> <code>str</code> <p>Date of reference scenes for InSAR processing in YYYY-MM-DD format</p> required <code>secondary_date</code> <code>str</code> <p>Date of secondary scenes for InSAR processing in YYYY-MM-DD format</p> required <code>frame_id</code> <code>int</code> <p>Subset GUNW products to this frame</p> required <code>name</code> <code>str | None</code> <p>A name for the job (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the ARIA S1 GUNW job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_aria_s1_gunw_job(\n    self, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n) -&gt; Batch:\n\"\"\"Submit an ARIA S1 GUNW job.\n\n    Args:\n        reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n        secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n        frame_id: Subset GUNW products to this frame\n        name: A name for the job (optional)\n\n    Returns:\n        A Batch object containing the ARIA S1 GUNW job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_aria_s1_gunw_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_autorift_job","title":"<code>submit_autorift_job(granule1, granule2, name=None)</code>","text":"<p>Submit an autoRIFT job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the autoRIFT job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_autorift_job(self, granule1: str, granule2: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an autoRIFT job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n\n    Returns:\n        A Batch object containing the autoRIFT job\n    \"\"\"\n    job_dict = self.prepare_autorift_job(granule1, granule2, name=name)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_insar_isce_burst_job","title":"<code>submit_insar_isce_burst_job(granule1, granule2, name=None, apply_water_mask=False, looks='20x4')</code>","text":"<p>Submit an InSAR ISCE burst job.</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR ISCE burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_isce_burst_job(\n    self,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE burst job.\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A Batch object containing the InSAR ISCE burst job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_isce_burst_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_insar_isce_multi_burst_job","title":"<code>submit_insar_isce_multi_burst_job(reference, secondary, name=None, apply_water_mask=False, looks='20x4')</code>","text":"<p>Submit an InSAR ISCE multi burst job.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>list[str]</code> <p>A list of reference granules (scenes) to use</p> required <code>secondary</code> <code>list[str]</code> <p>A list of secondary granules (scenes) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR ISCE multi burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_isce_multi_burst_job(\n    self,\n    reference: list[str],\n    secondary: list[str],\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE multi burst job.\n\n    Args:\n        reference: A list of reference granules (scenes) to use\n        secondary: A list of secondary granules (scenes) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A Batch object containing the InSAR ISCE multi burst job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_isce_multi_burst_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_insar_job","title":"<code>submit_insar_job(granule1, granule2, name=None, include_look_vectors=False, include_los_displacement=False, include_inc_map=False, looks='20x4', include_dem=False, include_wrapped_phase=False, apply_water_mask=False, include_displacement_maps=False, phase_filter_parameter=0.6)</code>","text":"<p>Submit an InSAR job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>include_look_vectors</code> <code>bool</code> <p>Include the look vector theta and phi files in the product package</p> <code>False</code> <code>include_los_displacement</code> <code>bool</code> <p>Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of <code>include_displacement_maps</code>, and will be removed in a future release.</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local and ellipsoidal incidence angle maps in the product package</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <code>include_dem</code> <code>bool</code> <p>Include the digital elevation model GeoTIFF in the product package</p> <code>False</code> <code>include_wrapped_phase</code> <code>bool</code> <p>Include the wrapped phase GeoTIFF in the product package</p> <code>False</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>include_displacement_maps</code> <code>bool</code> <p>Include displacement maps (line-of-sight and vertical) in the product package</p> <code>False</code> <code>phase_filter_parameter</code> <code>float</code> <p>Adaptive phase filter parameter. Useful values fall in the range 0.2 to 1. Larger values result in stronger filtering. If zero, adaptive phase filter will be skipped.</p> <code>0.6</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_job(\n    self,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    include_look_vectors: bool = False,\n    include_los_displacement: bool = False,\n    include_inc_map: bool = False,\n    looks: Literal['20x4', '10x2'] = '20x4',\n    include_dem: bool = False,\n    include_wrapped_phase: bool = False,\n    apply_water_mask: bool = False,\n    include_displacement_maps: bool = False,\n    phase_filter_parameter: float = 0.6,\n) -&gt; Batch:\n\"\"\"Submit an InSAR job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        include_look_vectors: Include the look vector theta and phi files in the product package\n        include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n            along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n            `include_displacement_maps`, and will be removed in a future release.\n        include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n        looks: Number of looks to take in range and azimuth\n        include_dem: Include the digital elevation model GeoTIFF in the product package\n        include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n        phase_filter_parameter: Adaptive phase filter parameter.\n            Useful values fall in the range 0.2 to 1.\n            Larger values result in stronger filtering.\n            If zero, adaptive phase filter will be skipped.\n\n    Returns:\n        A Batch object containing the InSAR job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_opera_rtc_s1_job","title":"<code>submit_opera_rtc_s1_job(granule, name=None)</code>","text":"<p>Submit an OPERA RTC-S1 job.</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The name of the S1 burst to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the OPERA RTC-S1 job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_opera_rtc_s1_job(self, granule: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an OPERA RTC-S1 job.\n\n    Args:\n        granule: The name of the S1 burst to use\n        name: A name for the job (optional)\n\n    Returns:\n        A Batch object containing the OPERA RTC-S1 job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_opera_rtc_s1_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_prepared_jobs","title":"<code>submit_prepared_jobs(prepared_jobs)</code>","text":"<p>Submit a prepared job dictionary, or list of prepared job dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>prepared_jobs</code> <code>dict | list[dict]</code> <p>A prepared job dictionary, or list of prepared job dictionaries</p> required <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the submitted job(s)</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_prepared_jobs(self, prepared_jobs: dict | list[dict]) -&gt; Batch:\n\"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries\n\n    Args:\n        prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries\n\n    Returns:\n        A Batch object containing the submitted job(s)\n    \"\"\"\n    if isinstance(prepared_jobs, dict):\n        payload = {'jobs': [prepared_jobs]}\n    else:\n        payload = {'jobs': prepared_jobs}\n\n    response = self.session.post(self._get_endpoint_url('/jobs'), json=payload)\n    _raise_for_hyp3_status(response)\n\n    batch = Batch()\n    for job in response.json()['jobs']:\n        batch += Job.from_dict(job)\n    return batch\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.submit_rtc_job","title":"<code>submit_rtc_job(granule, name=None, dem_matching=False, include_dem=False, include_inc_map=False, include_rgb=False, include_scattering_area=False, radiometry='gamma0', resolution=30, scale='power', speckle_filter=False, dem_name='copernicus')</code>","text":"<p>Submit an RTC job</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>dem_matching</code> <code>bool</code> <p>Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files</p> <code>False</code> <code>include_dem</code> <code>bool</code> <p>Include the DEM file in the product package</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local incidence angle map in the product package</p> <code>False</code> <code>include_rgb</code> <code>bool</code> <p>Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules)</p> <code>False</code> <code>include_scattering_area</code> <code>bool</code> <p>Include the scattering area in the product package</p> <code>False</code> <code>radiometry</code> <code>Literal['sigma0', 'gamma0']</code> <p>Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0)</p> <code>'gamma0'</code> <code>resolution</code> <code>Literal[10, 20, 30]</code> <p>Desired output pixel spacing in meters</p> <code>30</code> <code>scale</code> <code>Literal['amplitude', 'decibel', 'power']</code> <p>Scale of output image; power, decibel or amplitude</p> <code>'power'</code> <code>speckle_filter</code> <code>bool</code> <p>Apply an Enhanced Lee speckle filter</p> <code>False</code> <code>dem_name</code> <code>Literal['copernicus']</code> <p>Name of the DEM to use for processing. <code>copernicus</code> is the only option, and it will use</p> <code>'copernicus'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the RTC job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_rtc_job(\n    self,\n    granule: str,\n    name: str | None = None,\n    dem_matching: bool = False,\n    include_dem: bool = False,\n    include_inc_map: bool = False,\n    include_rgb: bool = False,\n    include_scattering_area: bool = False,\n    radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n    resolution: Literal[10, 20, 30] = 30,\n    scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n    speckle_filter: bool = False,\n    dem_name: Literal['copernicus'] = 'copernicus',\n) -&gt; Batch:\n\"\"\"Submit an RTC job\n\n    Args:\n        granule: The granule (scene) to use\n        name: A name for the job\n        dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n        include_dem: Include the DEM file in the product package\n        include_inc_map: Include the local incidence angle map in the product package\n        include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n            (ignored for single-pol granules)\n        include_scattering_area: Include the scattering area in the product package\n        radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n            projected into the look direction (gamma0)\n        resolution: Desired output pixel spacing in meters\n        scale: Scale of output image; power, decibel or amplitude\n        speckle_filter: Apply an Enhanced Lee speckle filter\n        dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n        the Copernicus GLO-30 Public DEM.\n\n    Returns:\n        A Batch object containing the RTC job\n    \"\"\"\n    arguments = locals()\n    arguments.pop('self')\n    job_dict = self.prepare_rtc_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.update_jobs","title":"<code>update_jobs(jobs, **kwargs)</code>","text":"<p>Update the name of one or more previously-submitted jobs.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Batch | Job</code> <p>The job(s) to update</p> required <code>kwargs</code> <code>object</code> <p>name: The new name, or None to remove the name</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch | Job</code> <p>The updated job(s)</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def update_jobs(self, jobs: Batch | Job, **kwargs: object) -&gt; Batch | Job:\n\"\"\"Update the name of one or more previously-submitted jobs.\n\n    Args:\n        jobs: The job(s) to update\n        kwargs:\n            name: The new name, or None to remove the name\n\n    Returns:\n        The updated job(s)\n    \"\"\"\n    if isinstance(jobs, Batch):\n        batch = hyp3_sdk.Batch()\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        for job in tqdm(jobs):\n            batch += self.update_jobs(job, **kwargs)\n        return batch\n\n    if not isinstance(jobs, Job):\n        raise TypeError(f\"'jobs' has type {type(jobs)}, must be {Batch} or {Job}\")\n\n    response = self.session.patch(self._get_endpoint_url(f'/jobs/{jobs.job_id}'), json=kwargs)\n    _raise_for_hyp3_status(response)\n    return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.HyP3.watch","title":"<code>watch(job_or_batch, timeout=10800, interval=60)</code>","text":"<p>Watch jobs until they complete</p> <p>Parameters:</p> Name Type Description Default <code>job_or_batch</code> <code>Batch | Job</code> <p>A Batch or Job object of jobs to watch</p> required <code>timeout</code> <code>int</code> <p>How long to wait until exiting in seconds</p> <code>10800</code> <code>interval</code> <code>int | float</code> <p>How often to check for updates in seconds</p> <code>60</code> <p>Returns:</p> Type Description <code>Batch | Job</code> <p>A Batch or Job object with refreshed watched jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@singledispatchmethod\ndef watch(self, job_or_batch: Batch | Job, timeout: int = 10800, interval: int | float = 60) -&gt; Batch | Job:\n\"\"\"Watch jobs until they complete\n\n    Args:\n        job_or_batch: A Batch or Job object of jobs to watch\n        timeout: How long to wait until exiting in seconds\n        interval: How often to check for updates in seconds\n\n    Returns:\n        A Batch or Job object with refreshed watched jobs\n    \"\"\"\n    raise NotImplementedError(f'Cannot watch {type(job_or_batch)} type object')\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Job","title":"<code>Job</code>","text":"Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>class Job:\n    _attributes_for_resubmit = {'name', 'job_parameters', 'job_type'}\n\n    def __init__(\n        self,\n        job_type: str,\n        job_id: str,\n        request_time: datetime,\n        status_code: str,\n        user_id: str,\n        name: str | None = None,\n        job_parameters: dict | None = None,\n        files: list | None = None,\n        logs: list | None = None,\n        browse_images: list | None = None,\n        thumbnail_images: list | None = None,\n        expiration_time: datetime | None = None,\n        processing_times: list[float] | None = None,\n        credit_cost: float | None = None,\n        priority: int | None = None,\n    ):\n        self.job_id = job_id\n        self.job_type = job_type\n        self.request_time = request_time\n        self.status_code = status_code\n        self.user_id = user_id\n        self.name = name\n        self.job_parameters = job_parameters\n        self.files = files\n        self.logs = logs\n        self.browse_images = browse_images\n        self.thumbnail_images = thumbnail_images\n        self.expiration_time = expiration_time\n        self.processing_times = processing_times\n        self.credit_cost = credit_cost\n        self.priority = priority\n\n    def __repr__(self):\n        return f'Job.from_dict({self.to_dict()})'\n\n    def __str__(self):\n        return f'HyP3 {self.job_type} job {self.job_id}'\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    @staticmethod\n    def from_dict(input_dict: dict):\n        expiration_time = parse_date(input_dict['expiration_time']) if input_dict.get('expiration_time') else None\n        return Job(\n            job_type=input_dict['job_type'],\n            job_id=input_dict['job_id'],\n            request_time=parse_date(input_dict['request_time']),\n            status_code=input_dict['status_code'],\n            user_id=input_dict['user_id'],\n            name=input_dict.get('name'),\n            job_parameters=input_dict.get('job_parameters'),\n            files=input_dict.get('files'),\n            logs=input_dict.get('logs'),\n            browse_images=input_dict.get('browse_images'),\n            thumbnail_images=input_dict.get('thumbnail_images'),\n            expiration_time=expiration_time,\n            processing_times=input_dict.get('processing_times'),\n            credit_cost=input_dict.get('credit_cost'),\n            priority=input_dict.get('priority'),\n        )\n\n    def to_dict(self, for_resubmit: bool = False):\n        job_dict = {}\n        if for_resubmit:\n            keys_to_process = Job._attributes_for_resubmit\n        else:\n            keys_to_process = set(vars(self).keys())\n\n        for key in keys_to_process:\n            value = self.__getattribute__(key)\n            if value is not None:\n                if isinstance(value, datetime):\n                    job_dict[key] = value.isoformat(timespec='seconds')\n                else:\n                    job_dict[key] = value\n\n        return job_dict\n\n    def succeeded(self) -&gt; bool:\n        return self.status_code == 'SUCCEEDED'\n\n    def failed(self) -&gt; bool:\n        return self.status_code == 'FAILED'\n\n    def complete(self) -&gt; bool:\n        return self.succeeded() or self.failed()\n\n    def pending(self) -&gt; bool:\n        return self.status_code == 'PENDING'\n\n    def running(self) -&gt; bool:\n        return self.status_code == 'RUNNING'\n\n    def expired(self) -&gt; bool:\n        return self.expiration_time is not None and datetime.now(tz.UTC) &gt;= self.expiration_time\n\n    def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n            location: Directory location to put files into\n            create: Create `location` if it does not point to an existing directory\n\n        Returns: list of Path objects to downloaded files\n        \"\"\"\n        location = Path(location)\n\n        if not self.succeeded():\n            raise HyP3SDKError(f'Only succeeded jobs can be downloaded; job is {self.status_code}.')\n        if self.expired():\n            assert self.expiration_time is not None\n            raise HyP3SDKError(\n                f'Expired jobs cannot be downloaded; job expired {self.expiration_time.isoformat(timespec=\"seconds\")}.'\n            )\n\n        if create:\n            location.mkdir(parents=True, exist_ok=True)\n        elif not location.is_dir():\n            raise NotADirectoryError(str(location))\n\n        assert self.files is not None\n\n        downloaded_files = []\n        for file in self.files:\n            download_url = file['url']\n            filename = location / file['filename']\n            try:\n                downloaded_files.append(download_file(download_url, filename, chunk_size=10485760))\n            except HTTPError:\n                raise HyP3SDKError(f'Unable to download file: {download_url}')\n        return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.Job.download_files","title":"<code>download_files(location='.', create=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>location</code> <code>Path | str</code> <p>Directory location to put files into</p> <code>'.'</code> <code>create</code> <code>bool</code> <p>Create <code>location</code> if it does not point to an existing directory</p> <code>True</code> <p>Returns: list of Path objects to downloaded files</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n        location: Directory location to put files into\n        create: Create `location` if it does not point to an existing directory\n\n    Returns: list of Path objects to downloaded files\n    \"\"\"\n    location = Path(location)\n\n    if not self.succeeded():\n        raise HyP3SDKError(f'Only succeeded jobs can be downloaded; job is {self.status_code}.')\n    if self.expired():\n        assert self.expiration_time is not None\n        raise HyP3SDKError(\n            f'Expired jobs cannot be downloaded; job expired {self.expiration_time.isoformat(timespec=\"seconds\")}.'\n        )\n\n    if create:\n        location.mkdir(parents=True, exist_ok=True)\n    elif not location.is_dir():\n        raise NotADirectoryError(str(location))\n\n    assert self.files is not None\n\n    downloaded_files = []\n    for file in self.files:\n        download_url = file['url']\n        filename = location / file['filename']\n        try:\n            downloaded_files.append(download_file(download_url, filename, chunk_size=10485760))\n        except HTTPError:\n            raise HyP3SDKError(f'Unable to download file: {download_url}')\n    return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions","title":"<code>exceptions</code>","text":"<p>Errors and exceptions to raise when the SDK runs into problems</p>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.ASFSearchError","title":"<code>ASFSearchError</code>","text":"<p>               Bases: <code>HyP3SDKError</code></p> <p>Raise for errors when using the ASF Search module</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class ASFSearchError(HyP3SDKError):\n\"\"\"Raise for errors when using the ASF Search module\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>               Bases: <code>HyP3SDKError</code></p> <p>Raise when authentication does not succeed</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class AuthenticationError(HyP3SDKError):\n\"\"\"Raise when authentication does not succeed\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.HyP3Error","title":"<code>HyP3Error</code>","text":"<p>               Bases: <code>HyP3SDKError</code></p> <p>Raise for errors when using the HyP3 module</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class HyP3Error(HyP3SDKError):\n\"\"\"Raise for errors when using the HyP3 module\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.HyP3SDKError","title":"<code>HyP3SDKError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base Exception for the HyP3 SDK</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class HyP3SDKError(Exception):\n\"\"\"Base Exception for the HyP3 SDK\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.ServerError","title":"<code>ServerError</code>","text":"<p>               Bases: <code>HyP3SDKError</code></p> <p>Raise when the HyP3 SDK encounters a server error</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class ServerError(HyP3SDKError):\n\"\"\"Raise when the HyP3 SDK encounters a server error\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.exceptions.ServiceUnavailableError","title":"<code>ServiceUnavailableError</code>","text":"<p>               Bases: <code>HyP3SDKError</code></p> <p>Raise when the HyP3 API is unavailable</p> Source code in <code>hyp3_sdk/exceptions.py</code> <pre><code>class ServiceUnavailableError(HyP3SDKError):\n\"\"\"Raise when the HyP3 API is unavailable\"\"\"\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3","title":"<code>hyp3</code>","text":""},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3","title":"<code>HyP3</code>","text":"<p>A python wrapper around the HyP3 API.</p> <p>Warning: All jobs submitted to HyP3 are publicly visible. For more information, see https://hyp3-docs.asf.alaska.edu/#public-visibility-of-jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>class HyP3:\n\"\"\"A python wrapper around the HyP3 API.\n\n    Warning: All jobs submitted to HyP3 are publicly visible. For more information, see\n    https://hyp3-docs.asf.alaska.edu/#public-visibility-of-jobs\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str = PROD_API,\n        username: str | None = None,\n        password: str | None = None,\n        token: str | None = None,\n        prompt: Literal['password', 'token'] | bool | None = None,\n    ):\n\"\"\"If username and password are not provided, attempts to use credentials from a `.netrc` file.\n\n        Args:\n            api_url: Address of the HyP3 API\n            username: Username for authenticating to `urs.earthdata.nasa.gov`.\n                Both username and password must be provided if either is provided.\n            password: Password for authenticating to `urs.earthdata.nasa.gov`.\n                Both username and password must be provided if either is provided.\n            token: Earthdata Login Bearer Token for authenticating to `urs.earthdata.nasa.gov`\n            prompt: Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.\n        \"\"\"\n        self.url = api_url\n\n        if prompt not in (True, False, 'password', 'token', None):\n            raise ValueError(f'Unexpected value {prompt} for `prompt`')\n\n        if prompt is True:\n            warnings.warn(\n                'Passing `prompt=True` is deprecated. Please use either `prompt=\"password\"` or `prompt=\"token\"`',\n                UserWarning,\n            )\n            prompt = 'password'\n\n        if prompt == 'password':\n            if username is None:\n                username = input('NASA Earthdata Login username: ')\n            if password is None:\n                password = getpass('NASA Earthdata Login password: ')\n\n        if prompt == 'token' and token is None:\n            token = getpass('NASA Earthdata Login bearer token: ')\n\n        self.session = hyp3_sdk.util.get_authenticated_session(username, password, token)\n        self.session.headers.update({'User-Agent': f'{hyp3_sdk.__name__}/{hyp3_sdk.__version__}'})\n\n        hostname = urlsplit(self.url).hostname\n        assert hostname is not None\n        if not hostname.endswith('.asf.alaska.edu') and 'asf-urs' in self.session.cookies:\n            self.session.cookies.set('asf-urs', self.session.cookies['asf-urs'], domain=hostname)\n\n    def _get_endpoint_url(self, endpoint: str) -&gt; str:\n        parts = urlsplit(self.url)\n        path = '/'.join([parts.path.strip('/'), endpoint.strip('/')])\n        return urlunsplit(SplitResult(scheme=parts.scheme, netloc=parts.netloc, path=path, query='', fragment=''))\n\n    def find_jobs(\n        self,\n        start: datetime | None = None,\n        end: datetime | None = None,\n        status_code: str | None = None,\n        name: str | None = None,\n        job_type: str | None = None,\n        user_id: str | None = None,\n    ) -&gt; Batch:\n\"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria\n\n        Args:\n            start: only jobs submitted after given time\n            end: only jobs submitted before given time\n            status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)\n            name: only jobs with this name\n            job_type: only jobs with this job_type\n            user_id: only jobs submitted by this user (defaults to the current user)\n\n        Returns:\n            A Batch object containing the found jobs\n        \"\"\"\n        params = {}\n        for param_name in ('start', 'end', 'status_code', 'name', 'job_type', 'user_id'):\n            param_value = locals().get(param_name)\n            if param_value is not None:\n                if isinstance(param_value, datetime):\n                    if param_value.tzinfo is None:\n                        param_value = param_value.replace(tzinfo=timezone.utc)\n                    param_value = param_value.isoformat(timespec='seconds')\n\n                params[param_name] = param_value\n\n        response = self.session.get(self._get_endpoint_url('/jobs'), params=params)\n        _raise_for_hyp3_status(response)\n        jobs = [Job.from_dict(job) for job in response.json()['jobs']]\n\n        while 'next' in response.json():\n            next_url = response.json()['next']\n            response = self.session.get(next_url)\n            _raise_for_hyp3_status(response)\n            jobs.extend([Job.from_dict(job) for job in response.json()['jobs']])\n\n        return Batch(jobs)\n\n    def get_job_by_id(self, job_id: str) -&gt; Job:\n\"\"\"Get job by job ID\n\n        Args:\n            job_id: A job ID\n\n        Returns:\n            A Job object\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url(f'/jobs/{job_id}'))\n        _raise_for_hyp3_status(response)\n\n        return Job.from_dict(response.json())\n\n    @singledispatchmethod\n    def watch(self, job_or_batch: Batch | Job, timeout: int = 10800, interval: int | float = 60) -&gt; Batch | Job:\n\"\"\"Watch jobs until they complete\n\n        Args:\n            job_or_batch: A Batch or Job object of jobs to watch\n            timeout: How long to wait until exiting in seconds\n            interval: How often to check for updates in seconds\n\n        Returns:\n            A Batch or Job object with refreshed watched jobs\n        \"\"\"\n        raise NotImplementedError(f'Cannot watch {type(job_or_batch)} type object')\n\n    @watch.register\n    def _watch_batch(self, batch: Batch, timeout: int = 10800, interval: int | float = 60) -&gt; Batch:\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        iterations_until_timeout = math.ceil(timeout / interval)\n        bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{postfix[0]}]'\n        with tqdm(total=len(batch), bar_format=bar_format, postfix=[f'timeout in {timeout} s']) as progress_bar:\n            for ii in range(iterations_until_timeout):\n                batch = self.refresh(batch)  # type: ignore [assignment]\n\n                counts = batch._count_statuses()\n                complete = counts['SUCCEEDED'] + counts['FAILED']\n\n                progress_bar.postfix = [f'timeout in {timeout - ii * interval}s']\n                # to control n/total manually; update is n += value\n                progress_bar.n = complete\n                progress_bar.update(0)\n\n                if batch.complete():\n                    return batch\n                time.sleep(interval)\n        raise HyP3Error(f'Timeout occurred while waiting for {batch}')\n\n    @watch.register\n    def _watch_job(self, job: Job, timeout: int = 10800, interval: int | float = 60) -&gt; Job:\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        iterations_until_timeout = math.ceil(timeout / interval)\n        bar_format = '{n_fmt}/{total_fmt} [{postfix[0]}]'\n        with tqdm(total=1, bar_format=bar_format, postfix=[f'timeout in {timeout} s']) as progress_bar:\n            for ii in range(iterations_until_timeout):\n                job = self.refresh(job)  # type: ignore [assignment]\n                progress_bar.postfix = [f'timeout in {timeout - ii * interval}s']\n                progress_bar.update(int(job.complete()))\n\n                if job.complete():\n                    return job\n                time.sleep(interval)\n        raise HyP3Error(f'Timeout occurred while waiting for {job}')\n\n    @singledispatchmethod\n    def refresh(self, job_or_batch: Batch | Job) -&gt; Batch | Job:\n\"\"\"Refresh each jobs' information\n\n        Args:\n            job_or_batch: A Batch of Job object to refresh\n\n        Returns:\n            A Batch or Job object with refreshed information\n        \"\"\"\n        raise NotImplementedError(f'Cannot refresh {type(job_or_batch)} type object')\n\n    @refresh.register\n    def _refresh_batch(self, batch: Batch):\n        jobs = []\n        for job in batch.jobs:\n            jobs.append(self.refresh(job))\n        return Batch(jobs)  # type: ignore [arg-type]\n\n    @refresh.register\n    def _refresh_job(self, job: Job):\n        return self.get_job_by_id(job.job_id)\n\n    def submit_prepared_jobs(self, prepared_jobs: dict | list[dict]) -&gt; Batch:\n\"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries\n\n        Args:\n            prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries\n\n        Returns:\n            A Batch object containing the submitted job(s)\n        \"\"\"\n        if isinstance(prepared_jobs, dict):\n            payload = {'jobs': [prepared_jobs]}\n        else:\n            payload = {'jobs': prepared_jobs}\n\n        response = self.session.post(self._get_endpoint_url('/jobs'), json=payload)\n        _raise_for_hyp3_status(response)\n\n        batch = Batch()\n        for job in response.json()['jobs']:\n            batch += Job.from_dict(job)\n        return batch\n\n    def submit_autorift_job(self, granule1: str, granule2: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an autoRIFT job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n\n        Returns:\n            A Batch object containing the autoRIFT job\n        \"\"\"\n        job_dict = self.prepare_autorift_job(granule1, granule2, name=name)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_autorift_job(cls, granule1: str, granule2: str, name: str | None = None) -&gt; dict:\n\"\"\"Submit an autoRIFT job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared autoRIFT job\n        \"\"\"\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2]},\n            'job_type': 'AUTORIFT',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_rtc_job(\n        self,\n        granule: str,\n        name: str | None = None,\n        dem_matching: bool = False,\n        include_dem: bool = False,\n        include_inc_map: bool = False,\n        include_rgb: bool = False,\n        include_scattering_area: bool = False,\n        radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n        resolution: Literal[10, 20, 30] = 30,\n        scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n        speckle_filter: bool = False,\n        dem_name: Literal['copernicus'] = 'copernicus',\n    ) -&gt; Batch:\n\"\"\"Submit an RTC job\n\n        Args:\n            granule: The granule (scene) to use\n            name: A name for the job\n            dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n            include_dem: Include the DEM file in the product package\n            include_inc_map: Include the local incidence angle map in the product package\n            include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n                (ignored for single-pol granules)\n            include_scattering_area: Include the scattering area in the product package\n            radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n                projected into the look direction (gamma0)\n            resolution: Desired output pixel spacing in meters\n            scale: Scale of output image; power, decibel or amplitude\n            speckle_filter: Apply an Enhanced Lee speckle filter\n            dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n            the Copernicus GLO-30 Public DEM.\n\n        Returns:\n            A Batch object containing the RTC job\n        \"\"\"\n        arguments = locals()\n        arguments.pop('self')\n        job_dict = self.prepare_rtc_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_rtc_job(\n        cls,\n        granule: str,\n        name: str | None = None,\n        dem_matching: bool = False,\n        include_dem: bool = False,\n        include_inc_map: bool = False,\n        include_rgb: bool = False,\n        include_scattering_area: bool = False,\n        radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n        resolution: Literal[10, 20, 30] = 30,\n        scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n        speckle_filter: bool = False,\n        dem_name: Literal['copernicus'] = 'copernicus',\n    ) -&gt; dict:\n\"\"\"Submit an RTC job\n\n        Args:\n            granule: The granule (scene) to use\n            name: A name for the job\n            dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n            include_dem: Include the DEM file in the product package\n            include_inc_map: Include the local incidence angle map in the product package\n            include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n                (ignored for single-pol granules)\n            include_scattering_area: Include the scattering area in the product package\n            radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n                projected into the look direction (gamma0)\n            resolution: Desired output pixel spacing in meters\n            scale: Scale of output image; power, decibel or amplitude\n            speckle_filter: Apply an Enhanced Lee speckle filter\n            dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n            the Copernicus GLO-30 Public DEM.\n\n        Returns:\n            A dictionary containing the prepared RTC job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['granule', 'name', 'cls']:\n            job_parameters.pop(key, None)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule], **job_parameters},\n            'job_type': 'RTC_GAMMA',\n        }\n\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_job(\n        self,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        include_look_vectors: bool = False,\n        include_los_displacement: bool = False,\n        include_inc_map: bool = False,\n        looks: Literal['20x4', '10x2'] = '20x4',\n        include_dem: bool = False,\n        include_wrapped_phase: bool = False,\n        apply_water_mask: bool = False,\n        include_displacement_maps: bool = False,\n        phase_filter_parameter: float = 0.6,\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            include_look_vectors: Include the look vector theta and phi files in the product package\n            include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n                along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n                `include_displacement_maps`, and will be removed in a future release.\n            include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n            looks: Number of looks to take in range and azimuth\n            include_dem: Include the digital elevation model GeoTIFF in the product package\n            include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n            phase_filter_parameter: Adaptive phase filter parameter.\n                Useful values fall in the range 0.2 to 1.\n                Larger values result in stronger filtering.\n                If zero, adaptive phase filter will be skipped.\n\n        Returns:\n            A Batch object containing the InSAR job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_job(\n        cls,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        include_look_vectors: bool = False,\n        include_los_displacement: bool = False,\n        include_inc_map: bool = False,\n        looks: Literal['20x4', '10x2'] = '20x4',\n        include_dem: bool = False,\n        include_wrapped_phase: bool = False,\n        apply_water_mask: bool = False,\n        include_displacement_maps: bool = False,\n        phase_filter_parameter: float = 0.6,\n    ) -&gt; dict:\n\"\"\"Submit an InSAR job\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            include_look_vectors: Include the look vector theta and phi files in the product package\n            include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n                along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n                `include_displacement_maps`, and will be removed in a future release.\n            include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n            looks: Number of looks to take in range and azimuth\n            include_dem: Include the digital elevation model GeoTIFF in the product package\n            include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n            phase_filter_parameter: Adaptive phase filter parameter.\n                Useful values fall in the range 0.2 to 1.\n                Larger values result in stronger filtering.\n                If zero, adaptive phase filter will be skipped.\n\n        Returns:\n            A dictionary containing the prepared InSAR job\n        \"\"\"\n        if include_los_displacement:\n            warnings.warn(\n                'The include_los_displacement parameter has been deprecated in favor of '\n                'include_displacement_maps, and will be removed in a future release.',\n                FutureWarning,\n            )\n\n        job_parameters = locals().copy()\n        for key in ['cls', 'granule1', 'granule2', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n            'job_type': 'INSAR_GAMMA',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_isce_multi_burst_job(\n        self,\n        reference: list[str],\n        secondary: list[str],\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE multi burst job.\n\n        Args:\n            reference: A list of reference granules (scenes) to use\n            secondary: A list of secondary granules (scenes) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A Batch object containing the InSAR ISCE multi burst job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_isce_multi_burst_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_isce_multi_burst_job(\n        cls,\n        reference: list[str],\n        secondary: list[str],\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE multi burst job.\n\n        Args:\n            reference: A list of reference granules (scenes) to use\n            secondary: A list of secondary granules (scenes) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A dictionary containing the prepared InSAR ISCE multi burst job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {**job_parameters},\n            'job_type': 'INSAR_ISCE_MULTI_BURST',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_insar_isce_burst_job(\n        self,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE burst job.\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A Batch object containing the InSAR ISCE burst job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_insar_isce_burst_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_insar_isce_burst_job(\n        cls,\n        granule1: str,\n        granule2: str,\n        name: str | None = None,\n        apply_water_mask: bool = False,\n        looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n    ) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE burst job.\n\n        Args:\n            granule1: The first granule (scene) to use\n            granule2: The second granule (scene) to use\n            name: A name for the job\n            apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n                as invalid for phase unwrapping\n            looks: Number of looks to take in range and azimuth\n\n        Returns:\n            A dictionary containing the prepared InSAR ISCE burst job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'granule1', 'granule2', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n            'job_type': 'INSAR_ISCE_BURST',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_aria_s1_gunw_job(\n        self, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n    ) -&gt; Batch:\n\"\"\"Submit an ARIA S1 GUNW job.\n\n        Args:\n            reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n            secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n            frame_id: Subset GUNW products to this frame\n            name: A name for the job (optional)\n\n        Returns:\n            A Batch object containing the ARIA S1 GUNW job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_aria_s1_gunw_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_aria_s1_gunw_job(\n        cls, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n    ) -&gt; dict:\n\"\"\"Prepare an ARIA S1 GUNW job.\n\n        Args:\n            reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n            secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n            frame_id: Subset GUNW products to this frame\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared ARIA S1 GUNW job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': job_parameters,\n            'job_type': 'ARIA_S1_GUNW',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def submit_opera_rtc_s1_job(self, granule: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an OPERA RTC-S1 job.\n\n        Args:\n            granule: The name of the S1 burst to use\n            name: A name for the job (optional)\n\n        Returns:\n            A Batch object containing the OPERA RTC-S1 job\n        \"\"\"\n        arguments = locals().copy()\n        arguments.pop('self')\n        job_dict = self.prepare_opera_rtc_s1_job(**arguments)\n        return self.submit_prepared_jobs(prepared_jobs=job_dict)\n\n    @classmethod\n    def prepare_opera_rtc_s1_job(cls, granule: str, name: str | None = None) -&gt; dict:\n\"\"\"Prepare an OPERA RTC-S1 job.\n\n        Args:\n            granule: The name of the S1 burst to use\n            name: A name for the job\n\n        Returns:\n            A dictionary containing the prepared OPERA RTC-S1 job\n        \"\"\"\n        job_parameters = locals().copy()\n        for key in ['cls', 'name', 'granule']:\n            job_parameters.pop(key)\n\n        job_dict = {\n            'job_parameters': {'granules': [granule], **job_parameters},\n            'job_type': 'OPERA_RTC_S1',\n        }\n        if name is not None:\n            job_dict['name'] = name\n        return job_dict\n\n    def my_info(self) -&gt; dict:\n\"\"\"Returns:\n        Your user information\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url('/user'))\n        _raise_for_hyp3_status(response)\n        return response.json()\n\n    def check_credits(self) -&gt; float | int | None:\n\"\"\"Returns:\n        Your remaining processing credits, or None if you have no processing limit\n        \"\"\"\n        info = self.my_info()\n        return info['remaining_credits']\n\n    def check_quota(self) -&gt; float | int | None:\n\"\"\"Deprecated method for checking your remaining processing credits; replaced by `HyP3.check_credits`\n\n        Returns:\n            Your remaining processing credits, or None if you have no processing limit\n        \"\"\"\n        warn(\n            'This method is deprecated and will be removed in a future release.\\n'\n            'Please use `HyP3.check_credits` instead.',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.check_credits()\n\n    def costs(self) -&gt; dict:\n\"\"\"Returns:\n        Table of job costs\n        \"\"\"\n        response = self.session.get(self._get_endpoint_url('/costs'))\n        _raise_for_hyp3_status(response)\n        return response.json()\n\n    def update_jobs(self, jobs: Batch | Job, **kwargs: object) -&gt; Batch | Job:\n\"\"\"Update the name of one or more previously-submitted jobs.\n\n        Args:\n            jobs: The job(s) to update\n            kwargs:\n                name: The new name, or None to remove the name\n\n        Returns:\n            The updated job(s)\n        \"\"\"\n        if isinstance(jobs, Batch):\n            batch = hyp3_sdk.Batch()\n            tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n            for job in tqdm(jobs):\n                batch += self.update_jobs(job, **kwargs)\n            return batch\n\n        if not isinstance(jobs, Job):\n            raise TypeError(f\"'jobs' has type {type(jobs)}, must be {Batch} or {Job}\")\n\n        response = self.session.patch(self._get_endpoint_url(f'/jobs/{jobs.job_id}'), json=kwargs)\n        _raise_for_hyp3_status(response)\n        return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.__init__","title":"<code>__init__(api_url=PROD_API, username=None, password=None, token=None, prompt=None)</code>","text":"<p>If username and password are not provided, attempts to use credentials from a <code>.netrc</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>Address of the HyP3 API</p> <code>PROD_API</code> <code>username</code> <code>str | None</code> <p>Username for authenticating to <code>urs.earthdata.nasa.gov</code>. Both username and password must be provided if either is provided.</p> <code>None</code> <code>password</code> <code>str | None</code> <p>Password for authenticating to <code>urs.earthdata.nasa.gov</code>. Both username and password must be provided if either is provided.</p> <code>None</code> <code>token</code> <code>str | None</code> <p>Earthdata Login Bearer Token for authenticating to <code>urs.earthdata.nasa.gov</code></p> <code>None</code> <code>prompt</code> <code>Literal['password', 'token'] | bool | None</code> <p>Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.</p> <code>None</code> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def __init__(\n    self,\n    api_url: str = PROD_API,\n    username: str | None = None,\n    password: str | None = None,\n    token: str | None = None,\n    prompt: Literal['password', 'token'] | bool | None = None,\n):\n\"\"\"If username and password are not provided, attempts to use credentials from a `.netrc` file.\n\n    Args:\n        api_url: Address of the HyP3 API\n        username: Username for authenticating to `urs.earthdata.nasa.gov`.\n            Both username and password must be provided if either is provided.\n        password: Password for authenticating to `urs.earthdata.nasa.gov`.\n            Both username and password must be provided if either is provided.\n        token: Earthdata Login Bearer Token for authenticating to `urs.earthdata.nasa.gov`\n        prompt: Either 'password' or 'token' to prompt for EDL username and password or EDL bearer token, respectively.\n    \"\"\"\n    self.url = api_url\n\n    if prompt not in (True, False, 'password', 'token', None):\n        raise ValueError(f'Unexpected value {prompt} for `prompt`')\n\n    if prompt is True:\n        warnings.warn(\n            'Passing `prompt=True` is deprecated. Please use either `prompt=\"password\"` or `prompt=\"token\"`',\n            UserWarning,\n        )\n        prompt = 'password'\n\n    if prompt == 'password':\n        if username is None:\n            username = input('NASA Earthdata Login username: ')\n        if password is None:\n            password = getpass('NASA Earthdata Login password: ')\n\n    if prompt == 'token' and token is None:\n        token = getpass('NASA Earthdata Login bearer token: ')\n\n    self.session = hyp3_sdk.util.get_authenticated_session(username, password, token)\n    self.session.headers.update({'User-Agent': f'{hyp3_sdk.__name__}/{hyp3_sdk.__version__}'})\n\n    hostname = urlsplit(self.url).hostname\n    assert hostname is not None\n    if not hostname.endswith('.asf.alaska.edu') and 'asf-urs' in self.session.cookies:\n        self.session.cookies.set('asf-urs', self.session.cookies['asf-urs'], domain=hostname)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.check_credits","title":"<code>check_credits()</code>","text":"<p>Returns: Your remaining processing credits, or None if you have no processing limit</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def check_credits(self) -&gt; float | int | None:\n\"\"\"Returns:\n    Your remaining processing credits, or None if you have no processing limit\n    \"\"\"\n    info = self.my_info()\n    return info['remaining_credits']\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.check_quota","title":"<code>check_quota()</code>","text":"<p>Deprecated method for checking your remaining processing credits; replaced by <code>HyP3.check_credits</code></p> <p>Returns:</p> Type Description <code>float | int | None</code> <p>Your remaining processing credits, or None if you have no processing limit</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def check_quota(self) -&gt; float | int | None:\n\"\"\"Deprecated method for checking your remaining processing credits; replaced by `HyP3.check_credits`\n\n    Returns:\n        Your remaining processing credits, or None if you have no processing limit\n    \"\"\"\n    warn(\n        'This method is deprecated and will be removed in a future release.\\n'\n        'Please use `HyP3.check_credits` instead.',\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return self.check_credits()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.costs","title":"<code>costs()</code>","text":"<p>Returns: Table of job costs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def costs(self) -&gt; dict:\n\"\"\"Returns:\n    Table of job costs\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url('/costs'))\n    _raise_for_hyp3_status(response)\n    return response.json()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.find_jobs","title":"<code>find_jobs(start=None, end=None, status_code=None, name=None, job_type=None, user_id=None)</code>","text":"<p>Gets a Batch of jobs from HyP3 matching the provided search criteria</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime | None</code> <p>only jobs submitted after given time</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>only jobs submitted before given time</p> <code>None</code> <code>status_code</code> <code>str | None</code> <p>only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)</p> <code>None</code> <code>name</code> <code>str | None</code> <p>only jobs with this name</p> <code>None</code> <code>job_type</code> <code>str | None</code> <p>only jobs with this job_type</p> <code>None</code> <code>user_id</code> <code>str | None</code> <p>only jobs submitted by this user (defaults to the current user)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the found jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def find_jobs(\n    self,\n    start: datetime | None = None,\n    end: datetime | None = None,\n    status_code: str | None = None,\n    name: str | None = None,\n    job_type: str | None = None,\n    user_id: str | None = None,\n) -&gt; Batch:\n\"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria\n\n    Args:\n        start: only jobs submitted after given time\n        end: only jobs submitted before given time\n        status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING)\n        name: only jobs with this name\n        job_type: only jobs with this job_type\n        user_id: only jobs submitted by this user (defaults to the current user)\n\n    Returns:\n        A Batch object containing the found jobs\n    \"\"\"\n    params = {}\n    for param_name in ('start', 'end', 'status_code', 'name', 'job_type', 'user_id'):\n        param_value = locals().get(param_name)\n        if param_value is not None:\n            if isinstance(param_value, datetime):\n                if param_value.tzinfo is None:\n                    param_value = param_value.replace(tzinfo=timezone.utc)\n                param_value = param_value.isoformat(timespec='seconds')\n\n            params[param_name] = param_value\n\n    response = self.session.get(self._get_endpoint_url('/jobs'), params=params)\n    _raise_for_hyp3_status(response)\n    jobs = [Job.from_dict(job) for job in response.json()['jobs']]\n\n    while 'next' in response.json():\n        next_url = response.json()['next']\n        response = self.session.get(next_url)\n        _raise_for_hyp3_status(response)\n        jobs.extend([Job.from_dict(job) for job in response.json()['jobs']])\n\n    return Batch(jobs)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.get_job_by_id","title":"<code>get_job_by_id(job_id)</code>","text":"<p>Get job by job ID</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>A job ID</p> required <p>Returns:</p> Type Description <code>Job</code> <p>A Job object</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def get_job_by_id(self, job_id: str) -&gt; Job:\n\"\"\"Get job by job ID\n\n    Args:\n        job_id: A job ID\n\n    Returns:\n        A Job object\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url(f'/jobs/{job_id}'))\n    _raise_for_hyp3_status(response)\n\n    return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.my_info","title":"<code>my_info()</code>","text":"<p>Returns: Your user information</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def my_info(self) -&gt; dict:\n\"\"\"Returns:\n    Your user information\n    \"\"\"\n    response = self.session.get(self._get_endpoint_url('/user'))\n    _raise_for_hyp3_status(response)\n    return response.json()\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_aria_s1_gunw_job","title":"<code>prepare_aria_s1_gunw_job(reference_date, secondary_date, frame_id, name=None)</code>  <code>classmethod</code>","text":"<p>Prepare an ARIA S1 GUNW job.</p> <p>Parameters:</p> Name Type Description Default <code>reference_date</code> <code>str</code> <p>Date of reference scenes for InSAR processing in YYYY-MM-DD format</p> required <code>secondary_date</code> <code>str</code> <p>Date of secondary scenes for InSAR processing in YYYY-MM-DD format</p> required <code>frame_id</code> <code>int</code> <p>Subset GUNW products to this frame</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared ARIA S1 GUNW job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_aria_s1_gunw_job(\n    cls, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n) -&gt; dict:\n\"\"\"Prepare an ARIA S1 GUNW job.\n\n    Args:\n        reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n        secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n        frame_id: Subset GUNW products to this frame\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared ARIA S1 GUNW job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': job_parameters,\n        'job_type': 'ARIA_S1_GUNW',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_autorift_job","title":"<code>prepare_autorift_job(granule1, granule2, name=None)</code>  <code>classmethod</code>","text":"<p>Submit an autoRIFT job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared autoRIFT job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_autorift_job(cls, granule1: str, granule2: str, name: str | None = None) -&gt; dict:\n\"\"\"Submit an autoRIFT job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared autoRIFT job\n    \"\"\"\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2]},\n        'job_type': 'AUTORIFT',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_insar_isce_burst_job","title":"<code>prepare_insar_isce_burst_job(granule1, granule2, name=None, apply_water_mask=False, looks='20x4')</code>  <code>classmethod</code>","text":"<p>Prepare an InSAR ISCE burst job.</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR ISCE burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_isce_burst_job(\n    cls,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE burst job.\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A dictionary containing the prepared InSAR ISCE burst job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'granule1', 'granule2', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n        'job_type': 'INSAR_ISCE_BURST',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_insar_isce_multi_burst_job","title":"<code>prepare_insar_isce_multi_burst_job(reference, secondary, name=None, apply_water_mask=False, looks='20x4')</code>  <code>classmethod</code>","text":"<p>Prepare an InSAR ISCE multi burst job.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>list[str]</code> <p>A list of reference granules (scenes) to use</p> required <code>secondary</code> <code>list[str]</code> <p>A list of secondary granules (scenes) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR ISCE multi burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_isce_multi_burst_job(\n    cls,\n    reference: list[str],\n    secondary: list[str],\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; dict:\n\"\"\"Prepare an InSAR ISCE multi burst job.\n\n    Args:\n        reference: A list of reference granules (scenes) to use\n        secondary: A list of secondary granules (scenes) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A dictionary containing the prepared InSAR ISCE multi burst job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {**job_parameters},\n        'job_type': 'INSAR_ISCE_MULTI_BURST',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_insar_job","title":"<code>prepare_insar_job(granule1, granule2, name=None, include_look_vectors=False, include_los_displacement=False, include_inc_map=False, looks='20x4', include_dem=False, include_wrapped_phase=False, apply_water_mask=False, include_displacement_maps=False, phase_filter_parameter=0.6)</code>  <code>classmethod</code>","text":"<p>Submit an InSAR job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>include_look_vectors</code> <code>bool</code> <p>Include the look vector theta and phi files in the product package</p> <code>False</code> <code>include_los_displacement</code> <code>bool</code> <p>Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of <code>include_displacement_maps</code>, and will be removed in a future release.</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local and ellipsoidal incidence angle maps in the product package</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <code>include_dem</code> <code>bool</code> <p>Include the digital elevation model GeoTIFF in the product package</p> <code>False</code> <code>include_wrapped_phase</code> <code>bool</code> <p>Include the wrapped phase GeoTIFF in the product package</p> <code>False</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>include_displacement_maps</code> <code>bool</code> <p>Include displacement maps (line-of-sight and vertical) in the product package</p> <code>False</code> <code>phase_filter_parameter</code> <code>float</code> <p>Adaptive phase filter parameter. Useful values fall in the range 0.2 to 1. Larger values result in stronger filtering. If zero, adaptive phase filter will be skipped.</p> <code>0.6</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared InSAR job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_insar_job(\n    cls,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    include_look_vectors: bool = False,\n    include_los_displacement: bool = False,\n    include_inc_map: bool = False,\n    looks: Literal['20x4', '10x2'] = '20x4',\n    include_dem: bool = False,\n    include_wrapped_phase: bool = False,\n    apply_water_mask: bool = False,\n    include_displacement_maps: bool = False,\n    phase_filter_parameter: float = 0.6,\n) -&gt; dict:\n\"\"\"Submit an InSAR job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        include_look_vectors: Include the look vector theta and phi files in the product package\n        include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n            along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n            `include_displacement_maps`, and will be removed in a future release.\n        include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n        looks: Number of looks to take in range and azimuth\n        include_dem: Include the digital elevation model GeoTIFF in the product package\n        include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n        phase_filter_parameter: Adaptive phase filter parameter.\n            Useful values fall in the range 0.2 to 1.\n            Larger values result in stronger filtering.\n            If zero, adaptive phase filter will be skipped.\n\n    Returns:\n        A dictionary containing the prepared InSAR job\n    \"\"\"\n    if include_los_displacement:\n        warnings.warn(\n            'The include_los_displacement parameter has been deprecated in favor of '\n            'include_displacement_maps, and will be removed in a future release.',\n            FutureWarning,\n        )\n\n    job_parameters = locals().copy()\n    for key in ['cls', 'granule1', 'granule2', 'name']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule1, granule2], **job_parameters},\n        'job_type': 'INSAR_GAMMA',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_opera_rtc_s1_job","title":"<code>prepare_opera_rtc_s1_job(granule, name=None)</code>  <code>classmethod</code>","text":"<p>Prepare an OPERA RTC-S1 job.</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The name of the S1 burst to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared OPERA RTC-S1 job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_opera_rtc_s1_job(cls, granule: str, name: str | None = None) -&gt; dict:\n\"\"\"Prepare an OPERA RTC-S1 job.\n\n    Args:\n        granule: The name of the S1 burst to use\n        name: A name for the job\n\n    Returns:\n        A dictionary containing the prepared OPERA RTC-S1 job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['cls', 'name', 'granule']:\n        job_parameters.pop(key)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule], **job_parameters},\n        'job_type': 'OPERA_RTC_S1',\n    }\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_rtc_job","title":"<code>prepare_rtc_job(granule, name=None, dem_matching=False, include_dem=False, include_inc_map=False, include_rgb=False, include_scattering_area=False, radiometry='gamma0', resolution=30, scale='power', speckle_filter=False, dem_name='copernicus')</code>  <code>classmethod</code>","text":"<p>Submit an RTC job</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>dem_matching</code> <code>bool</code> <p>Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files</p> <code>False</code> <code>include_dem</code> <code>bool</code> <p>Include the DEM file in the product package</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local incidence angle map in the product package</p> <code>False</code> <code>include_rgb</code> <code>bool</code> <p>Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules)</p> <code>False</code> <code>include_scattering_area</code> <code>bool</code> <p>Include the scattering area in the product package</p> <code>False</code> <code>radiometry</code> <code>Literal['sigma0', 'gamma0']</code> <p>Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0)</p> <code>'gamma0'</code> <code>resolution</code> <code>Literal[10, 20, 30]</code> <p>Desired output pixel spacing in meters</p> <code>30</code> <code>scale</code> <code>Literal['amplitude', 'decibel', 'power']</code> <p>Scale of output image; power, decibel or amplitude</p> <code>'power'</code> <code>speckle_filter</code> <code>bool</code> <p>Apply an Enhanced Lee speckle filter</p> <code>False</code> <code>dem_name</code> <code>Literal['copernicus']</code> <p>Name of the DEM to use for processing. <code>copernicus</code> is the only option, and it will use</p> <code>'copernicus'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the prepared RTC job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@classmethod\ndef prepare_rtc_job(\n    cls,\n    granule: str,\n    name: str | None = None,\n    dem_matching: bool = False,\n    include_dem: bool = False,\n    include_inc_map: bool = False,\n    include_rgb: bool = False,\n    include_scattering_area: bool = False,\n    radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n    resolution: Literal[10, 20, 30] = 30,\n    scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n    speckle_filter: bool = False,\n    dem_name: Literal['copernicus'] = 'copernicus',\n) -&gt; dict:\n\"\"\"Submit an RTC job\n\n    Args:\n        granule: The granule (scene) to use\n        name: A name for the job\n        dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n        include_dem: Include the DEM file in the product package\n        include_inc_map: Include the local incidence angle map in the product package\n        include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n            (ignored for single-pol granules)\n        include_scattering_area: Include the scattering area in the product package\n        radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n            projected into the look direction (gamma0)\n        resolution: Desired output pixel spacing in meters\n        scale: Scale of output image; power, decibel or amplitude\n        speckle_filter: Apply an Enhanced Lee speckle filter\n        dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n        the Copernicus GLO-30 Public DEM.\n\n    Returns:\n        A dictionary containing the prepared RTC job\n    \"\"\"\n    job_parameters = locals().copy()\n    for key in ['granule', 'name', 'cls']:\n        job_parameters.pop(key, None)\n\n    job_dict = {\n        'job_parameters': {'granules': [granule], **job_parameters},\n        'job_type': 'RTC_GAMMA',\n    }\n\n    if name is not None:\n        job_dict['name'] = name\n    return job_dict\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.refresh","title":"<code>refresh(job_or_batch)</code>","text":"<p>Refresh each jobs' information</p> <p>Parameters:</p> Name Type Description Default <code>job_or_batch</code> <code>Batch | Job</code> <p>A Batch of Job object to refresh</p> required <p>Returns:</p> Type Description <code>Batch | Job</code> <p>A Batch or Job object with refreshed information</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@singledispatchmethod\ndef refresh(self, job_or_batch: Batch | Job) -&gt; Batch | Job:\n\"\"\"Refresh each jobs' information\n\n    Args:\n        job_or_batch: A Batch of Job object to refresh\n\n    Returns:\n        A Batch or Job object with refreshed information\n    \"\"\"\n    raise NotImplementedError(f'Cannot refresh {type(job_or_batch)} type object')\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_aria_s1_gunw_job","title":"<code>submit_aria_s1_gunw_job(reference_date, secondary_date, frame_id, name=None)</code>","text":"<p>Submit an ARIA S1 GUNW job.</p> <p>Parameters:</p> Name Type Description Default <code>reference_date</code> <code>str</code> <p>Date of reference scenes for InSAR processing in YYYY-MM-DD format</p> required <code>secondary_date</code> <code>str</code> <p>Date of secondary scenes for InSAR processing in YYYY-MM-DD format</p> required <code>frame_id</code> <code>int</code> <p>Subset GUNW products to this frame</p> required <code>name</code> <code>str | None</code> <p>A name for the job (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the ARIA S1 GUNW job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_aria_s1_gunw_job(\n    self, reference_date: str, secondary_date: str, frame_id: int, name: str | None = None\n) -&gt; Batch:\n\"\"\"Submit an ARIA S1 GUNW job.\n\n    Args:\n        reference_date: Date of reference scenes for InSAR processing in YYYY-MM-DD format\n        secondary_date: Date of secondary scenes for InSAR processing in YYYY-MM-DD format\n        frame_id: Subset GUNW products to this frame\n        name: A name for the job (optional)\n\n    Returns:\n        A Batch object containing the ARIA S1 GUNW job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_aria_s1_gunw_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_autorift_job","title":"<code>submit_autorift_job(granule1, granule2, name=None)</code>","text":"<p>Submit an autoRIFT job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the autoRIFT job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_autorift_job(self, granule1: str, granule2: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an autoRIFT job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n\n    Returns:\n        A Batch object containing the autoRIFT job\n    \"\"\"\n    job_dict = self.prepare_autorift_job(granule1, granule2, name=name)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_isce_burst_job","title":"<code>submit_insar_isce_burst_job(granule1, granule2, name=None, apply_water_mask=False, looks='20x4')</code>","text":"<p>Submit an InSAR ISCE burst job.</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR ISCE burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_isce_burst_job(\n    self,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE burst job.\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A Batch object containing the InSAR ISCE burst job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_isce_burst_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_isce_multi_burst_job","title":"<code>submit_insar_isce_multi_burst_job(reference, secondary, name=None, apply_water_mask=False, looks='20x4')</code>","text":"<p>Submit an InSAR ISCE multi burst job.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>list[str]</code> <p>A list of reference granules (scenes) to use</p> required <code>secondary</code> <code>list[str]</code> <p>A list of secondary granules (scenes) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2', '5x1']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR ISCE multi burst job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_isce_multi_burst_job(\n    self,\n    reference: list[str],\n    secondary: list[str],\n    name: str | None = None,\n    apply_water_mask: bool = False,\n    looks: Literal['20x4', '10x2', '5x1'] = '20x4',\n) -&gt; Batch:\n\"\"\"Submit an InSAR ISCE multi burst job.\n\n    Args:\n        reference: A list of reference granules (scenes) to use\n        secondary: A list of secondary granules (scenes) to use\n        name: A name for the job\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        looks: Number of looks to take in range and azimuth\n\n    Returns:\n        A Batch object containing the InSAR ISCE multi burst job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_isce_multi_burst_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_job","title":"<code>submit_insar_job(granule1, granule2, name=None, include_look_vectors=False, include_los_displacement=False, include_inc_map=False, looks='20x4', include_dem=False, include_wrapped_phase=False, apply_water_mask=False, include_displacement_maps=False, phase_filter_parameter=0.6)</code>","text":"<p>Submit an InSAR job</p> <p>Parameters:</p> Name Type Description Default <code>granule1</code> <code>str</code> <p>The first granule (scene) to use</p> required <code>granule2</code> <code>str</code> <p>The second granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>include_look_vectors</code> <code>bool</code> <p>Include the look vector theta and phi files in the product package</p> <code>False</code> <code>include_los_displacement</code> <code>bool</code> <p>Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of <code>include_displacement_maps</code>, and will be removed in a future release.</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local and ellipsoidal incidence angle maps in the product package</p> <code>False</code> <code>looks</code> <code>Literal['20x4', '10x2']</code> <p>Number of looks to take in range and azimuth</p> <code>'20x4'</code> <code>include_dem</code> <code>bool</code> <p>Include the digital elevation model GeoTIFF in the product package</p> <code>False</code> <code>include_wrapped_phase</code> <code>bool</code> <p>Include the wrapped phase GeoTIFF in the product package</p> <code>False</code> <code>apply_water_mask</code> <code>bool</code> <p>Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping</p> <code>False</code> <code>include_displacement_maps</code> <code>bool</code> <p>Include displacement maps (line-of-sight and vertical) in the product package</p> <code>False</code> <code>phase_filter_parameter</code> <code>float</code> <p>Adaptive phase filter parameter. Useful values fall in the range 0.2 to 1. Larger values result in stronger filtering. If zero, adaptive phase filter will be skipped.</p> <code>0.6</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the InSAR job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_insar_job(\n    self,\n    granule1: str,\n    granule2: str,\n    name: str | None = None,\n    include_look_vectors: bool = False,\n    include_los_displacement: bool = False,\n    include_inc_map: bool = False,\n    looks: Literal['20x4', '10x2'] = '20x4',\n    include_dem: bool = False,\n    include_wrapped_phase: bool = False,\n    apply_water_mask: bool = False,\n    include_displacement_maps: bool = False,\n    phase_filter_parameter: float = 0.6,\n) -&gt; Batch:\n\"\"\"Submit an InSAR job\n\n    Args:\n        granule1: The first granule (scene) to use\n        granule2: The second granule (scene) to use\n        name: A name for the job\n        include_look_vectors: Include the look vector theta and phi files in the product package\n        include_los_displacement: Include a GeoTIFF in the product package containing displacement values\n            along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of\n            `include_displacement_maps`, and will be removed in a future release.\n        include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package\n        looks: Number of looks to take in range and azimuth\n        include_dem: Include the digital elevation model GeoTIFF in the product package\n        include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package\n        apply_water_mask: Sets pixels over coastal waters and large inland waterbodies\n            as invalid for phase unwrapping\n        include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package\n        phase_filter_parameter: Adaptive phase filter parameter.\n            Useful values fall in the range 0.2 to 1.\n            Larger values result in stronger filtering.\n            If zero, adaptive phase filter will be skipped.\n\n    Returns:\n        A Batch object containing the InSAR job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_insar_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_opera_rtc_s1_job","title":"<code>submit_opera_rtc_s1_job(granule, name=None)</code>","text":"<p>Submit an OPERA RTC-S1 job.</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The name of the S1 burst to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the OPERA RTC-S1 job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_opera_rtc_s1_job(self, granule: str, name: str | None = None) -&gt; Batch:\n\"\"\"Submit an OPERA RTC-S1 job.\n\n    Args:\n        granule: The name of the S1 burst to use\n        name: A name for the job (optional)\n\n    Returns:\n        A Batch object containing the OPERA RTC-S1 job\n    \"\"\"\n    arguments = locals().copy()\n    arguments.pop('self')\n    job_dict = self.prepare_opera_rtc_s1_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_prepared_jobs","title":"<code>submit_prepared_jobs(prepared_jobs)</code>","text":"<p>Submit a prepared job dictionary, or list of prepared job dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>prepared_jobs</code> <code>dict | list[dict]</code> <p>A prepared job dictionary, or list of prepared job dictionaries</p> required <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the submitted job(s)</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_prepared_jobs(self, prepared_jobs: dict | list[dict]) -&gt; Batch:\n\"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries\n\n    Args:\n        prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries\n\n    Returns:\n        A Batch object containing the submitted job(s)\n    \"\"\"\n    if isinstance(prepared_jobs, dict):\n        payload = {'jobs': [prepared_jobs]}\n    else:\n        payload = {'jobs': prepared_jobs}\n\n    response = self.session.post(self._get_endpoint_url('/jobs'), json=payload)\n    _raise_for_hyp3_status(response)\n\n    batch = Batch()\n    for job in response.json()['jobs']:\n        batch += Job.from_dict(job)\n    return batch\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job","title":"<code>submit_rtc_job(granule, name=None, dem_matching=False, include_dem=False, include_inc_map=False, include_rgb=False, include_scattering_area=False, radiometry='gamma0', resolution=30, scale='power', speckle_filter=False, dem_name='copernicus')</code>","text":"<p>Submit an RTC job</p> <p>Parameters:</p> Name Type Description Default <code>granule</code> <code>str</code> <p>The granule (scene) to use</p> required <code>name</code> <code>str | None</code> <p>A name for the job</p> <code>None</code> <code>dem_matching</code> <code>bool</code> <p>Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files</p> <code>False</code> <code>include_dem</code> <code>bool</code> <p>Include the DEM file in the product package</p> <code>False</code> <code>include_inc_map</code> <code>bool</code> <p>Include the local incidence angle map in the product package</p> <code>False</code> <code>include_rgb</code> <code>bool</code> <p>Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules)</p> <code>False</code> <code>include_scattering_area</code> <code>bool</code> <p>Include the scattering area in the product package</p> <code>False</code> <code>radiometry</code> <code>Literal['sigma0', 'gamma0']</code> <p>Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0)</p> <code>'gamma0'</code> <code>resolution</code> <code>Literal[10, 20, 30]</code> <p>Desired output pixel spacing in meters</p> <code>30</code> <code>scale</code> <code>Literal['amplitude', 'decibel', 'power']</code> <p>Scale of output image; power, decibel or amplitude</p> <code>'power'</code> <code>speckle_filter</code> <code>bool</code> <p>Apply an Enhanced Lee speckle filter</p> <code>False</code> <code>dem_name</code> <code>Literal['copernicus']</code> <p>Name of the DEM to use for processing. <code>copernicus</code> is the only option, and it will use</p> <code>'copernicus'</code> <p>Returns:</p> Type Description <code>Batch</code> <p>A Batch object containing the RTC job</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def submit_rtc_job(\n    self,\n    granule: str,\n    name: str | None = None,\n    dem_matching: bool = False,\n    include_dem: bool = False,\n    include_inc_map: bool = False,\n    include_rgb: bool = False,\n    include_scattering_area: bool = False,\n    radiometry: Literal['sigma0', 'gamma0'] = 'gamma0',\n    resolution: Literal[10, 20, 30] = 30,\n    scale: Literal['amplitude', 'decibel', 'power'] = 'power',\n    speckle_filter: bool = False,\n    dem_name: Literal['copernicus'] = 'copernicus',\n) -&gt; Batch:\n\"\"\"Submit an RTC job\n\n    Args:\n        granule: The granule (scene) to use\n        name: A name for the job\n        dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files\n        include_dem: Include the DEM file in the product package\n        include_inc_map: Include the local incidence angle map in the product package\n        include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules\n            (ignored for single-pol granules)\n        include_scattering_area: Include the scattering area in the product package\n        radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area\n            projected into the look direction (gamma0)\n        resolution: Desired output pixel spacing in meters\n        scale: Scale of output image; power, decibel or amplitude\n        speckle_filter: Apply an Enhanced Lee speckle filter\n        dem_name: Name of the DEM to use for processing. `copernicus` is the only option, and it will use\n        the Copernicus GLO-30 Public DEM.\n\n    Returns:\n        A Batch object containing the RTC job\n    \"\"\"\n    arguments = locals()\n    arguments.pop('self')\n    job_dict = self.prepare_rtc_job(**arguments)\n    return self.submit_prepared_jobs(prepared_jobs=job_dict)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.update_jobs","title":"<code>update_jobs(jobs, **kwargs)</code>","text":"<p>Update the name of one or more previously-submitted jobs.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Batch | Job</code> <p>The job(s) to update</p> required <code>kwargs</code> <code>object</code> <p>name: The new name, or None to remove the name</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch | Job</code> <p>The updated job(s)</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>def update_jobs(self, jobs: Batch | Job, **kwargs: object) -&gt; Batch | Job:\n\"\"\"Update the name of one or more previously-submitted jobs.\n\n    Args:\n        jobs: The job(s) to update\n        kwargs:\n            name: The new name, or None to remove the name\n\n    Returns:\n        The updated job(s)\n    \"\"\"\n    if isinstance(jobs, Batch):\n        batch = hyp3_sdk.Batch()\n        tqdm = hyp3_sdk.util.get_tqdm_progress_bar()\n        for job in tqdm(jobs):\n            batch += self.update_jobs(job, **kwargs)\n        return batch\n\n    if not isinstance(jobs, Job):\n        raise TypeError(f\"'jobs' has type {type(jobs)}, must be {Batch} or {Job}\")\n\n    response = self.session.patch(self._get_endpoint_url(f'/jobs/{jobs.job_id}'), json=kwargs)\n    _raise_for_hyp3_status(response)\n    return Job.from_dict(response.json())\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.watch","title":"<code>watch(job_or_batch, timeout=10800, interval=60)</code>","text":"<p>Watch jobs until they complete</p> <p>Parameters:</p> Name Type Description Default <code>job_or_batch</code> <code>Batch | Job</code> <p>A Batch or Job object of jobs to watch</p> required <code>timeout</code> <code>int</code> <p>How long to wait until exiting in seconds</p> <code>10800</code> <code>interval</code> <code>int | float</code> <p>How often to check for updates in seconds</p> <code>60</code> <p>Returns:</p> Type Description <code>Batch | Job</code> <p>A Batch or Job object with refreshed watched jobs</p> Source code in <code>hyp3_sdk/hyp3.py</code> <pre><code>@singledispatchmethod\ndef watch(self, job_or_batch: Batch | Job, timeout: int = 10800, interval: int | float = 60) -&gt; Batch | Job:\n\"\"\"Watch jobs until they complete\n\n    Args:\n        job_or_batch: A Batch or Job object of jobs to watch\n        timeout: How long to wait until exiting in seconds\n        interval: How often to check for updates in seconds\n\n    Returns:\n        A Batch or Job object with refreshed watched jobs\n    \"\"\"\n    raise NotImplementedError(f'Cannot watch {type(job_or_batch)} type object')\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs","title":"<code>jobs</code>","text":""},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch","title":"<code>Batch</code>","text":"Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>class Batch:\n    def __init__(self, jobs: list[Job] | None = None):\n        if jobs is None:\n            jobs = []\n        self.jobs = jobs\n\n    def __add__(self, other: Union[Job, 'Batch']):\n        if isinstance(other, Batch):\n            return Batch(self.jobs + other.jobs)\n        elif isinstance(other, Job):\n            return Batch(self.jobs + [other])\n        else:\n            raise TypeError(f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\")\n\n    def __iadd__(self, other: Union[Job, 'Batch']):\n        if isinstance(other, Batch):\n            self.jobs += other.jobs\n        elif isinstance(other, Job):\n            self.jobs += [other]\n        else:\n            raise TypeError(f\"unsupported operand type(s) for +=: '{type(self)}' and '{type(other)}'\")\n        return self\n\n    def __iter__(self):\n        return iter(self.jobs)\n\n    def __len__(self):\n        return len(self.jobs)\n\n    def __contains__(self, job: Job):\n        return job in self.jobs\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Batch):\n            raise TypeError('`__eq__` can only compare a Batch object with another Batch object.')\n        return self.jobs == other.jobs\n\n    def __delitem__(self, job: int):\n        self.jobs.pop(job)\n        return self\n\n    def __getitem__(self, index: int | slice):\n        if isinstance(index, slice):\n            return Batch(self.jobs[index])\n        return self.jobs[index]\n\n    def __setitem__(self, index: int, job: Job):\n        self.jobs[index] = job\n        return self\n\n    def __repr__(self):\n        reprs = ', '.join([job.__repr__() for job in self.jobs])\n        return f'Batch([{reprs}])'\n\n    def __str__(self):\n        count = self._count_statuses()\n        return (\n            f'{len(self)} HyP3 Jobs: '\n            f'{count[\"SUCCEEDED\"]} succeeded, '\n            f'{count[\"FAILED\"]} failed, '\n            f'{count[\"RUNNING\"]} running, '\n            f'{count[\"PENDING\"]} pending.'\n        )\n\n    def _count_statuses(self):\n        return Counter([job.status_code for job in self.jobs])\n\n    def complete(self) -&gt; bool:\n\"\"\"Returns: True if all jobs are complete, otherwise returns False\"\"\"\n        for job in self.jobs:\n            if not job.complete():\n                return False\n        return True\n\n    def succeeded(self) -&gt; bool:\n\"\"\"Returns: True if all jobs have succeeded, otherwise returns False\"\"\"\n        for job in self.jobs:\n            if not job.succeeded():\n                return False\n        return True\n\n    def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n            location: Directory location to put files into\n            create: Create `location` if it does not point to an existing directory\n\n        Returns: list of Path objects to downloaded files\n        \"\"\"\n        downloaded_files = []\n        tqdm = get_tqdm_progress_bar()\n        for job in tqdm(self.jobs):\n            try:\n                downloaded_files.extend(job.download_files(location, create))\n            except HyP3SDKError as e:\n                print(f'Warning: {e} Skipping download for {job}.')\n        return downloaded_files\n\n    def any_expired(self) -&gt; bool:\n\"\"\"Check succeeded jobs for expiration\"\"\"\n        for job in self.jobs:\n            try:\n                if job.expired():\n                    return True\n            except HyP3SDKError:\n                continue\n        return False\n\n    def filter_jobs(\n        self,\n        succeeded: bool = True,\n        pending: bool = True,\n        running: bool = True,\n        failed: bool = False,\n        include_expired: bool = True,\n    ) -&gt; 'Batch':\n\"\"\"Filter jobs by status. By default, only succeeded, pending,\n        and still running jobs will be in the returned batch.\n\n        Args:\n            succeeded: Include all succeeded jobs\n            pending: Include all pending jobs\n            running: Include all running jobs\n            failed: Include all failed jobs\n            include_expired: Include expired jobs in the result\n\n\n        Returns:\n             batch: A batch object containing jobs matching all the selected statuses\n        \"\"\"\n        filtered_jobs = []\n\n        for job in self.jobs:\n            if job.succeeded() and succeeded:\n                if include_expired or not job.expired():\n                    filtered_jobs.append(job)\n\n            elif job.running() and running:\n                filtered_jobs.append(job)\n\n            elif job.pending() and pending:\n                filtered_jobs.append(job)\n\n            elif job.failed() and failed:\n                filtered_jobs.append(job)\n\n        return Batch(filtered_jobs)\n\n    def total_credit_cost(self):\n        return sum(job.credit_cost for job in self.jobs if job.credit_cost is not None)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.any_expired","title":"<code>any_expired()</code>","text":"<p>Check succeeded jobs for expiration</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def any_expired(self) -&gt; bool:\n\"\"\"Check succeeded jobs for expiration\"\"\"\n    for job in self.jobs:\n        try:\n            if job.expired():\n                return True\n        except HyP3SDKError:\n            continue\n    return False\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.complete","title":"<code>complete()</code>","text":"<p>Returns: True if all jobs are complete, otherwise returns False</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def complete(self) -&gt; bool:\n\"\"\"Returns: True if all jobs are complete, otherwise returns False\"\"\"\n    for job in self.jobs:\n        if not job.complete():\n            return False\n    return True\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.download_files","title":"<code>download_files(location='.', create=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>location</code> <code>Path | str</code> <p>Directory location to put files into</p> <code>'.'</code> <code>create</code> <code>bool</code> <p>Create <code>location</code> if it does not point to an existing directory</p> <code>True</code> <p>Returns: list of Path objects to downloaded files</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n        location: Directory location to put files into\n        create: Create `location` if it does not point to an existing directory\n\n    Returns: list of Path objects to downloaded files\n    \"\"\"\n    downloaded_files = []\n    tqdm = get_tqdm_progress_bar()\n    for job in tqdm(self.jobs):\n        try:\n            downloaded_files.extend(job.download_files(location, create))\n        except HyP3SDKError as e:\n            print(f'Warning: {e} Skipping download for {job}.')\n    return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.filter_jobs","title":"<code>filter_jobs(succeeded=True, pending=True, running=True, failed=False, include_expired=True)</code>","text":"<p>Filter jobs by status. By default, only succeeded, pending, and still running jobs will be in the returned batch.</p> <p>Parameters:</p> Name Type Description Default <code>succeeded</code> <code>bool</code> <p>Include all succeeded jobs</p> <code>True</code> <code>pending</code> <code>bool</code> <p>Include all pending jobs</p> <code>True</code> <code>running</code> <code>bool</code> <p>Include all running jobs</p> <code>True</code> <code>failed</code> <code>bool</code> <p>Include all failed jobs</p> <code>False</code> <code>include_expired</code> <code>bool</code> <p>Include expired jobs in the result</p> <code>True</code> <p>Returns:</p> Name Type Description <code>batch</code> <code>Batch</code> <p>A batch object containing jobs matching all the selected statuses</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def filter_jobs(\n    self,\n    succeeded: bool = True,\n    pending: bool = True,\n    running: bool = True,\n    failed: bool = False,\n    include_expired: bool = True,\n) -&gt; 'Batch':\n\"\"\"Filter jobs by status. By default, only succeeded, pending,\n    and still running jobs will be in the returned batch.\n\n    Args:\n        succeeded: Include all succeeded jobs\n        pending: Include all pending jobs\n        running: Include all running jobs\n        failed: Include all failed jobs\n        include_expired: Include expired jobs in the result\n\n\n    Returns:\n         batch: A batch object containing jobs matching all the selected statuses\n    \"\"\"\n    filtered_jobs = []\n\n    for job in self.jobs:\n        if job.succeeded() and succeeded:\n            if include_expired or not job.expired():\n                filtered_jobs.append(job)\n\n        elif job.running() and running:\n            filtered_jobs.append(job)\n\n        elif job.pending() and pending:\n            filtered_jobs.append(job)\n\n        elif job.failed() and failed:\n            filtered_jobs.append(job)\n\n    return Batch(filtered_jobs)\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.succeeded","title":"<code>succeeded()</code>","text":"<p>Returns: True if all jobs have succeeded, otherwise returns False</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def succeeded(self) -&gt; bool:\n\"\"\"Returns: True if all jobs have succeeded, otherwise returns False\"\"\"\n    for job in self.jobs:\n        if not job.succeeded():\n            return False\n    return True\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Job","title":"<code>Job</code>","text":"Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>class Job:\n    _attributes_for_resubmit = {'name', 'job_parameters', 'job_type'}\n\n    def __init__(\n        self,\n        job_type: str,\n        job_id: str,\n        request_time: datetime,\n        status_code: str,\n        user_id: str,\n        name: str | None = None,\n        job_parameters: dict | None = None,\n        files: list | None = None,\n        logs: list | None = None,\n        browse_images: list | None = None,\n        thumbnail_images: list | None = None,\n        expiration_time: datetime | None = None,\n        processing_times: list[float] | None = None,\n        credit_cost: float | None = None,\n        priority: int | None = None,\n    ):\n        self.job_id = job_id\n        self.job_type = job_type\n        self.request_time = request_time\n        self.status_code = status_code\n        self.user_id = user_id\n        self.name = name\n        self.job_parameters = job_parameters\n        self.files = files\n        self.logs = logs\n        self.browse_images = browse_images\n        self.thumbnail_images = thumbnail_images\n        self.expiration_time = expiration_time\n        self.processing_times = processing_times\n        self.credit_cost = credit_cost\n        self.priority = priority\n\n    def __repr__(self):\n        return f'Job.from_dict({self.to_dict()})'\n\n    def __str__(self):\n        return f'HyP3 {self.job_type} job {self.job_id}'\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    @staticmethod\n    def from_dict(input_dict: dict):\n        expiration_time = parse_date(input_dict['expiration_time']) if input_dict.get('expiration_time') else None\n        return Job(\n            job_type=input_dict['job_type'],\n            job_id=input_dict['job_id'],\n            request_time=parse_date(input_dict['request_time']),\n            status_code=input_dict['status_code'],\n            user_id=input_dict['user_id'],\n            name=input_dict.get('name'),\n            job_parameters=input_dict.get('job_parameters'),\n            files=input_dict.get('files'),\n            logs=input_dict.get('logs'),\n            browse_images=input_dict.get('browse_images'),\n            thumbnail_images=input_dict.get('thumbnail_images'),\n            expiration_time=expiration_time,\n            processing_times=input_dict.get('processing_times'),\n            credit_cost=input_dict.get('credit_cost'),\n            priority=input_dict.get('priority'),\n        )\n\n    def to_dict(self, for_resubmit: bool = False):\n        job_dict = {}\n        if for_resubmit:\n            keys_to_process = Job._attributes_for_resubmit\n        else:\n            keys_to_process = set(vars(self).keys())\n\n        for key in keys_to_process:\n            value = self.__getattribute__(key)\n            if value is not None:\n                if isinstance(value, datetime):\n                    job_dict[key] = value.isoformat(timespec='seconds')\n                else:\n                    job_dict[key] = value\n\n        return job_dict\n\n    def succeeded(self) -&gt; bool:\n        return self.status_code == 'SUCCEEDED'\n\n    def failed(self) -&gt; bool:\n        return self.status_code == 'FAILED'\n\n    def complete(self) -&gt; bool:\n        return self.succeeded() or self.failed()\n\n    def pending(self) -&gt; bool:\n        return self.status_code == 'PENDING'\n\n    def running(self) -&gt; bool:\n        return self.status_code == 'RUNNING'\n\n    def expired(self) -&gt; bool:\n        return self.expiration_time is not None and datetime.now(tz.UTC) &gt;= self.expiration_time\n\n    def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n            location: Directory location to put files into\n            create: Create `location` if it does not point to an existing directory\n\n        Returns: list of Path objects to downloaded files\n        \"\"\"\n        location = Path(location)\n\n        if not self.succeeded():\n            raise HyP3SDKError(f'Only succeeded jobs can be downloaded; job is {self.status_code}.')\n        if self.expired():\n            assert self.expiration_time is not None\n            raise HyP3SDKError(\n                f'Expired jobs cannot be downloaded; job expired {self.expiration_time.isoformat(timespec=\"seconds\")}.'\n            )\n\n        if create:\n            location.mkdir(parents=True, exist_ok=True)\n        elif not location.is_dir():\n            raise NotADirectoryError(str(location))\n\n        assert self.files is not None\n\n        downloaded_files = []\n        for file in self.files:\n            download_url = file['url']\n            filename = location / file['filename']\n            try:\n                downloaded_files.append(download_file(download_url, filename, chunk_size=10485760))\n            except HTTPError:\n                raise HyP3SDKError(f'Unable to download file: {download_url}')\n        return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Job.download_files","title":"<code>download_files(location='.', create=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>location</code> <code>Path | str</code> <p>Directory location to put files into</p> <code>'.'</code> <code>create</code> <code>bool</code> <p>Create <code>location</code> if it does not point to an existing directory</p> <code>True</code> <p>Returns: list of Path objects to downloaded files</p> Source code in <code>hyp3_sdk/jobs.py</code> <pre><code>def download_files(self, location: Path | str = '.', create: bool = True) -&gt; list[Path]:\n\"\"\"Args:\n        location: Directory location to put files into\n        create: Create `location` if it does not point to an existing directory\n\n    Returns: list of Path objects to downloaded files\n    \"\"\"\n    location = Path(location)\n\n    if not self.succeeded():\n        raise HyP3SDKError(f'Only succeeded jobs can be downloaded; job is {self.status_code}.')\n    if self.expired():\n        assert self.expiration_time is not None\n        raise HyP3SDKError(\n            f'Expired jobs cannot be downloaded; job expired {self.expiration_time.isoformat(timespec=\"seconds\")}.'\n        )\n\n    if create:\n        location.mkdir(parents=True, exist_ok=True)\n    elif not location.is_dir():\n        raise NotADirectoryError(str(location))\n\n    assert self.files is not None\n\n    downloaded_files = []\n    for file in self.files:\n        download_url = file['url']\n        filename = location / file['filename']\n        try:\n            downloaded_files.append(download_file(download_url, filename, chunk_size=10485760))\n        except HTTPError:\n            raise HyP3SDKError(f'Unable to download file: {download_url}')\n    return downloaded_files\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.util","title":"<code>util</code>","text":"<p>Extra utilities for working with HyP3</p>"},{"location":"using/sdk_api/#hyp3_sdk.util.chunk","title":"<code>chunk(itr, n=200)</code>","text":"<p>Split a sequence into small chunks</p> <p>Parameters:</p> Name Type Description Default <code>itr</code> <code>Sequence[Any]</code> <p>A sequence object to chunk</p> required <code>n</code> <code>int</code> <p>Size of the chunks to return</p> <code>200</code> Source code in <code>hyp3_sdk/util.py</code> <pre><code>def chunk(itr: Sequence[Any], n: int = 200) -&gt; Generator[Sequence[Any], None, None]:\n\"\"\"Split a sequence into small chunks\n\n    Args:\n        itr: A sequence object to chunk\n        n: Size of the chunks to return\n    \"\"\"\n    error_message = f'n must be a positive integer: {n}'\n    if not isinstance(n, int):\n        raise TypeError(error_message)\n    if n &lt; 1:\n        raise ValueError(error_message)\n\n    for i in range(0, len(itr), n):\n        yield itr[i : i + n]\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.util.download_file","title":"<code>download_file(url, filepath, chunk_size=None, retries=2, backoff_factor=1)</code>","text":"<p>Download a file Args:     url: URL of the file to download     filepath: Location to place file into     chunk_size: Size to chunk the download into     retries: Number of retries to attempt     backoff_factor: Factor for calculating time between retries Returns:     download_path: The path to the downloaded file</p> Source code in <code>hyp3_sdk/util.py</code> <pre><code>def download_file(url: str, filepath: Path | str, chunk_size=None, retries=2, backoff_factor=1) -&gt; Path:\n\"\"\"Download a file\n    Args:\n        url: URL of the file to download\n        filepath: Location to place file into\n        chunk_size: Size to chunk the download into\n        retries: Number of retries to attempt\n        backoff_factor: Factor for calculating time between retries\n    Returns:\n        download_path: The path to the downloaded file\n    \"\"\"\n    filepath = Path(filepath)\n    session = requests.Session()\n    retry_strategy = Retry(\n        total=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=[429, 500, 502, 503, 504],\n    )\n\n    session.mount('https://', HTTPAdapter(max_retries=retry_strategy))\n    session.mount('http://', HTTPAdapter(max_retries=retry_strategy))\n    stream = False if chunk_size is None else True\n    with session.get(url, stream=stream) as s:\n        s.raise_for_status()\n        tqdm = get_tqdm_progress_bar()\n        with tqdm.wrapattr(\n            open(filepath, 'wb'), 'write', miniters=1, desc=filepath.name, total=int(s.headers.get('content-length', 0))\n        ) as f:\n            for chunk in s.iter_content(chunk_size=chunk_size):\n                if chunk:\n                    f.write(chunk)\n    session.close()\n\n    return filepath\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.util.extract_zipped_product","title":"<code>extract_zipped_product(zip_file, delete=True)</code>","text":"<p>Extract a zipped HyP3 product</p> <p>Extract a zipped HyP3 product to the same directory as the zipped HyP3 product, optionally deleting <code>zip file</code> afterward.</p> <p>Parameters:</p> Name Type Description Default <code>zip_file</code> <code>str | Path</code> <p>Zipped HyP3 product to extract</p> required <code>delete</code> <code>bool</code> <p>Delete <code>zip_file</code> after it has been extracted</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the HyP3 product folder containing the product files</p> Source code in <code>hyp3_sdk/util.py</code> <pre><code>def extract_zipped_product(zip_file: str | Path, delete: bool = True) -&gt; Path:\n\"\"\"Extract a zipped HyP3 product\n\n    Extract a zipped HyP3 product to the same directory as the zipped HyP3 product, optionally\n    deleting `zip file` afterward.\n\n    Args:\n        zip_file: Zipped HyP3 product to extract\n        delete: Delete `zip_file` after it has been extracted\n\n    Returns:\n        Path to the HyP3 product folder containing the product files\n    \"\"\"\n    zip_file = Path(zip_file)\n    with ZipFile(zip_file) as z:\n        z.extractall(path=zip_file.parent)\n\n    if delete:\n        zip_file.unlink()\n\n    return zip_file.parent / zip_file.stem\n</code></pre>"},{"location":"using/sdk_api/#hyp3_sdk.util.get_authenticated_session","title":"<code>get_authenticated_session(username=None, password=None, token=None)</code>","text":"<p>Log into HyP3 using credentials for <code>urs.earthdata.nasa.gov</code> from either the provided  credentials or a <code>.netrc</code> file.</p> <p>Returns:</p> Type Description <code>Session</code> <p>An authenticated HyP3 Session</p> Source code in <code>hyp3_sdk/util.py</code> <pre><code>def get_authenticated_session(\n    username: str | None = None, password: str | None = None, token: str | None = None\n) -&gt; requests.Session:\n\"\"\"Log into HyP3 using credentials for `urs.earthdata.nasa.gov` from either the provided\n     credentials or a `.netrc` file.\n\n    Returns:\n        An authenticated HyP3 Session\n    \"\"\"\n    s = requests.Session()\n\n    if token is not None:\n        s.headers.update({'Authorization': f'Bearer {token}'})\n        return s\n\n    if username is not None and password is not None:\n        response = s.get(AUTH_URL, auth=(username, password))\n        auth_error_message = (\n            'Was not able to authenticate with username and password provided\\n'\n            'This could be due to invalid credentials or a connection error.'\n        )\n    else:\n        response = s.get(AUTH_URL)\n        auth_error_message = (\n            'Was not able to authenticate with .netrc file and no credentials provided\\n'\n            'This could be due to invalid credentials in .netrc or a connection error.'\n        )\n\n    parsed_url = urllib.parse.urlparse(response.url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    error_msg = query_params.get('error_msg')\n    resolution_url = query_params.get('resolution_url')\n\n    if error_msg is not None and resolution_url is not None:\n        raise AuthenticationError(f'{error_msg[0]}: {resolution_url[0]}')\n\n    if error_msg is not None and 'Please update your profile' in error_msg[0]:\n        raise AuthenticationError(f'{error_msg[0]}: {PROFILE_URL}')\n\n    try:\n        response.raise_for_status()\n    except requests.HTTPError:\n        raise AuthenticationError(auth_error_message)\n\n    return s\n</code></pre>"},{"location":"using/vertex/","title":"On Demand Sentinel-1 Processing in Vertex","text":"<p>The Alaska Satellite Facility offers  On Demand processing of Sentinel-1 datasets to Radiometric Terrain Correction (RTC) or Interferometric SAR (InSAR) products through Vertex,  ASF's Data Search web portal. You can submit scenes to be processed into higher-level products, avoiding the  cost and complexity of performing such processing yourself.</p> <p></p> <p>On Demand products are generated using ASF's  HyP3 processing platform. Refer to the Products page for more information about the  various On Demand products ASF offers. </p>"},{"location":"using/vertex/#getting-started","title":"Getting Started","text":"<p>To request On Demand products, visit  ASF Data Search - Vertex  and Sign In with your Earthdata Login credentials.</p>"},{"location":"using/vertex/#1-select-your-hyp3-api-optional","title":"1. Select your HyP3 API (OPTIONAL)","text":"<p>Vertex supports requesting On Demand products through both  HyP3 Basic (default) and  HyP3+ by changing the HyP3 API URL. </p> <p>If you are only using the free HyP3 Basic service,  you do not need to make any changes to the HyP3 API in Vertex. </p> <p>To change the HyP3 API in Vertex:</p> <ol> <li>Click on your username icon and select Preferences.    </li> <li>Enter your desired HyP3 API URL in the HyP3 API URL field.    <ul> <li>HyP3 Basic: https://hyp3-api.asf.alaska.edu</li> <li>HyP3+: https://hyp3-plus.asf.alaska.edu</li> <li>URLs that are entered in this field will be available as a drop-down menu item for future use.</li> </ul> </li> <li>Click Done to exit the Preferences page.</li> </ol>"},{"location":"using/vertex/#2-select-your-scenes","title":"2. Select your scenes","text":"<p>RTC processing is available for Sentinel-1 GRD-H and SLC scenes acquired using the  Interferometric Wide Swath (IW) mode.  InSAR processing requires pairs of IW SLC scenes or bursts. </p> <p>Use the Geographic Search  in Vertex to find individual scenes to submit for RTC processing, or to find reference scenes to use for generating  InSAR pairs. </p> <ul> <li>For InSAR, once     you find a reference scene or burst, use either the     Baseline     or SBAS     Search to find SLC pairs to submit for processing. </li> <li>For burst-based InSAR processing, search for the Sentinel-1 Bursts Dataset instead of Sentinel-1.</li> <li>To process ARIA S1 GUNW products On Demand, search for the ARIA S1 GUNW Dataset instead of Sentinel-1,     and activate the On Demand toggle to view the ARIA Frames.</li> </ul> <p>Click the On Demand icon   displayed next to valid source granules or pairs to select the job type and add them to the On Demand queue.</p>"},{"location":"using/vertex/#3-submit-your-request","title":"3. Submit your request","text":"<p>After selecting your scenes, access the On Demand Queue   to submit your processing requests. There is a separate tab for each job type, which displays available processing  options.</p> <p>When you submit jobs for processing, you will have the option to add a Project Name, which makes it easier to search  for and manage your On Demand products. </p> <p>If you have multiple job types in your queue, you can choose which job types to submit for processing under that  project name. If you deselect any of the job types, they will remain in your queue, but will not be  submitted for processing.</p> <p>You may process jobs worth up to a total of 8,000  credits per month. See our Credits  page for more details.</p>"},{"location":"using/vertex/#4-monitor-your-request","title":"4. Monitor your request","text":"<p>The On Demand Products  Search Type displays your running and completed requests. New requests are typically available for download  within an hour or two, but wait time will depend on processing load and product type.</p>"},{"location":"using/vertex/#5-download-your-data","title":"5. Download your data","text":"<p>Once On Demand products have been processed, download options will be available in the results of an  On Demand Products search. Products can be downloaded individually through your browser    or by adding them to the Download Queue .</p> <ul> <li>Refer to the    Downloads     page for more information about download options. </li> <li>On Demand products are retained and available to download for two weeks (14 days) after processing.</li> </ul>"},{"location":"using/vertex/#tutorials","title":"Tutorials","text":"<p>Refer to our StoryMap Tutorials  for step-by-step guidance on submitting, downloading, and working with many of the different On Demand products  available from ASF.</p> <p></p>"}]}